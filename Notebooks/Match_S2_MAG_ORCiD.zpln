{
 "paragraphs": [
  {
   "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explictly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\nSPARK_HOME /home/ometaxas/Programs/spark-3.1.2-bin-hadoop3.2\n\n#spark.jars /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/rapids-4-spark_2.12-0.2.0.jar, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/cudf-0.15-cuda10-1.jar\n\n#spark.sql.warehouse.dir /home/ometaxas/Programs/zeppelin-0.9.0-preview2-bin-all/spark-warehouse\n\n \nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryoserializer.buffer.max 1000M\nspark.driver.memory 60g\nspark.driver.maxResultSize 5g \n\n#spark.kryo.registrator com.nvidia.spark.rapids.GpuKryoRegistrator\n#spark.rapids.sql.concurrentGpuTasks=3\n#spark.rapids.sql.enabled true\n#spark.rapids.memory.pinnedPool.size 2G \n#spark.plugins com.nvidia.spark.SQLPlugin \n\n#spark.locality.wait 0s \n#spark.sql.files.maxPartitionBytes 512m \n#spark.sql.shuffle.partitions 100 \nspark.executor.resource.gpu.amount=1\n#spark.task.resource.gpu.amount=0.25\n\nSPARK_LOCAL_DIRS /media/ometaxas/nvme/spark                                         \n# /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp, /media/datadisk/Datasets/Spark\n#,/media/datadisk/Datasets/Spark \n#/media/datadisk/Datasets/Spark\n\n# set executor memrory 110g\n#spark.executor.memory  30g\n\n\n# set executor number to be 6\n# spark.executor.instances  6\n#spark.executor.cores=4\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode after Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use it after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true",
   "user": "anonymous",
   "dateUpdated": "2023-06-14 17:45:59.669",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636716979859_479619425",
   "id": "paragraph_1636716979859_479619425",
   "dateCreated": "2021-11-12T13:36:19+0200",
   "dateStarted": "2023-06-14 17:45:59.559",
   "dateFinished": "2023-06-14 17:45:59.669",
   "status": "FINISHED",
   "results": {
    "code": "SUCCESS",
    "msg": []
   }
  },
  {
   "title": "Register Normalized UDFs",
   "text": "%spark\nimport org.apache.commons.lang3.StringUtils\nimport java.util.Locale;\n\nval shortNormName = udf[String, String]((e:String) => {\n    \n    val normname = org.apache.commons.lang3.StringUtils.stripAccents(e.toLowerCase(Locale.ENGLISH))\n    //.replaceAll(\"[^\\\\p{ASCII}]\", \"\")\n     .replaceAll(\"[<>:´,’./\\\\'\\\\\\\";(){}!@#$%^&+‐–*\\\\\\\\-]+\", \"\")\n    .replaceAll(\"Æ\", \"AE\")\n    .replaceAll(\"Ð\", \"D\")\n    .replaceAll(\"Ø\", \"O\")\n    .replaceAll(\"Þ\", \"TH\")\n    .replaceAll(\"ß\", \"ss\")\n    .replaceAll(\"ð\", \"d\")\n    .replaceAll(\"æ\", \"ae\")\n    .replaceAll(\"ø\", \"o\")\n    .replaceAll(\"þ\", \"th\")\n    .replaceAll(\"Œ\", \"OE\")\n    .replaceAll(\"œ\", \"oe\")\n    .replaceAll(\"ƒ\", \"f\")\n\t.trim().split(\" \")\n    \n\n val shortName =  if (normname.length == 1) normname(0) else normname(0).take(1) + normname(normname.length-1)\n shortName\n\n    \n})\n\nspark.udf.register(\"shortNormName\", shortNormName)\n\nval normName = udf[String, String]((e:String) => {\n    \n org.apache.commons.lang3.StringUtils.stripAccents(e.toLowerCase(Locale.ENGLISH))\n     .replaceAll(\"[ <>:´,’./\\\\'\\\\\\\";(){}!@#$%^&+‐–*\\\\\\\\-]+\", \"\")\n    .replaceAll(\"Æ\", \"AE\")\n    .replaceAll(\"Ð\", \"D\")\n    .replaceAll(\"Ø\", \"O\")\n    .replaceAll(\"Þ\", \"TH\")\n    .replaceAll(\"ß\", \"ss\")\n    .replaceAll(\"ð\", \"d\")\n    .replaceAll(\"æ\", \"ae\")\n    .replaceAll(\"ø\", \"o\")\n    .replaceAll(\"þ\", \"th\")\n    .replaceAll(\"Œ\", \"OE\")\n    .replaceAll(\"œ\", \"oe\")\n    .replaceAll(\"ƒ\", \"f\")\n\t.trim()\n    \n})\n\nspark.udf.register(\"normName\", normName)\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T13:36:28+0200",
   "progress": 0.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "editorHide": false,
    "title": true,
    "results": {},
    "enabled": true,
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {}
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.commons.lang3.StringUtils\nimport java.util.Locale\n\u001b[1m\u001b[34mshortNormName\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = SparkUserDefinedFunction($Lambda$2453/0x00007fa24e7fe040@44030597,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n\u001b[1m\u001b[34mnormName\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = SparkUserDefinedFunction($Lambda$2551/0x00007fa24e4f7040@3a32136,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n\u001b[1m\u001b[34mres1\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = SparkUserDefinedFunction($Lambda$2551/0x00007fa24e4f7040@3a32136,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: strin...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636716988200_2023448723",
   "id": "paragraph_1636716988200_2023448723",
   "dateCreated": "2021-11-12T13:36:28+0200",
   "dateStarted": "2021-11-12T13:36:28+0200",
   "dateFinished": "2021-11-12T13:36:47+0200",
   "status": "FINISHED"
  },
  {
   "title": "Pre-process S2 data ",
   "text": "%spark\nval S2_HOME = \"/media/datadisk/Datasets/SemanticScholar/06112020\"\nval S2_sample = \"/media/datadisk/Datasets/SemanticScholar/sample/sample-S2-records.gz\"\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\nimport  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\n\n\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\n\n//Get Semantic Scholar articles \nval S2articlesdf = spark.read.json(s\"file://$S2_HOME\")\n//val S2articlesdf = spark.read.json(s\"file://$S2_sample\")\n//S2articlesdf.printSchema\n//S2articlesdf.show(5) \n\nval S2subsetdf = S2articlesdf\n             //.join(broadcast(doisdf), lower(S2articlesdf(\"doi\"))===doisdf(\"doi\"), \"inner\")\n             //.filter(($\"magId\" =!= \"\") && ($\"magId\".isNotNull))\n             .select($\"title\", $\"id\",lower(S2articlesdf(\"doi\")).as(\"doi\"), $\"magId\", $\"fieldsOfStudy\".as(\"S2fos\"), $\"pmid\".as(\"pmId\"), $\"authors\")\n             \n//S2subsetdf.show(10)\n\n//Create pub - author rows\nval S2flatdf = S2subsetdf.select($\"title\", $\"id\", $\"doi\", $\"magId\", $\"pmId\", $\"S2fos\", explode($\"authors\").as(\"authorsflat\"))\n\nval S2_Pub_Authors = S2flatdf.select($\"title\".as(\"S2title\"), $\"id\".as(\"S2paperId\"), lower($\"doi\").as(\"S2doi\"), $\"S2fos\", $\"magId\", $\"pmId\", $\"authorsflat.name\".as(\"S2name\"),concat_ws(\"\",$\"authorsflat.ids\").as(\"S2authorId\"),\n                                    shorNormName($\"authorsflat.name\").as(\"S2shortNormName\"),normName( $\"authorsflat.name\").as(\"S2normName\"))\n//.persist(StorageLevel.DISK_ONLY)\n//.cache()\n\n//S2_Pub_Authors.printSchema\n//S2_Pub_Authors.show(5)\n\n//println(\"S2_Author_Pub_Cnt:\" + S2_Pub_Authors.count())\nS2_Pub_Authors.write.mode(\"overwrite\").parquet(\"/media/datadisk/Datasets/MAG_S2/S2_Pub_Authors.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-09T12:26:04+0200",
   "progress": 0.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "editorHide": false,
    "title": true,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636453564180_1189631092",
   "id": "paragraph_1636453564180_1189631092",
   "dateCreated": "2021-11-09T12:26:04+0200",
   "dateStarted": "2021-11-09T12:26:04+0200",
   "dateFinished": "2021-11-09T12:26:04+0200",
   "status": "ERROR"
  },
  {
   "title": "Pre-process MAG Data",
   "text": "%spark\n\n\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\nimport java.util.Calendar;\n\n//org.apache.commons.lang3.StringUtils.stripAccents(input.toLowerCase(Locale.ENGLISH));\n\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\n\n//Get MAG AUthor - Pub pairs\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/2021-11-10/mag\"\nval MAG_ADV =  \"/media/datadisk/Datasets/MAG/2021-11-10/advanced\"\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\nval authorsAffTsvFilename = \"Authors.txt\"\nval papersTsvFilename = \"Papers.txt\"\n\n\nval paperAuthorAffSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"authorId\", LongType, false).                \n                add(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n                \nval paperAuthorAffdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperAuthorAffSchema).\n                csv(s\"file://$MAG_HOME/$paperAuthorsAffTsvFilename\")\n                \n                \nval authorSchema = new StructType().\n                add(\"authorId\", LongType, false).\n                add(\"rank\", LongType, true).                \n                add(\"normalizedName\", StringType, true).\n                add(\"displayName\",StringType, true).\n                add(\"lastKnownAffiliationId\", LongType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \n\n                \nval authordf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(authorSchema).\n                csv(s\"file://$MAG_HOME/$authorsAffTsvFilename\")\n\nval paperSchema = new StructType().\n    add(\"paperId\", LongType, false).\n    add(\"magRank\", IntegerType, true).\n    add(\"doi\", StringType, true).\n    add(\"docTypetmp\", StringType, true).\n    add(\"normalizedTitle\", StringType, true).\n    add(\"title\", StringType, false).\n    add(\"bookTitle\", StringType, true).\n    add(\"pubYear\", IntegerType, true).\n    add(\"pubDate\", StringType, true).\n    add(\"onlineDate\", StringType, true).\n    add(\"publisherName\", StringType, true).\n    add(\"journalId\", StringType, true).\n    add(\"conferenceSeriesId\", LongType, true).\n    add(\"conferenceInstancesId\", LongType, true).\n    add(\"volume\", StringType, true).\n    add(\"issue\", StringType, true).\n    add(\"firstPage\", StringType, true).\n    add(\"lastPage\", StringType, true).\n    add(\"referenceCount\", LongType, true).\n    add(\"citationCount\", LongType, true).\n    add(\"estimatedCitation\", LongType, true).\n    add(\"originalVenue\", StringType, true).\n    add(\"familyId\", StringType, true).\n    add(\"createdDate\", DateType, true)\n    \nval papersdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n              schema(paperSchema).\n             csv(s\"file://$MAG_HOME/$papersTsvFilename\")\n\nval paperAuthorsExtdf = paperAuthorAffdf\n                        .join(authordf, paperAuthorAffdf(\"authorId\")===authordf(\"authorId\"), \"inner\")\n                        .select(paperAuthorAffdf(\"authorId\").as(\"MAGauthorId\"),paperAuthorAffdf(\"paperId\").as(\"MAGpaperId\"),authordf(\"normalizedName\").as(\"MAGnormName\") , $\"affiliationId\",\n                          authordf(\"displayName\") as \"MAGname\", normName(authordf(\"normalizedName\")).as(\"normName\"), shortNormName(authordf(\"normalizedName\")).as(\"shortNormName\"))\n                        .persist(StorageLevel.DISK_ONLY)\n\npaperAuthorsExtdf.show(5)\n//println(\"paperAuthorsExtdf:\"+paperAuthorsExtdf.count())\nprintln(\"paperAuthorsExtdf completed \"+ Calendar.getInstance().getTime())                        \n\nval paperAuthorGrpAffdf = paperAuthorsExtdf\n                .groupBy($\"MAGpaperId\", $\"MAGauthorId\", $\"MAGname\", $\"MAGnormName\",  $\"normName\", $\"shortNormName\")\n                .agg( collect_set(paperAuthorAffdf(\"affiliationId\")).as(\"affiliationIds\"))\n                .persist(StorageLevel.DISK_ONLY)\n                \nprintln(\"paperAuthorGrpAffdf completed \"+ Calendar.getInstance().getTime())\n                \n\n\n                      \nval authorsPerPaperdf =   paperAuthorsExtdf\n                          .groupBy(paperAuthorsExtdf(\"MAGpaperId\"))\n                          .agg( collect_set($\"MAGauthorId\").as(\"authorIds\"), collect_list($\"normName\").as(\"authorNormNames\"), collect_list($\"shortNormName\").as(\"authorShortNormNames\"))\n                          .persist(StorageLevel.DISK_ONLY)\n\nauthorsPerPaperdf.show(5)\n//println(\"authorsPerPaperdf:\"+authorsPerPaperdf.count())\nprintln(\"authorsPerPaperdf completed \"+ Calendar.getInstance().getTime())                        \n                        \n//authorsPerPaperdf.write.parquet(\"/media/datadisk/Datasets/MAG_S2/authorsPerPaperdf.parquet\")\n  \n  \nval fieldsOfStudyTsvFilename = \"FieldsOfStudy.txt\"\nval paperFieldsOfStudyTsvFilename = \"PaperFieldsOfStudy.txt\"\nval fieldOfStudyChildrenTsvFilename =  \"FieldOfStudyChildren.txt\"\n\n\n\nval paperFieldsOfStudyschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"score\", DoubleType, true)\n                \nval paperFieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperFieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$paperFieldsOfStudyTsvFilename\")\n//paperFieldsOfStudydf.printSchema\n//paperFieldsOfStudydf.show(5)\n\n//Enrich with FoS (lvl0, lvl1 and other)\n\nval fieldsOfStudyschema = new StructType().\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"name\", StringType, true).\n                add(\"mainType\", StringType, true).\n                add(\"level\", IntegerType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval fieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$fieldsOfStudyTsvFilename\")\n\nval fieldOfStudyChildrenschema = new StructType().\n                add(\"fieldOfStudyId\", LongType, false).\n                add(\"childFieldOfStudyId\", LongType, false)\n                \n                \nval fieldOfStudyChildrendf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldOfStudyChildrenschema).\n                csv(s\"file://$MAG_ADV/$fieldOfStudyChildrenTsvFilename\")\n\n//val fos_lvl0 = fieldsOfStudydf.filter($\"level\"===0).show()\n\n//val fos_lvl1 = fieldsOfStudydf.filter($\"level\"===1).show()\n\n\nval paper_fos = paperFieldsOfStudydf.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(paperFieldsOfStudydf(\"paperId\").as(\"paper_fos_paperId\"))\n                .agg( \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),\n                    //sum(when($\"date\" > \"2017-04\", $\"value\")).alias(\"value4\")\n                    collect_set(when($\"level\" > 1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids\"),\n                    collect_set(when($\"level\" ===  0, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl0\"),\n                    collect_set(when($\"level\" ===  1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl1\")\n                    )\n                    .persist(StorageLevel.DISK_ONLY)\n\npaper_fos.show(5)\n//println(\"authorsPerPaperdf:\"+authorsPerPaperdf.count())\nprintln(\"paper_fos completed \"+ Calendar.getInstance().getTime())                        \n\nval MAG_pub_authorsdf = paperAuthorGrpAffdf\n              //.join(broadcast(doisdf), lower(papersdf(\"doi\"))===doisdf(\"doi\"), \"inner\")\n              .join(papersdf, paperAuthorGrpAffdf(\"MAGpaperId\")===papersdf(\"paperId\"), \"inner\")\n              .join(paper_fos, paper_fos(\"paper_fos_paperId\")=== paperAuthorGrpAffdf(\"MAGpaperId\"), \"outer\")\n              .join(authorsPerPaperdf, authorsPerPaperdf(\"MAGpaperId\")===paperAuthorGrpAffdf(\"MAGpaperId\"), \"inner\")\n              .select(papersdf(\"title\").as(\"MAGtitle\"), paperAuthorGrpAffdf(\"MAGpaperId\"),lower(papersdf(\"doi\")).as(\"MAGdoi\"), paperAuthorGrpAffdf(\"MAGname\"), \n                  paperAuthorGrpAffdf(\"MAGnormName\"),  paperAuthorGrpAffdf(\"normName\"), paperAuthorGrpAffdf(\"shortNormName\"),\n                  authorsPerPaperdf(\"authorIds\"), authorsPerPaperdf(\"authorNormNames\"), authorsPerPaperdf(\"authorShortNormNames\"),\n                  paperAuthorGrpAffdf(\"MAGauthorId\"), paperAuthorGrpAffdf(\"affiliationIds\"), paper_fos(\"fosids\"), paper_fos(\"fosids_lvl0\") ,paper_fos(\"fosids_lvl1\"))\n              //.persist(StorageLevel.DISK_ONLY)\n              //.cache()\n             //.write.csv(\"relatedFoS.csv\")\n                //fieldsOfStudydf.dropDuplicates(\"mainType\").select(\"mainType\").show(200)\n\n//println(\"MAG_Author_pubCnt:\"+MAG_pub_authorsdf.count())\n\nMAG_pub_authorsdf.write.parquet(\"/media/datadisk/Datasets/MAG_S2/MAG_pub_authors.parquet\")\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-08T17:17:23+0200",
   "progress": 46.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "title": true,
    "results": {},
    "enabled": true,
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636384643519_1894391252",
   "id": "paragraph_1636384643519_1894391252",
   "dateCreated": "2021-11-08T17:17:23+0200",
   "dateStarted": "2021-11-08T17:17:23+0200",
   "dateFinished": "2021-11-08T17:56:25+0200",
   "status": "ABORT"
  },
  {
   "text": "%spark\nval MAG_pub_authorsdf = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/MAG_pub_authors.parquet\")\n\nMAG_pub_authorsdf.show(20)\nMAG_pub_authorsdf.printSchema()\nprintln(\"MAG_pub_authorsdf  cnt:\"+MAG_pub_authorsdf.count())\n\n\n\n//661546284\n//653044459\n//661546284\n\n",
   "user": "anonymous",
   "dateUpdated": "2020-12-15 00:16:31.388",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "editorHide": false,
    "title": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://PC192.168.2.5.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://PC192.168.2.5.station:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://PC192.168.2.5.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1606814247953_2103858075",
   "id": "paragraph_1606814247953_2103858075",
   "dateCreated": "2020-12-01 11:17:27.953",
   "dateStarted": "2020-12-02 12:21:35.952",
   "dateFinished": "2020-12-02 12:21:49.420",
   "status": "FINISHED",
   "title": "Stats on MAG Author-Pub records"
  },
  {
   "title": "Read ORCiD JSON Author-Pub files & convert to parquet",
   "text": "%spark\n\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\nimport  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\n//org.apache.commons.lang3.StringUtils.stripAccents(input.toLowerCase(Locale.ENGLISH));\n\n\n//val jsondf = spark.read.json(\"/media/datadisk/Datasets/ORCiD/9988322/ORCID_2019_summaries/outgz/0.gz\")\nval orcid_df = spark.read.json(\"/home/ometaxas/Datasets/orcid\")\n//.select(lower($\"DOI\").as(\"doi\"), concat_ws(\" \", $\"firstName\",$\"surName\").as(\"ORCfullName\"), shortNormName( concat_ws(\" \", $\"firstName\",$\"surName\")).as(\"ORCshortNormName\"), normName(concat_ws(\" \", $\"firstName\",$\"surName\")).as(\"ORCnormName\"),  $\"pmID\".as(\"pmId\"), $\"orcId\")\n.dropDuplicates()\n//.persist(StorageLevel.DISK_ONLY)\n.cache()\n\n\norcid_df.printSchema\nprintln(\"ORCiD cnt:\" + orcid_df.count())\norcid_df.show(20)\n\n//orcid_df.write.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/orcid.parquet\")                    \n\n",
   "user": "anonymous",
   "dateUpdated": "2023-06-14 17:52:24.536",
   "progress": 0.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "editorHide": false,
    "title": true,
    "results": {
     "0": {
      "graph": {
       "mode": "table"
      }
     }
    },
    "enabled": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636546728107_1022166985",
   "id": "paragraph_1636546728107_1022166985",
   "dateCreated": "2021-11-10T14:18:48+0200",
   "dateStarted": "2023-06-14 17:52:10.670",
   "dateFinished": "2023-06-14 17:52:24.536",
   "status": "ERROR",
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 67 in stage 3.0 failed 1 times, most recent failure: Lost task 67.0 in stage 3.0 (TID 20147) (192.168.2.11 executor driver): java.io.IOException: invalid block type\n\tat org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:209)\n\tat org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n\tat java.base/java.io.InputStream.read(InputStream.java:205)\n\tat org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:182)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n  at scala.Option.foreach(Option.scala:407)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n  at org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:94)\n  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n  at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:99)\n  at org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:65)\n  at org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:58)\n  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)\n  at scala.Option.orElse(Option.scala:447)\n  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:428)\n  ... 44 elided\nCaused by: java.io.IOException: invalid block type\n  at org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:209)\n  at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111)\n  at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105)\n  at java.base/java.io.InputStream.read(InputStream.java:205)\n  at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:182)\n  at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:218)\n  at org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n  at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n  at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)\n  at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:190)\n  at scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:183)\n  at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:208)\n  at scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:207)\n  at scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1429)\n  at scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:215)\n  at scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:215)\n  at scala.collection.AbstractIterator.reduceOption(Iterator.scala:1429)\n  at org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:81)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n  at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:131)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n  ... 3 more\n"
     }
    ]
   }
  },
  {
   "title": "Join MAG - S2 pub-author records",
   "text": "%spark\n\n\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\n//NonNVME Took 9 hrs 12 min 44 sec. Last updated by anonymous at December 04 2020, 6:09:13 AM.\n//Took 2 hrs 36 min 12 sec. Last updated by anonymous at December 14 2020, 5:04:52 PM.\n\n\nval MAG_pub_authors1 = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/MAG_pub_authors.parquet\")\n//MAG_pub_authors1.printSchema()\n\n\n\n//MAG_pub_authors.repartition(500,\"\")\n//println(MAG_pub_authors.rdd.getNumPartitions)\n//println(\"Join_Mag_auth_pubCnt:\"+MAG_pub_authors.count())\n//MAG_pub_authors.write.format(\"parquet\").bucketBy(100, \"MAGpaperId\").sortBy(\"shortNormName\").saveAsTable(\"MAG_pub_authors_bucket\")\n//MAG_pub_authorsdf.write.bucketBy(100, \"MAGpaperId\").sortBy(\"shortNormName\").parquet(\"/media/datadisk/Datasets/MAG_S2/MAG_pub_authors_bucket.parquet\")                    \n\n\nval S2_Pub_Authors1 = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_Pub_Authors.parquet\").withColumn(\"magId\",col(\"magId\").cast(LongType)).withColumnRenamed(\"magId\", \"MAGpaperId\")\n//S2_Pub_Authors1.printSchema()\n//println(S2_Pub_Authors.rdd.getNumPartitions)\n//S2_Pub_Authorsdf.write.format(\"parquet\").bucketBy(100, \"magId\").sortBy(\"S2shortNormName\").saveAsTable(\"S2_pub_authors_bucket\")\n\n\n//val MAG_pub_authorsdf = spark.table(\"MAG_pub_authors_bucket\")\n//println(\"Join_Mag_auth_pubCnt:\"+MAG_pub_authorsdf.count())\n\n//val S2_Pub_Authors = spark.table(\"S2_pub_authors_bucket\")\n//println(\"Join_S2_auth_pubCnt:\"+MAG_pub_authorsdf.count())\n\n//repartitioning to avoid huge shuffling and speed-up sort-merge join \nval MAG_pub_authors = MAG_pub_authors1.repartition(500, MAG_pub_authors1.col(\"MAGpaperId\"))\nval S2_Pub_Authors = S2_Pub_Authors1.repartition(500, S2_Pub_Authors1.col(\"MAGpaperId\"))\n\n\nval S2_MAG_df = MAG_pub_authors\n                   .join(S2_Pub_Authors, S2_Pub_Authors(\"MAGpaperId\") === MAG_pub_authors(\"MAGpaperId\")  && ( S2_Pub_Authors(\"S2shortNormName\")===MAG_pub_authors(\"shortNormName\")  || (  jaro_winkler($\"normName\", $\"S2normName\") > 0.82 && levenshtein($\"normName\", $\"S2normName\")<3  && soundex($\"normName\") === soundex($\"S2normName\") ) ), \"left_outer\")                  \n                   //MAG_pub_authorsdf(\"MAGtitle\"), \n                   .select(MAG_pub_authors(\"MAGpaperId\"), MAG_pub_authors(\"MAGdoi\"),  MAG_pub_authors(\"MAGname\").as(\"MAGdisplayName\"), MAG_pub_authors(\"MAGnormName\").as(\"MAGname\"), MAG_pub_authors(\"normName\").as(\"MAGnormName\"), \n                            MAG_pub_authors(\"shortNormName\").as(\"MAGshortNormName\"),  MAG_pub_authors(\"MAGauthorId\"), MAG_pub_authors(\"fosids\"), MAG_pub_authors(\"fosids_lvl0\") ,MAG_pub_authors(\"fosids_lvl1\"), \n                            MAG_pub_authors(\"authorIds\"), MAG_pub_authors(\"authorNormNames\"), MAG_pub_authors(\"authorShortNormNames\"),MAG_pub_authors(\"affiliationIds\"), \n                            S2_Pub_Authors(\"S2authorId\"),S2_Pub_Authors(\"S2paperId\"), S2_Pub_Authors(\"S2doi\"), S2_Pub_Authors(\"S2name\"), S2_Pub_Authors(\"S2shortNormName\"), S2_Pub_Authors(\"S2normName\"), S2_Pub_Authors(\"S2fos\") )\n                            .dropDuplicates()\n                            // .persist(StorageLevel.DISK_ONLY)\n\n//S2_MAG_df.show(10)\n//println(\"Join_S2_Mag_auth_pubCnt:\"+S2_MAG_df.count())\nS2_MAG_df.write.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_df.parquet\")\n\n",
   "user": "anonymous",
   "dateUpdated": "2020-12-16 13:16:06.862",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "title": true,
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1605993979140_229979962",
   "id": "paragraph_1605993979140_229979962",
   "dateCreated": "2020-11-21 23:26:19.141",
   "dateStarted": "2020-12-16 13:16:06.871",
   "dateFinished": "2020-12-16 15:27:08.928",
   "status": "FINISHED"
  },
  {
   "title": "Analyze & remove false positives (double matches)",
   "text": "%spark\nimport org.apache.spark.sql.functions.countDistinct;\nimport org.apache.spark.storage.StorageLevel;\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_df.parquet\")\n//val df3 = S2_MAG_df.select(countDistinct($\"MAGpaperId\", $\"MAGauthorId\"))\n//df3.show(false)\n\n/*\nval S2_MAG_grp_df = S2_MAG_df\n                .groupBy($\"MAGpaperId\", $\"MAGauthorId\")\n                //.agg(count(\"S2authorId\").as(\"S2authorCnt\"), collect_list(S2_MAG_df(\"S2authorId\")).as(\"S2authorIds\"), collect_set(S2_MAG_df(\"S2paperId\")).as(\"S2paperIds\"), collect_list(S2_MAG_df(\"S2name\")).as(\"S2names\"), collect_list(S2_MAG_df(\"S2shortNormName\")).as(\"S2shortNormNames\"), collect_list(S2_MAG_df(\"S2normName\")).as(\"S2normNames\") )\n                .where($\"S2authorCnt\">1)\n                .persist(StorageLevel.DISK_ONLY)\n                */\n\nval S2_MAG_fp_df = S2_MAG_df\n                .groupBy($\"MAGpaperId\", $\"MAGauthorId\")\n                //.agg(count(\"S2authorId\").as(\"S2authorCnt\"))\n                .agg(count(\"S2authorId\").as(\"S2authorCnt\")\n                , collect_list(S2_MAG_df(\"S2authorId\")).as(\"S2authorIds\"), collect_set(S2_MAG_df(\"S2paperId\")).as(\"S2paperIds\"), collect_list(S2_MAG_df(\"S2name\")).as(\"S2names\"), collect_list(S2_MAG_df(\"S2shortNormName\")).as(\"S2shortNormNames\"), collect_list(S2_MAG_df(\"S2normName\")).as(\"S2normNames\") )\n                .where($\"S2authorCnt\">1)\n  //              .persist(StorageLevel.DISK_ONLY)\n\n//println(\"S2_MAG_fp_df cnt:\" + S2_MAG_fp_df.count())\n//S2_MAG_fp_df.show(50)\n//S2_MAG_fp_df.limit(200).coalesce(1).write.json(\"/media/ometaxas/nvme/datasets/MAG_S2/doubleMatch.json\")\n\nval S2_MAG_nonDoubles= S2_MAG_df.join(broadcast(S2_MAG_fp_df), S2_MAG_df(\"MAGpaperId\") === S2_MAG_fp_df(\"MAGpaperId\") && S2_MAG_df(\"MAGauthorId\") === S2_MAG_fp_df(\"MAGauthorId\") && S2_MAG_df(\"S2normName\")=!=S2_MAG_df(\"MAGnormName\"), \"leftanti\")\n\nS2_MAG_nonDoubles.write.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_NFP_df.parquet\")\n\n//.coalesce(1).write.csv(\"PaperReferenceswithDOIs.csv\")\n//661546284\n\n//val S2_MAG_Orciddf = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/S2_MAG_Orcid.parquet\").dropDuplicates()\n//802561855\n//println(\"S2_MAG cnt:\" + S2_MAG_df.count())\n\n//val S2_MAG_df1 = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_df.parquet\").dropDuplicates()\n//val S2_MAG_Orciddf = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/S2_MAG_Orcid.parquet\").dropDuplicates()\n\n//println(\"S2_MAG1 cnt:\" + S2_MAG_df1.count())\n\n",
   "user": "anonymous",
   "dateUpdated": "2020-12-17 21:17:34.047",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "title": true,
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1607966825569_667439463",
   "id": "paragraph_1607966825569_667439463",
   "dateCreated": "2020-12-14 19:27:05.569",
   "dateStarted": "2020-12-17 21:17:34.051",
   "dateFinished": "2020-12-17 22:36:01.971",
   "status": "FINISHED"
  },
  {
   "title": "Join ORCiD Data with MAG & S2 (based on MAG_doi)",
   "text": "%spark\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n//val jsondf = spark.read.json(\"/media/datadisk/Datasets/ORCiD/9988322/ORCID_2019_summaries/outgz/0.gz\")\nval orcid_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/orcid.parquet\")\n//orcid_df.printSchema\n//println(\"ORCiD cnt:\" + orcid_df.count())\n//orcid_df.show(40)\n/*\nroot\n |-- doi: string (nullable = true)\n |-- ORCfullName: string (nullable = true)\n |-- ORCshortNormName: string (nullable = true)\n |-- ORCnormName: string (nullable = true)\n |-- pmId: string (nullable = true)\n |-- orcId: string (nullable = true)\nORCiD cnt:43403542\n */\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\").filter($\"magDoi\"=!=\"\"  && $\"magDoi\".isNotNull) \n //spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_NFP_df.parquet\")\n//S2_MAG_df.groupBy($\"S2doi\").show(10)\n//S2_MAG_df.filter($\"S2doi\"=!=\"\"  && $\"S2doi\".isNotNull && $\"S2doi\"=!=$\"MAGdoi\")\n//.cache()\n//println(\"s2doi diffs cnt:\"+S2_MAG_df.count())\n//println(\"s2doi diffs cnt:\"+S2_MAG_Orcid_df\n//S2_MAG_df.filter($\"S2doi\"=!=\"\"  && ($\"magId\".isNotNull) && $\"S2doi\"=!=$\"MAGdoi\")\n\nval S2_MAG_Orcid_df1 = S2_MAG_df \n                    .join(orcid_df, (lower(S2_MAG_df(\"magDoi\")) === lower(orcid_df(\"doi\")))\n                            && ( S2_MAG_df(\"s2AuthorNormName\")===orcid_df(\"ORCshortNormName\") ||  S2_MAG_df(\"magNormName\")===orcid_df(\"ORCshortNormName\") ||  ( jaro_winkler($\"magShortNormName\", orcid_df(\"ORCnormName\")) > 0.82 && levenshtein($\"magShortNormName\", orcid_df(\"ORCnormName\"))<3 && soundex($\"magShortNormName\") === soundex(orcid_df(\"ORCnormName\")) ) )                   \n                   , \"left_outer\")\n        .select(S2_MAG_df(\"magPaperId\"), S2_MAG_df(\"magDoi\"),  S2_MAG_df(\"magDisplayName\"), S2_MAG_df(\"magName\"), S2_MAG_df(\"magNormName\"), \n                            S2_MAG_df(\"magShortNormName\"),  S2_MAG_df(\"magAuthorId\"), S2_MAG_df(\"fosIds\"), S2_MAG_df(\"fosIdsLevel0\") ,S2_MAG_df(\"fosIdsLevel1\"),      S2_MAG_df(\"authorIds\"), S2_MAG_df(\"authorNormNames\"), S2_MAG_df(\"authorShortNames\"),S2_MAG_df(\"affiliationIds\"), \n                            S2_MAG_df(\"s2AuthorId\"),S2_MAG_df(\"s2PaperId\"), S2_MAG_df(\"s2Doi\"), S2_MAG_df(\"s2AuthorName\"), S2_MAG_df(\"s2AuthorShorNormName\"), S2_MAG_df(\"s2AuthorNormName\"), S2_MAG_df(\"s2Fos\"), S2_MAG_df(\"soundex\"),orcid_df(\"ORCfullName\").as(\"orcFullName\"),orcid_df(\"ORCnormName\").as(\"orcNormName\"), orcid_df(\"orcId\") )\n  \n                   \nS2_MAG_Orcid_df1.write.mode(\"overwrite\").parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid2021.parquet\")      \n\n//NOnvme 4 hrs 6 min 0 sec. Last updated by anonymous at December 04 2020, 7:21:43 PM. (outdated)            \n//NVme 2h41m\n//Only those with DOIs 58m\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-11T10:25:16+0200",
   "progress": 99.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "editorHide": false,
    "title": true,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.concat_ws\nimport org.apache.spark.sql.functions.countDistinct\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\n\u001b[1m\u001b[34morcid_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [doi: string, ORCfullName: string ... 4 more fields]\n\u001b[1m\u001b[34mS2_MAG_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [magPaperId: bigint, magDoi: string ....\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636619116916_345348046",
   "id": "paragraph_1636619116916_345348046",
   "dateCreated": "2021-11-11T10:25:16+0200",
   "dateStarted": "2021-11-11T10:25:16+0200",
   "dateFinished": "2021-11-11T11:23:20+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval S2_MAG_Orcid_df1 = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid2021.parquet\")\n\n        \n        val S2_MAG_Orcid_df = S2_MAG_Orcid_df1.select(S2_MAG_Orcid_df1(\"magPaperId\"), S2_MAG_Orcid_df1(\"magDoi\"),  S2_MAG_Orcid_df1(\"magDisplayName\"), S2_MAG_Orcid_df1(\"magName\"), S2_MAG_Orcid_df1(\"magNormName\").as(\"magShortNormName\"),  S2_MAG_Orcid_df(\"magShortNormName\").as(\"magNormName\"),  S2_MAG_Orcid_df1(\"magAuthorId\"), S2_MAG_Orcid_df1(\"fosIds\"), S2_MAG_Orcid_df1(\"fosIdsLevel0\") ,S2_MAG_Orcid_df1(\"fosIdsLevel1\"), \n                            S2_MAG_Orcid_df1(\"authorIds\"), S2_MAG_Orcid_df1(\"authorNormNames\"), S2_MAG_Orcid_df1(\"authorShortNames\"),S2_MAG_Orcid_df(\"affiliationIds\"), \n                            S2_MAG_Orcid_df1(\"s2AuthorId\"),S2_MAG_Orcid_df1(\"s2PaperId\"), S2_MAG_Orcid_df1(\"s2Doi\"), S2_MAG_Orcid_df1(\"s2AuthorName\"), S2_MAG_Orcid_df1(\"s2AuthorShorNormName\").as(\"s2AuthorNormName\"), S2_MAG_Orcid_df(\"s2AuthorNormName\").as(\"s2AuthorShorNormName\"), S2_MAG_Orcid_df1(\"s2Fos\"), S2_MAG_Orcid_df1(\"soundex\"),S2_MAG_Orcid_df1(\"orcFullName\"),S2_MAG_Orcid_df1(\"orcNormName\"), S2_MAG_Orcid_df1(\"orcId\") )\n\nS2_MAG_Orcid_df.printSchema()\nS2_MAG_Orcid_df.show(10)\n\nval S2_MAG_df1 = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\").filter($\"magDoi\"===\"\"  || $\"magDoi\".isNull)\n\nval S2_MAG_df = S2_MAG_df1.select(S2_MAG_df1(\"magPaperId\"), S2_MAG_df1(\"magDoi\"),  S2_MAG_df1(\"magDisplayName\"), S2_MAG_df1(\"magName\"), S2_MAG_df1(\"magNormName\").as(\"magShortNormName\"),  S2_MAG_df1(\"magShortNormName\").as(\"magNormName\"),  S2_MAG_df1(\"magAuthorId\"), S2_MAG_df1(\"fosIds\"), S2_MAG_df1(\"fosIdsLevel0\") ,S2_MAG_df1(\"fosIdsLevel1\"), \n                            S2_MAG_df1(\"authorIds\"), S2_MAG_df1(\"authorNormNames\"), S2_MAG_df1(\"authorShortNames\"),S2_MAG_df1(\"affiliationIds\"), \n                            S2_MAG_df1(\"s2AuthorId\"),S2_MAG_df1(\"s2PaperId\"), S2_MAG_df1(\"s2Doi\"), S2_MAG_df1(\"s2AuthorName\"), S2_MAG_df1(\"s2AuthorShorNormName\").as(\"s2AuthorNormName\"), S2_MAG_df1(\"s2AuthorNormName\").as(\"s2AuthorShorNormName\"), S2_MAG_df1(\"s2Fos\"), S2_MAG_df1(\"soundex\"),S2_MAG_df1(\"orcFullName\"),S2_MAG_df1(\"orcNormName\"), S2_MAG_df1(\"orcId\") )\n\nS2_MAG_df.show(10)\n\nval S2_MAG_Orcid_all_df = S2_MAG_Orcid_df.union(S2_MAG_df)\n\nS2_MAG_Orcid_all_df.write.mode(\"overwrite\").parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_all_2021.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-11T13:56:28+0200",
   "progress": 99.0,
   "config": {
    "results": [
     {
      "mode": "table"
     }
    ],
    "title": true,
    "editorHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "root\n |-- magPaperId: long (nullable = true)\n |-- magDoi: string (nullable = true)\n |-- magDisplayName: string (nullable = true)\n |-- magName: string (nullable = true)\n |-- magShortNormName: string (nullable = true)\n |-- magNormName: string (nullable = true)\n |-- magAuthorId: long (nullable = true)\n |-- fosIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel0: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel1: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorNormNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- authorShortNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- affiliationIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- s2AuthorId: long (nullable = true)\n |-- s2PaperId: string (nullable = true)\n |-- s2Doi: string (nullable = true)\n |-- s2AuthorName: string (nullable = true)\n |-- s2AuthorNormName: string (nullable = true)\n |-- s2AuthorShorNormName: string (nullable = true)\n |-- s2Fos: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- soundex: string (nullable = true)\n |-- orcFullName: string (nullable = true)\n |-- orcNormName: string (nullable = true)\n |-- orcId: string (nullable = true)\n\n+----------+--------------------+--------------------+--------------------+----------------+-----------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+------------------+--------------------+----------+-------+--------------------+------------------+-------------------+\n|magPaperId|              magDoi|      magDisplayName|             magName|magShortNormName|      magNormName|magAuthorId|              fosIds|fosIdsLevel0|        fosIdsLevel1|           authorIds|     authorNormNames|    authorShortNames|affiliationIds|s2AuthorId|           s2PaperId|               s2Doi|        s2AuthorName|  s2AuthorNormName|s2AuthorShorNormName|     s2Fos|soundex|         orcFullName|       orcNormName|              orcId|\n+----------+--------------------+--------------------+--------------------+----------------+-----------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+------------------+--------------------+----------+-------+--------------------+------------------+-------------------+\n|2046457193|10.1001/2012.jama...|surgical vs lifes...|     David S. Ludwig|         dludwig|     davidsludwig| 2113245403|[2987512134, 2779...|  [71924100]|[126322002, 14107...|[925414589, 22974...|[cara b ebbeling,...|[carabebbeling, e...|  [1288882113]|   5357296|75447d4d3084b1279...|10.1001/2012.jama...|      David S Ludwig|      davidsludwig|             dludwig|[Medicine]|   D432|        David Ludwig|       davidludwig|0000-0003-3307-8544|\n|2091293432|10.1001/archderm....|lichenoid papular...|      Agustín España|         aespana|          aespana| 2122750511|[2909304250, 2910...|  [71924100]|          [16005928]|[2137550265, 3068...|[e quintanilla, m...|[equintanilla, mj...|            []|   3680133|79c9b4e7aa9a2b1eb...|10.1001/ARCHDERM....|           A  España|           aespana|             aespana|[Medicine]|   A215|      Agustín España|     agustinespana|0000-0003-0703-8733|\n|2068515346|10.1001/archderm....|acute generalized...|       Nicolas Dupin|          ndupin|           ndupin| 2044597927|[2778236110, 2780...|  [71924100]|          [16005928]|[2658301080, 2011...|[j p escande, isa...|[jpescande, isabe...|            []|   5027761|412ef155a9ca6d28b...|10.1001/ARCHDERM....|            N  Dupin|            ndupin|              ndupin|[Medicine]|   N315|       Nicolas Dupin|      nicolasdupin|0000-0002-6237-4951|\n|1976101571|10.1001/archderm....|mucosal morbidity...|   Thomas N. Darling|        tdarling|   thomasndarling| 2098348842|[2779305546, 2778...|  [71924100]|[16005928, 141071...|[2134565608, 2250...|[ronald m summers...|[ronaldmsummers, ...|  [1299303238]|   6908603|7a0b1b97c9e9c9cf1...|10.1001/ARCHDERM....|         T N Darling|         tndarling|            tdarling|[Medicine]|   T645|      Thomas Darling|     thomasdarling|0000-0002-5161-1974|\n|2155274286|10.1001/archderm....|effects of cryoge...|       Bahman Anvari|         banvari|     bahmananvari| 2403637419|[520434653, 11133...|  [71924100]|         [142724271]|[2154663402, 2097...|[jorge h torres, ...|[jorgehtorres, ba...|    [74775410]|   5746535|d2be717529ce12368...|10.1001/ARCHDERM....|      Bahman  Anvari|      bahmananvari|             banvari|[Medicine]|   B516|       Bahman Anvari|      bahmananvari|0000-0002-2511-5854|\n|2163796740|10.1001/archderm....|successful treatm...|  Angela Dispenzieri|    adispenzieri|angeladispenzieri|   62674847|[28328180, 291098...|  [71924100]|         [141071460]|[2016381842, 2604...|[arnold l schroet...|[arnoldlschroeter...|            []|   4882576|38ea95b0a4e4065fe...|10.1001/ARCHDERM....| Angela  Dispenzieri| angeladispenzieri|        adispenzieri|[Medicine]|   A321|  Angela Dispenzieri| angeladispenzieri|0000-0001-8780-9512|\n|2163796740|10.1001/archderm....|successful treatm...| S. Vincent Rajkumar|       srajkumar| svincentrajkumar|  726470785|[28328180, 291098...|  [71924100]|         [141071460]|[2016381842, 2604...|[arnold l schroet...|[arnoldlschroeter...|            []| 145375630|38ea95b0a4e4065fe...|10.1001/ARCHDERM....|  S Vincent Rajkumar|  svincentrajkumar|           srajkumar|[Medicine]|   S625| S. Vincent Rajkumar|  svincentrajkumar|0000-0002-5862-1833|\n|2057652749|10.1001/archderm....|a congenital peri...|            Emi Dika|           edika|          emidika| 2148894812|[2910750100, 2777...|  [71924100]|         [126838900]|[2123324527, 2162...|[francesco savoia...|[francescosavoia,...|            []|   3876667|ab2daf4629048680f...|10.1001/ARCHDERM....|           Emi  Dika|           emidika|               edika|[Medicine]|   E320|            Emi Dika|           emidika|0000-0003-3186-2861|\n|2054299279|10.1001/archderm....|severe eczematous...|      Lars E. French|         lfrench|      larsefrench| 2187068216|[2911126774, 8118...|  [71924100]|[16005928, 141071...|[1979459769, 2032...|[gianluca vecchie...|[gianlucavecchiet...|            []|   4023341|44ab867d187b95b31...|10.1001/ARCHDERM....|       Lars E French|       larsefrench|             lfrench|[Medicine]|   L165|         Lars French|        larsfrench|0000-0002-4629-1486|\n|2013600953|10.1001/archderm....|lack of efficacy ...|Jean-Philippe Lacour|         jlacour|         jplacour| 2319537296|[2909675724, 2779...|  [71924100]|          [16005928]|[2424307854, 2043...|[jeanpaul ortonne...|[jeanpaulortonne,...|            []|  79319243|9fb75b28b068df86a...|10.1001/ARCHDERM....|Jean-Philippe  La...|jeanphilippelacour|             jlacour|[Medicine]|   J426|Jean-Philippe Lacour|jeanphilippelacour|0000-0001-7663-2053|\n+----------+--------------------+--------------------+--------------------+----------------+-----------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+--------------------+--------------------+------------------+--------------------+----------+-------+--------------------+------------------+-------------------+\nonly showing top 10 rows\n\nroot\n |-- magPaperId: long (nullable = true)\n |-- magDoi: string (nullable = true)\n |-- magDisplayName: string (nullable = true)\n |-- magName: string (nullable = true)\n |-- magShortNormName: string (nullable = true)\n |-- magNormName: string (nullable = true)\n |-- magAuthorId: long (nullable = true)\n |-- fosIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel0: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel1: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorNormNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- authorShortNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- affiliationIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- s2AuthorId: long (nullable = true)\n |-- s2PaperId: string (nullable = true)\n |-- s2Doi: string (nullable = true)\n |-- s2AuthorName: string (nullable = true)\n |-- s2AuthorNormName: string (nullable = true)\n |-- s2AuthorShorNormName: string (nullable = true)\n |-- s2Fos: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- soundex: string (nullable = true)\n |-- orcFullName: string (nullable = true)\n |-- orcNormName: string (nullable = true)\n |-- orcId: string (nullable = true)\n\n+----------+------+--------------------+--------------------+------------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------------+-------+-----------+-----------+-----+\n|magPaperId|magDoi|      magDisplayName|             magName|  magShortNormName|         magNormName|magAuthorId|              fosIds|fosIdsLevel0|        fosIdsLevel1|           authorIds|     authorNormNames|    authorShortNames|affiliationIds|s2AuthorId|           s2PaperId|s2Doi|        s2AuthorName|    s2AuthorNormName|s2AuthorShorNormName|              s2Fos|soundex|orcFullName|orcNormName|orcId|\n+----------+------+--------------------+--------------------+------------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------------+-------+-----------+-----------+-----+\n|2159203084|  null|regiolect veranke...|   R.W.N.M. van Hout|             rhout|         rwnmvanhout| 2614149245|                null|        null|                null|[2561534976, 2614...|[m wilting, r w n...|[mwilting, rwnmva...|            []|      null|                null| null|                null|                null|                null|               null|   R300|       null|       null| null|\n| 192730231|  null|optimierung eines...|Andreas Florian F...|          abaumann|andreasflorianfur...| 2694854556|                null|        null|                null|        [2694854556]|[andreas florian ...|[andreasflorianfu...|            []|  92071114|ebe7be6992c260dac...|     |Andreas Florian F...|andreasflorianfur...|            abaumann|        [Chemistry]|   A155|       null|       null| null|\n|2460472261|  null|enstehung der nad...|           M. Wimmer|           mwimmer|             mwimmer| 2465615284|                null|        null|                null|        [2465615284]|          [m wimmer]|           [mwimmer]|            []|  48983512|aa502923112567a8d...|     |          M.  Wimmer|             mwimmer|             mwimmer|          [Geology]|   M560|       null|       null| null|\n|1546802910|  null|international pri...|Andreas F. Lowenfeld|        alowenfeld|   andreasflowenfeld| 2000685482|[38731271, 181308...| [144133560]|[10138342, 105639...|        [2000685482]|[andreas f lowenf...| [andreasflowenfeld]|            []|  94506157|af8dd21c3f30fdca1...|     |Andreas F. Lowenfeld|   andreasflowenfeld|          alowenfeld|         [Business]|   A451|       null|       null| null|\n|2320751492|  null|leading populatio...|            Wingen M|           mwingen|             mwingen| 2666190696|[67141207, 160050...|  [17744445]|[138921699, 50522...|        [2666190696]|          [m wingen]|           [mwingen]|            []|   2235999|4d3510ecd1c894771...|     |           M  Wingen|             mwingen|             mwingen|[Political Science]|   M525|       null|       null| null|\n|2509394959|  null|carrier disk syst...|   Andreas Fogelberg|        afogelberg|    andreasfogelberg| 2813097537|[2777571299, 3537...| [127413603]|[78519656, 66938386]|        [2813097537]| [andreas fogelberg]|  [andreasfogelberg]|  [2724335451]|      null|                null| null|                null|                null|                null|               null|   A124|       null|       null| null|\n|2395269563|  null|trials of using t...|Winiarska-Majczyno M|mwiniarskamajczyno|  mwiniarskamajczyno| 2712092192|                  []|  [71924100]|          [29694066]|        [2712092192]|[m winiarskamajcz...|[mwiniarskamajczyno]|            []|1410804667|5a93e643e23bfdf3b...|     |M  Winiarska-Majc...|  mwiniarskamajczyno|  mwiniarskamajczyno|         [Medicine]|   M562|       null|       null| null|\n|3194856569|  null|synsing forkynnel...|   Andreas Follesdal|        afollesdal|    andreasfollesdal|   17514105|                null|        null|                null|          [17514105]| [andreas follesdal]|  [andreasfollesdal]|            []|      null|                null| null|                null|                null|                null|               null|   A142|       null|       null| null|\n|1414717611|  null|report on nutriti...|            M Winick|           mwinick|             mwinick| 2618223587|[2776890026, 4717...|  [71924100]|[509550671, 49774...|        [2618223587]|          [m winick]|           [mwinick]|    [78577930]|   5757558|714f13a80d46ca394...|     |           M  Winick|             mwinick|             mwinick|         [Medicine]|   M520|       null|       null| null|\n| 214193829|  null|achieving stabili...|   Andreas Follesdal|        afollesdal|    andreasfollesdal|   17514105|[2781423480, 6569...| [162324750]|[190253527, 19953...|          [17514105]| [andreas follesdal]|  [andreasfollesdal]|            []| 152437881|9b3335c87ca38f388...|     |  Andreas  Follesdal|    andreasfollesdal|          afollesdal|        [Economics]|   A142|       null|       null| null|\n+----------+------+--------------------+--------------------+------------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-----+--------------------+--------------------+--------------------+-------------------+-------+-----------+-----------+-----+\nonly showing top 10 rows\n\n\u001b[1m\u001b[34mS2_MAG_Orcid_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 23 more fields]\n\u001b[1m\u001b[34mS2_MAG_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [magPaperId: bigint, magDoi: string ... 26 more fields]\n\u001b[1m\u001b[34mS2_MAG_Orcid_all_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [magPaperId: bigint, magDoi: string ... 23 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=9"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=10"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=11"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=12"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=13"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=14"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636631788420_1667197832",
   "id": "paragraph_1636631788420_1667197832",
   "dateCreated": "2021-11-11T13:56:28+0200",
   "dateStarted": "2021-11-11T13:56:28+0200",
   "dateFinished": "2021-11-11T14:54:44+0200",
   "status": "FINISHED",
   "title": "Union remain pubs without DOI"
  },
  {
   "title": "Join ORCiD Data with MAG & S2 (try to also match with S2_doi)",
   "text": "%spark\n\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\nval orcid_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/orcid.parquet\")\n//orcid_df.show(10)\n//println(\"orcid_df diffs cnt:\"+orcid_df.count())\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\").filter($\"s2Doi\"=!=\"\"  && $\"s2Doi\".isNotNull && lower($\"s2Doi\")=!=lower($\"magDoi\")) //take into consideration the dois which are different in S2\n\n//S2_MAG_df.show(10)\n//println(\"s2doi diffs cnt:\"+S2_MAG_df.count())\n// 1.150.573\n\nval S2_MAG_Orcid_df2 = S2_MAG_df \n                    .join(orcid_df, (lower(S2_MAG_df(\"s2Doi\")) === lower(orcid_df(\"doi\")))\n                            && ( S2_MAG_df(\"s2AuthorNormName\")===orcid_df(\"ORCshortNormName\") ||  S2_MAG_df(\"magNormName\")===orcid_df(\"ORCshortNormName\") ||  ( jaro_winkler($\"magShortNormName\", orcid_df(\"ORCnormName\")) > 0.82 && levenshtein($\"magShortNormName\", orcid_df(\"ORCnormName\"))<3 && soundex($\"magShortNormName\") === soundex(orcid_df(\"ORCnormName\")) ) )                   \n                   , \"left_outer\")\n        .select(S2_MAG_df(\"magPaperId\"), S2_MAG_df(\"magDoi\"),  S2_MAG_df(\"magDisplayName\"), S2_MAG_df(\"magName\"), S2_MAG_df(\"magNormName\"), \n                            S2_MAG_df(\"magShortNormName\"),  S2_MAG_df(\"magAuthorId\"), S2_MAG_df(\"fosIds\"), S2_MAG_df(\"fosIdsLevel0\") ,S2_MAG_df(\"fosIdsLevel1\"),      S2_MAG_df(\"authorIds\"), S2_MAG_df(\"authorNormNames\"), S2_MAG_df(\"authorShortNames\"),S2_MAG_df(\"affiliationIds\"), \n                            S2_MAG_df(\"s2AuthorId\"),S2_MAG_df(\"s2PaperId\"), S2_MAG_df(\"s2Doi\"), S2_MAG_df(\"s2AuthorName\"), S2_MAG_df(\"s2AuthorShorNormName\"), S2_MAG_df(\"s2AuthorNormName\"), S2_MAG_df(\"s2Fos\"), S2_MAG_df(\"soundex\"),orcid_df(\"ORCfullName\").as(\"orcFullName\"),orcid_df(\"ORCnormName\").as(\"orcNormName\"), orcid_df(\"orcId\") )\n  \n                   \nS2_MAG_Orcid_df2.write.mode(\"overwrite\").parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid2021_s2doi.parquet\") \n                            \n                \n    \n              \n",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T13:37:22+0200",
   "progress": 99.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "editorHide": false,
    "title": true,
    "results": {
     "0": {
      "graph": {
       "keys": [
        {
         "name": "magDoi",
         "index": 1.0,
         "aggr": "sum"
        }
       ],
       "groups": [],
       "values": [
        {
         "name": "magPaperId",
         "index": 0.0,
         "aggr": "avg"
        }
       ],
       "setting": {
        "lineChart": {}
       },
       "mode": "lineChart"
      }
     }
    },
    "enabled": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.concat_ws\nimport org.apache.spark.sql.functions.countDistinct\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\n\u001b[1m\u001b[34morcid_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [doi: string, ORCfullName: string ... 4 more fields]\n\u001b[1m\u001b[34mS2_MAG_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [magPaperId: bigint, magDoi: string ....\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636717041968_2045444905",
   "id": "paragraph_1636717041968_2045444905",
   "dateCreated": "2021-11-12T13:37:21+0200",
   "dateStarted": "2021-11-12T13:37:22+0200",
   "dateFinished": "2021-11-12T13:57:52+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval S2_MAG_Orcid_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_all_2021.parquet\")\n\nprintln(\"MAG_pub_authorsdf  cnt:\"+S2_MAG_Orcid_df.count())\n//S2_MAG_Orcid_S2doi.printSchema\nprintln(\"S2_MAG_Orcid_orcid cnt:\" + S2_MAG_Orcid_df.filter($\"orcId\"=!=\"\"  && $\"orcId\".isNotNull).count())\n//5175070\nS2_MAG_Orcid_df.select(countDistinct(\"s2PaperId\")).show(false)\n//168307507\nS2_MAG_Orcid_df.select(countDistinct(\"MAGpaperId\")).show(false)\n//267547151\nS2_MAG_Orcid_df.select(countDistinct(\"s2AuthorId\")).show(false)\n//64022040\nS2_MAG_Orcid_df.select(countDistinct(\"magAuthorId\")).show(false)\n//280078732\nS2_MAG_Orcid_df.select(countDistinct(\"orcId\")).show(false)\n\n\nval S2_MAG_Orcid_df_s2doi = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid2021_s2doi.parquet\")\n\nprintln(\"MAG_pub_authorsdf doi cnt:\"+S2_MAG_Orcid_df_s2doi.count())\nS2_MAG_Orcid_df_s2doi.select(countDistinct(\"s2PaperId\")).show(false)\n//168307507\nS2_MAG_Orcid_df_s2doi.select(countDistinct(\"MAGpaperId\")).show(false)\n//267547151\nS2_MAG_Orcid_df_s2doi.select(countDistinct(\"s2AuthorId\")).show(false)\n//64022040\nS2_MAG_Orcid_df_s2doi.select(countDistinct(\"magAuthorId\")).show(false)\n//280078732\n\nS2_MAG_Orcid_df_s2doi.select(countDistinct(\"orcId\")).show(false)\n//280078732\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T14:06:08+0200",
   "progress": 0.0,
   "config": {
    "results": [
     {}
    ],
    "editorHide": false,
    "title": true,
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "size": {
        "height": 3237.0
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "MAG_pub_authorsdf  cnt:721177821\nS2_MAG_Orcid_orcid cnt:39602641\n+-------------------------+\n|count(DISTINCT s2PaperId)|\n+-------------------------+\n|168307507                |\n+-------------------------+\n\n+--------------------------+\n|count(DISTINCT MAGpaperId)|\n+--------------------------+\n|267547151                 |\n+--------------------------+\n\n+--------------------------+\n|count(DISTINCT s2AuthorId)|\n+--------------------------+\n|64022040                  |\n+--------------------------+\n\n+---------------------------+\n|count(DISTINCT magAuthorId)|\n+---------------------------+\n|280078732                  |\n+---------------------------+\n\n+---------------------+\n|count(DISTINCT orcId)|\n+---------------------+\n|2667701              |\n+---------------------+\n\nMAG_pub_authorsdf doi cnt:1150789\n+-------------------------+\n|count(DISTINCT s2PaperId)|\n+-------------------------+\n|410644                   |\n+-------------------------+\n\n+--------------------------+\n|count(DISTINCT MAGpaperId)|\n+--------------------------+\n|410644                    |\n+--------------------------+\n\n+--------------------------+\n|count(DISTINCT s2AuthorId)|\n+--------------------------+\n|851834                    |\n+--------------------------+\n\n+---------------------------+\n|count(DISTINCT magAuthorId)|\n+---------------------------+\n|917033                     |\n+---------------------------+\n\n+---------------------+\n|count(DISTINCT orcId)|\n+---------------------+\n|50889                |\n+---------------------+\n\n\u001b[1m\u001b[34mS2_MAG_Orcid_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 23 more fields]\n\u001b[1m\u001b[34mS2_MAG_Orcid_df_s2doi\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 23 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=4"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=5"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=6"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=7"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=10"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=11"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=12"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=13"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=14"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=15"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=16"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=17"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636718768620_29258625",
   "id": "paragraph_1636718768620_29258625",
   "dateCreated": "2021-11-12T14:06:08+0200",
   "dateStarted": "2021-11-12T14:06:08+0200",
   "dateFinished": "2021-11-12T14:09:29+0200",
   "status": "FINISHED",
   "title": "Analyze counts & Stats on matched datasets"
  },
  {
   "text": "%spark\nval S2_MAG_Orcid_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_all_2021.parquet\")\n//S2_MAG_Orcid_df.printSchema\n\n//println(\"MAG_pub_authorsdf  cnt:\"+S2_MAG_Orcid_df.count())\n//721177821\n//println(\"MAG_pub_authorsdf multi-discipline cnt:\"+S2_MAG_Orcid_df.filter(size($\"fosIdsLevel0\")>1).count())\n//110360\n//println(\"MAG_pub_authorsdf multi-discipline cnt:\"+S2_MAG_Orcid_df.filter(size($\"fosIdsLevel0\")===1).count())\n//601000204\n\nval signGroupByshortNameFoSdf =  S2_MAG_Orcid_df.groupBy($\"magShortNormName\")\n        .agg(\n                    count(\"magPaperId\").as(\"cnt\")\n        )        \n\nprintln(\"[ 1K - 10K]:\",signGroupByshortNameFoSdf.filter($\"cnt\">1000 && $\"cnt\"<10000).count())\nprintln(\"[10K - 50K]:\",signGroupByshortNameFoSdf.filter($\"cnt\">10000 && $\"cnt\"<50000).count())\nprintln(\"[50K - 100K]:\",signGroupByshortNameFoSdf.filter($\"cnt\">50000 && $\"cnt\"<100000).count())\nprintln(\"[100K - 200K]:\",signGroupByshortNameFoSdf.filter($\"cnt\">100000 && $\"cnt\"< 200000).count())\nprintln(\">200.000:\",signGroupByshortNameFoSdf.filter($\"cnt\">200000).count())\n//(>10.000 cnt:,2632)\n//(>50.000 cnt:,291)\n//(>100.000 cnt:,81)\n\n//(>200.000 cnt:,17)\n\n\n//S2_MAG_Orcid_df.printSchema\n//println(\"S2_MAG_Orcid_orcid cnt:\" + S2_MAG_Orcid_df.filter($\"orcId\"=!=\"\"  && $\"orcId\".isNotNull).count())\n//5175070\n//S2_MAG_Orcid_df.select(countDistinct(\"s2PaperId\")).show(false)\n//168307507\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-10T12:58:08+0200",
   "progress": 99.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "([ 1K - 10K]:,65271)\n([10K - 50K]:,4007)\n([50K - 100K]:,396)\n([100K - 200K]:,153)\n(>200.000:,105)\n\u001b[1m\u001b[34mS2_MAG_Orcid_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 23 more fields]\n\u001b[1m\u001b[34msignGroupByshortNameFoSdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magShortNormName: string, cnt: bigint]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=4"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=5"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644490688245_1583646021",
   "id": "paragraph_1644490688245_1583646021",
   "dateCreated": "2022-02-10T12:58:08+0200",
   "dateStarted": "2022-02-10T12:58:08+0200",
   "dateFinished": "2022-02-10T13:03:13+0200",
   "status": "FINISHED"
  },
  {
   "title": "Match Hindawi Data",
   "text": "%spark\n\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\n\nval HIND_HOME = \"/home/ometaxas/Datasets/Hindawi\"\n\nval hind_author_pub_df = spark.read\n           .options(Map(\"header\"-> \"true\")).\n             //.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n               // schema(authorSchema).\n                csv(s\"file://$HIND_HOME/author_pub.csv\")\n      //  .filter($\"article_title\"===\"0\" || $\"hindaw_id\".isNull || $\"author_surname\".isNull )\n        //.filter($\"author_full_name\".isNotNull)\n        /*\n        .select(lower($\"DOI\").as(\"doi\"), $\"article_title\",$\"journal_name\", substring($\"published_date\",0,4).as(\"pub_year\"), \n$\"author_full_name\", $\"author_firstname\", $\"author_surname\", $\"orcid\".as(\"hind_orcId\"), $\"hindaw_id\",\nshortNormName($\"author_full_name\").as(\"hind_shortNormName\"), normName($\"author_full_name\").as(\"hind_normName\"))\n*/\n//hind_author_pub_df.printSchema()\nprintln(hind_author_pub_df.count())\n\nhind_author_pub_df.select(countDistinct(\"hindaw_id\")).show(false)\n//hind_author_pub_df.show(100)\n\n\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_all_2021.parquet\").filter($\"magDoi\"=!=\"\"  && $\"magDoi\".isNotNull)\n\nval S2_MAG_Orcid_Hind_df = S2_MAG_df\n                    .join(broadcast(hind_author_pub_df), lower(S2_MAG_df(\"magDoi\")) === lower(hind_author_pub_df(\"doi\")) , \"inner\")\n        .filter( S2_MAG_df(\"s2AuthorShorNormName\")===hind_author_pub_df(\"hind_shortNormName\") ||  S2_MAG_df(\"magShortNormName\")===hind_author_pub_df(\"hind_shortNormName\") ||  \n                    ( jaro_winkler($\"magNormName\", hind_author_pub_df(\"hind_normName\")) > 0.82 && levenshtein($\"magNormName\", hind_author_pub_df(\"hind_normName\"))<3 && soundex($\"magNormName\") === soundex(hind_author_pub_df(\"hind_normName\")) ))\n        .cache()\n*/\nval S2_MAG_Orcid_Hind_df = hind_author_pub_df\n                    .join(S2_MAG_df, lower(S2_MAG_df(\"magDoi\")) === lower(hind_author_pub_df(\"doi\")) , \"left_anti\")\n        /*.filter( S2_MAG_df(\"s2AuthorShorNormName\")===hind_author_pub_df(\"hind_shortNormName\") ||  S2_MAG_df(\"magShortNormName\")===hind_author_pub_df(\"hind_shortNormName\") ||  \n                    ( jaro_winkler($\"magNormName\", hind_author_pub_df(\"hind_normName\")) > 0.82 && levenshtein($\"magNormName\", hind_author_pub_df(\"hind_normName\"))<3 && soundex($\"magNormName\") === soundex(hind_author_pub_df(\"hind_normName\")) ))*/\n        .cache()\n        /*\n                     .select(S2_MAG_df(\"magPaperId\"), S2_MAG_df(\"magDoi\"),  S2_MAG_df(\"magDisplayName\"), S2_MAG_df(\"magName\"), S2_MAG_df(\"magNormName\"), \n                            S2_MAG_df(\"magShortNormName\"),  S2_MAG_df(\"magAuthorId\"), S2_MAG_df(\"fosIds\"), S2_MAG_df(\"fosIdsLevel0\") ,S2_MAG_df(\"fosIdsLevel1\"), \n                            S2_MAG_df(\"authorIds\"), S2_MAG_df(\"authorNormNames\"), S2_MAG_df(\"authorShortNames\"),S2_MAG_df(\"affiliationIds\"), \n                            S2_MAG_df(\"s2AuthorId\"),S2_MAG_df(\"s2PaperId\"), S2_MAG_df(\"s2Doi\"), S2_MAG_df(\"s2AuthorName\"), S2_MAG_df(\"s2AuthorShorNormName\"), S2_MAG_df(\"s2AuthorNormName\"), S2_MAG_df(\"s2Fos\"), S2_MAG_df(\"soundex\"),orcid_df(\"ORCfullName\").as(\"orcFullName\"),orcid_df(\"ORCnormName\").as(\"orcNormName\"), orcid_df(\"orcId\") )\n*/\n\nS2_MAG_Orcid_Hind_df.printSchema()\nS2_MAG_Orcid_Hind_df.show(5)\nprintln(S2_MAG_Orcid_Hind_df.count())\n\nS2_MAG_Orcid_Hind_df.write.mode(\"overwrite\").parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_Hind_df_anti.parquet\")\n\n\n//1087172\n//1022215",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T17:30:09+0200",
   "progress": 7.0,
   "config": {
    "editorHide": false,
    "title": true,
    "results": [
     {}
    ],
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "size": {
        "height": 221.0
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "1087172\n+-------------------------+\n|count(DISTINCT hindaw_id)|\n+-------------------------+\n|729934                   |\n+-------------------------+\n\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.concat_ws\nimport org.apache.spark.sql.functions.countDistinct\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\n\u001b[1m\u001b[34mHIND_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /home/ometaxas/Datasets/Hindawi\n\u001b[1m\u001b[34mhind_author_pub_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [DOI: string, article_title: string ... 7 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=153"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=154"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=155"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636731009878_153914531",
   "id": "paragraph_1636731009878_153914531",
   "dateCreated": "2021-11-12T17:30:09+0200",
   "dateStarted": "2021-11-12T17:30:09+0200",
   "dateFinished": "2021-11-12T17:30:11+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval S2_MAG_Orcid_Hind_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_Hind_df.parquet\")\n//.filter($\"article_title\"=!=\"0\")\n\nS2_MAG_Orcid_Hind_df.show(20)\n\n\n\n/*\nprintln(S2_MAG_Orcid_Hind_df.count())\n//1022215\n//S2_MAG_Orcid_Hind_df.printSchema()\n\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"s2PaperId\")).show(false)\n//194400\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"MAGpaperId\")).show(false)\n//211241\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"s2AuthorId\")).show(false)\n//606925\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"magAuthorId\")).show(false)\n//713054\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"orcId\")).show(false)\n//134388\nS2_MAG_Orcid_Hind_df.select(countDistinct(\"hindaw_id\")).show(false)\n//700790\n*/\nval hindAuthorLinks_df = S2_MAG_Orcid_Hind_df\n                .groupBy($\"hindaw_id\", $\"author_full_name\")                \n                .agg(\n                    count(\"orcId\").as(\"orcIdCnt\"),\n                    count(\"S2authorId\").as(\"S2authorIdCnt\"),\n                    count(\"magAuthorId\").as(\"magAuthorIdCnt\"),\n                collect_set(trim(S2_MAG_Orcid_Hind_df(\"S2authorId\"))).as(\"S2authorIds\"), collect_set(trim(S2_MAG_Orcid_Hind_df(\"magAuthorId\"))).as(\"magAuthorIdIds\"), collect_set(trim(S2_MAG_Orcid_Hind_df(\"orcId\"))).as(\"orcIds\"))\n              //  .where($\"orcIdCnt\">1).cache()\n  //              .persist(StorageLevel.DISK_ON\n\n\nprintln(hindAuthorLinks_df.count())\n\nprintln(hindAuthorLinks_df.where($\"orcIdCnt\">0).count())\nprintln(hindAuthorLinks_df.where($\"S2authorIdCnt\">0).count())\nprintln(hindAuthorLinks_df.where($\"magAuthorIdCnt\">0).count())\n//println(hindAuthorLinks_df.where($\"hindaw_id\".isNull).count())\n\n//println(hindAuthorLinks_df.show(10))\n\n//println(hindAuthorLinks_df.filter($\"hindaw_id\".isNotNull).show(10))\n\n//hindAuthorLinks_df.coalesce(1).write.mode(\"overwrite\").json(\"/media/ometaxas/nvme/datasets/MAG_S2/hindAuthorLinks_df.json\")\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T17:49:57+0200",
   "progress": 76.0,
   "config": {
    "results": [
     {
      "mode": "table"
     }
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----------+-------------------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------+---------------+--------------+-------------------+-------------------+--------------------+--------------------+--------+--------------------+----------------+--------------+----------+--------------------+------------------+--------------------+\n|magPaperId|             magDoi|      magDisplayName|             magName|magShortNormName|         magNormName|magAuthorId|              fosIds|fosIdsLevel0|        fosIdsLevel1|           authorIds|     authorNormNames|    authorShortNames|affiliationIds|s2AuthorId|           s2PaperId|              s2Doi|        s2AuthorName|    s2AuthorNormName|s2AuthorShorNormName|              s2Fos|soundex|    orcFullName|   orcNormName|              orcId|                doi|       article_title|        journal_name|pub_year|    author_full_name|author_firstname|author_surname|hind_orcId|           hindaw_id|hind_shortNormName|       hind_normName|\n+----------+-------------------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------+---------------+--------------+-------------------+-------------------+--------------------+--------------------+--------+--------------------+----------------+--------------+----------+--------------------+------------------+--------------------+\n|2064185560|10.1155/2008/257691|zeolite encapsula...|     Kasi Pitchumani|     kpitchumani|      kasipitchumani|  128748401|[2780357685, 1617...| [192562407]|[42360764, 179104...|[2294173429, 5155...|[thirumeni subram...|[thirumenisubrama...|    [13268429]|   5552132|c6ee46c81ae4f684a...|10.1155/2008/257691|    Kasi  Pitchumani|      kasipitchumani|         kpitchumani|[Materials Science]|   K132|Kasi Pitchumani|kasipitchumani|0000-0001-6216-245X|10.1155/2008/257691|Zeolite Encapsula...|Journal of Nanoma...|    2008|     Kasi Pitchumani|            Kasi|    Pitchumani|      null|dbabd3fe-46a1-444...|       kpitchumani|      kasipitchumani|\n|2064185560|10.1155/2008/257691|zeolite encapsula...|Sakthivel Vijaikumar|     svijaikumar| sakthivelvijaikumar|   51550653|[2780357685, 1617...| [192562407]|[42360764, 179104...|[2294173429, 5155...|[thirumeni subram...|[thirumenisubrama...|    [13268429]|  13879465|c6ee46c81ae4f684a...|10.1155/2008/257691|Sakthivel  Vijaik...| sakthivelvijaikumar|         svijaikumar|[Materials Science]|   S122|           null|          null|               null|10.1155/2008/257691|Zeolite Encapsula...|Journal of Nanoma...|    2008|Sakthivel vijaikumar|       Sakthivel|    vijaikumar|      null|cb0f56a7-c744-418...|       svijaikumar| sakthivelvijaikumar|\n|2064185560|10.1155/2008/257691|zeolite encapsula...|Thirumeni Subrama...|    tsubramanian|thirumenisubramanian| 2294173429|[2780357685, 1617...| [192562407]|[42360764, 179104...|[2294173429, 5155...|[thirumeni subram...|[thirumenisubrama...|    [13268429]|  31908148|c6ee46c81ae4f684a...|10.1155/2008/257691|Thirumeni  Subram...|thirumenisubramanian|        tsubramanian|[Materials Science]|   T216|           null|          null|               null|10.1155/2008/257691|Zeolite Encapsula...|Journal of Nanoma...|    2008|Thirumeni Subrama...|       Thirumeni|   Subramanian|      null|02b4e9de-924b-4ca...|      tsubramanian|thirumenisubramanian|\n|2049977607|10.1155/2008/754190|deficiency in pol...|Anton I. Ovsyannikov|    aovsyannikov|   antoniovsyannikov| 2566379125|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|  39972802|bda5e44af3b96bbff...|10.1155/2008/754190|Anton I. Ovsyannikov|   antoniovsyannikov|        aovsyannikov|[Medicine, Biology]|   A125|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|Anton I. Ovsyannikov|        Anton I.|   Ovsyannikov|      null|0774ddbe-910d-4b3...|      aovsyannikov|   antoniovsyannikov|\n|2049977607|10.1155/2008/754190|deficiency in pol...| Anna V. Semenchenko|    asemenchenko|    annavsemenchenko| 1258055198|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   4196903|bda5e44af3b96bbff...|10.1155/2008/754190| Anna V. Semenchenko|    annavsemenchenko|        asemenchenko|[Medicine, Biology]|   A255|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008| Anna V. Semenchenko|         Anna V.|   Semenchenko|      null|45cb0c7a-3979-4be...|      asemenchenko|    annavsemenchenko|\n|2049977607|10.1155/2008/754190|deficiency in pol...|   Irina G. Popovich|       ipopovich|          igpopovich| 1987693256|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   4615353|bda5e44af3b96bbff...|10.1155/2008/754190|   Irina G. Popovich|      irinagpopovich|           ipopovich|[Medicine, Biology]|   I111|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|   Irina G. Popovich|        Irina G.|      Popovich|      null|bbe9ec1c-4e76-443...|         ipopovich|      irinagpopovich|\n|2049977607|10.1155/2008/754190|deficiency in pol...| Mark A. Zabezhinski|    mzabezhinski|    markazabezhinski|  384259489|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   4788373|bda5e44af3b96bbff...|10.1155/2008/754190| Mark A. Zabezhinski|    markazabezhinski|        mzabezhinski|[Medicine, Biology]|   M212|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008| Mark A. Zabezhinski|         Mark A.|   Zabezhinski|      null|9a214d1e-c110-470...|      mzabezhinski|    markazabezhinski|\n|2049977607|10.1155/2008/754190|deficiency in pol...|     Maria N. Yurova|         myurova|        marianyurova| 2114975053|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []| 120097423|bda5e44af3b96bbff...|10.1155/2008/754190|     Maria N. Yurova|        marianyurova|             myurova|[Medicine, Biology]|   M610|   Maria Yurova|   mariayurova|0000-0003-3589-5871|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|     Maria N. Yurova|        Maria N.|        Yurova|      null|964f90d6-f040-44b...|           myurova|        marianyurova|\n|2049977607|10.1155/2008/754190|deficiency in pol...|Tatiana S. Piskunova|      tpiskunova|   tatianaspiskunova| 2130929180|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   4675425|bda5e44af3b96bbff...|10.1155/2008/754190|Tatiana S. Piskunova|   tatianaspiskunova|          tpiskunova|[Medicine, Biology]|   T125|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|Tatiana S. Piskunova|      Tatiana S.|     Piskunova|      null|4413ec1a-6dc7-484...|        tpiskunova|   tatianaspiskunova|\n|2049977607|10.1155/2008/754190|deficiency in pol...|Vladimir N. Anisimov|       vanisimov|          vnanisimov| 2630999395|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   5127911|bda5e44af3b96bbff...|10.1155/2008/754190|Vladimir N. Anisimov|   vladimirnanisimov|           vanisimov|[Medicine, Biology]|   V525|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|   Vladimir Anisimov|        Vladimir|      Anisimov|      null|d5afb248-26a8-41b...|         vanisimov|    vladimiranisimov|\n|2049977607|10.1155/2008/754190|deficiency in pol...|        Zhao-Qi Wang|           zwang|          zhaoqiwang| 2562816852|[63645605, 182979...|  [86803240]|[95444343, 203014...|[1258055198, 3842...|[maria n yurova, ...|[marianyurova, an...|            []|   2358276|bda5e44af3b96bbff...|10.1155/2008/754190|       Zhao-Qi  Wang|          zhaoqiwang|               zwang|[Medicine, Biology]|   Z520|           null|          null|               null|10.1155/2008/754190|Deficiency in Pol...|Current Gerontolo...|    2008|        Zhao Qi Wang|         Zhao Qi|          Wang|      null|377137c6-7669-431...|             zwang|          zhaoqiwang|\n|2104938386|10.1155/2009/120213|the solution of e...|     Reinhard Starkl|         rstarkl|      reinhardstarkl| 2647605715|[158622935, 18965...|  [33923547]|[134306372, 12556...|        [2647605715]|   [reinhard starkl]|    [reinhardstarkl]|            []| 102513051|f9bfb90dd2d8d27a2...|10.1155/2009/120213|    Reinhard  Starkl|      reinhardstarkl|             rstarkl|      [Mathematics]|   R236|           null|          null|               null|10.1155/2009/120213|The Solution of E...|Advances in Mathe...|    2010|     Reinhard Starkl|        Reinhard|        Starkl|      null|3ad1c813-5cc8-4ee...|           rstarkl|      reinhardstarkl|\n|2088440054|10.1155/2009/194148|exploiting redund...|        Peter Farkas|         pfarkas|         peterfarkas| 2803270717|[148063708, 10308...|  [41008148]| [9390403, 79403827]|[2803270717, 1468...|[peter farkas, to...|[peterfarkas, tom...|            []| 143601784|60c5979e3c0d685c4...|10.1155/2009/194148|       Peter  Farkas|         peterfarkas|             pfarkas| [Computer Science]|   P622|           null|          null|               null|10.1155/2009/194148|Exploiting Redund...|International Jou...|    2009|        Peter Farkas|           Peter|        Farkas|      null|611602b1-d9c1-4ed...|           pfarkas|         peterfarkas|\n|2088440054|10.1155/2009/194148|exploiting redund...|       Tomas Palenik|        tpalenik|        tomaspalenik| 1468179530|[148063708, 10308...|  [41008148]| [9390403, 79403827]|[2803270717, 1468...|[peter farkas, to...|[peterfarkas, tom...|            []|  50474249|60c5979e3c0d685c4...|10.1155/2009/194148|      Tomas  Palenik|        tomaspalenik|            tpalenik| [Computer Science]|   T145|           null|          null|               null|10.1155/2009/194148|Exploiting Redund...|International Jou...|    2009|       Tomas Palenik|           Tomas|       Palenik|      null|3b75fb58-1e42-4b9...|          tpalenik|        tomaspalenik|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|     Ann G. Schwartz|       aschwartz|        anngschwartz| 2157590450|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|            []|   5438714|6d69ffe5da4867966...|10.1155/2009/242151|     Ann G. Schwartz|        anngschwartz|           aschwartz|         [Medicine]|   A263|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|     Ann G. Schwartz|          Ann G.|      Schwartz|      null|6c62868b-39b3-43d...|         aschwartz|        anngschwartz|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|  Alison L. Van Dyke|           adyke|      alisonlvandyke| 2078079585|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|   [185443292]|   4099129|6d69ffe5da4867966...|10.1155/2009/242151|  Alison L. Van Dyke|      alisonlvandyke|               adyke|         [Medicine]|   A320|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|  Alison L. Van Dyke|       Alison L.|      Van Dyke|      null|287992fd-8bcd-452...|             adyke|      alisonlvandyke|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|  Angela S. Wenzlaff|       awenzlaff|     angelaswenzlaff| 1916615841|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|            []|   6846117|6d69ffe5da4867966...|10.1155/2009/242151|  Angela S. Wenzlaff|     angelaswenzlaff|           awenzlaff|         [Medicine]|   A524|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|  Angela S. Wenzlaff|       Angela S.|      Wenzlaff|      null|d29c630d-5e99-4ae...|         awenzlaff|     angelaswenzlaff|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|       Judith Abrams|         jabrams|        judithabrams| 2149615634|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|            []| 144418175|6d69ffe5da4867966...|10.1155/2009/242151|      Judith  Abrams|        judithabrams|             jabrams|         [Medicine]|   J165|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|       Judith Abrams|          Judith|        Abrams|      null|c161e6ff-953a-44a...|           jabrams|        judithabrams|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|     Michele L. Cote|           mcote|        michelelcote| 2107734661|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|            []|   2861792|6d69ffe5da4867966...|10.1155/2009/242151|     Michele L. Cote|        michelelcote|               mcote|         [Medicine]|   M230|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|     Michele L. Cote|      Michele L.|          Cote|      null|bae62e59-d387-4f6...|             mcote|        michelelcote|\n|2003033104|10.1155/2009/242151|chromosome 5p reg...|     Priyanka T Iyer|           piyer|       priyankatiyer| 2404457574|[2776256026, 1977...|  [71924100]|[126322002, 54355...|[2162106694, 2107...|[angela s wenzlaf...|[angelaswenzlaff,...|            []|  29796130|6d69ffe5da4867966...|10.1155/2009/242151|      Priyanka  Iyer|        priyankaiyer|               piyer|         [Medicine]|   P600|           null|          null|               null|10.1155/2009/242151|Chromosome 5p Reg...|Journal of Cancer...|    2010|       Priyanka Iyer|        Priyanka|          Iyer|      null|8321ef09-311c-4c4...|             piyer|        priyankaiyer|\n+----------+-------------------+--------------------+--------------------+----------------+--------------------+-----------+--------------------+------------+--------------------+--------------------+--------------------+--------------------+--------------+----------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------+-------+---------------+--------------+-------------------+-------------------+--------------------+--------------------+--------+--------------------+----------------+--------------+----------+--------------------+------------------+--------------------+\nonly showing top 20 rows\n\n706856\n141992\n636421\n706856\n\u001b[1m\u001b[34mS2_MAG_Orcid_Hind_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 34 more fields]\n\u001b[1m\u001b[34mhindAuthorLinks_df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [hindaw_id: string, author_full_name: string ... 6 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=162"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=163"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=164"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=165"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=166"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=167"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636732197323_2049676654",
   "id": "paragraph_1636732197323_2049676654",
   "dateCreated": "2021-11-12T17:49:57+0200",
   "dateStarted": "2021-11-12T17:49:57+0200",
   "dateFinished": "2021-11-12T17:50:03+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval S2_MAG_Orciddf = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\")\n        //.cache()\n//S2_MAG_Orciddf.show(20)\nS2_MAG_Orciddf.printSchema()\nprintln(\"MAG_pub_authorsdf  cnt:\"+S2_MAG_Orciddf.count())\n\nS2_MAG_Orciddf.select(countDistinct(\"s2PaperId\")).show(false)\n//168307507\nS2_MAG_Orciddf.select(countDistinct(\"MAGpaperId\")).show(false)\n//267547151\n\nS2_MAG_Orciddf.select(countDistinct(\"s2AuthorId\")).show(false)\n//64022040\n\nS2_MAG_Orciddf.select(countDistinct(\"magAuthorId\")).show(false)\n//280078732\n\n//println(\"MAG_pub_authorsdf no duplicates cnt:\"+S2_MAG_Orciddf.dropDuplicates().count())\n//CPU 2m23\n//GPU 3m 7s //2 tasks per GPU 2m 9s //3tasksperGPU 1.47//4tasks 1.43\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-10T13:04:25+0200",
   "progress": 98.0,
   "config": {
    "results": [
     {}
    ],
    "editorHide": false,
    "tableHide": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "root\n |-- magPaperId: long (nullable = true)\n |-- magDoi: string (nullable = true)\n |-- magDisplayName: string (nullable = true)\n |-- magName: string (nullable = true)\n |-- magNormName: string (nullable = true)\n |-- magShortNormName: string (nullable = true)\n |-- magAuthorId: long (nullable = true)\n |-- fosIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel0: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosIdsLevel1: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorNormNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- authorShortNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- affiliationIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- s2AuthorId: long (nullable = true)\n |-- s2PaperId: string (nullable = true)\n |-- s2Doi: string (nullable = true)\n |-- s2AuthorName: string (nullable = true)\n |-- s2AuthorShorNormName: string (nullable = true)\n |-- s2AuthorNormName: string (nullable = true)\n |-- s2Fos: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- soundex: string (nullable = true)\n |-- doi: string (nullable = true)\n |-- orcFullName: string (nullable = true)\n |-- orcShortNormName: string (nullable = true)\n |-- orcNormName: string (nullable = true)\n |-- pmId: string (nullable = true)\n |-- orcId: string (nullable = true)\n\nMAG_pub_authorsdf  cnt:721000253\n+-------------------------+\n|count(DISTINCT s2PaperId)|\n+-------------------------+\n|168307507                |\n+-------------------------+\n\n+--------------------------+\n|count(DISTINCT MAGpaperId)|\n+--------------------------+\n|267547151                 |\n+--------------------------+\n\n+--------------------------+\n|count(DISTINCT s2AuthorId)|\n+--------------------------+\n|64022040                  |\n+--------------------------+\n\n+---------------------------+\n|count(DISTINCT magAuthorId)|\n+---------------------------+\n|280078732                  |\n+---------------------------+\n\n\u001b[1m\u001b[34mS2_MAG_Orciddf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [magPaperId: bigint, magDoi: string ... 26 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=4"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=5"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636542265262_112250905",
   "id": "paragraph_1636542265262_112250905",
   "dateCreated": "2021-11-10T13:04:25+0200",
   "dateStarted": "2021-11-10T13:04:25+0200",
   "dateFinished": "2021-11-10T13:06:08+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval S2_MAG_Orciddf_old = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid.parquet\")\n        //.cache()\n//S2_MAG_Orciddf_old.show(20)\nS2_MAG_Orciddf_old.printSchema()\nprintln(\"MAG_pub_authorsdf  cnt:\"+S2_MAG_Orciddf_old.count())\n//println(\"MAG_pub_authorsdf no duplicates cnt:\"+S2_MAG_Orciddf.dropDuplicates().count())\n\n\n\n\n//S2_MAG_Orcid_S2doi cnt:816642169\n//                        49376386\n// \n\n//S2_MAG_Orcid_df1.write.parquet(\"/media/datadisk/Datasets/MAG_S2/S2_MAG_Orcid_S2doi.parquet\")      ",
   "user": "anonymous",
   "dateUpdated": "2021-11-09T17:21:30+0200",
   "progress": 99.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "results": {},
    "enabled": true,
    "editorHide": false,
    "tableHide": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true,
       "state": {}
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "root\n |-- MAGpaperId: long (nullable = true)\n |-- MAGdoi: string (nullable = true)\n |-- MAGdisplayName: string (nullable = true)\n |-- MAGname: string (nullable = true)\n |-- MAGnormName: string (nullable = true)\n |-- MAGshortNormName: string (nullable = true)\n |-- MAGauthorId: long (nullable = true)\n |-- fosids: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosids_lvl0: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fosids_lvl1: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- authorNormNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- authorShortNormNames: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- affiliationIds: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- S2authorId: string (nullable = true)\n |-- S2paperId: string (nullable = true)\n |-- S2doi: string (nullable = true)\n |-- S2name: string (nullable = true)\n |-- S2shortNormName: string (nullable = true)\n |-- S2normName: string (nullable = true)\n |-- S2fos: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- orcId: string (nullable = true)\n\nMAG_pub_authorsdf  cnt:661651048\n\u001b[1m\u001b[34mS2_MAG_Orciddf_old\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [MAGpaperId: bigint, MAGdoi: string ... 20 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4041/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4041/jobs/job?id=3"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636471290239_1939839210",
   "id": "paragraph_1636471290239_1939839210",
   "dateCreated": "2021-11-09T17:21:30+0200",
   "dateStarted": "2021-11-09T17:21:30+0200",
   "dateFinished": "2021-11-09T17:21:34+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval S2_MAG_Orciddf = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\")\nval S2_MAG_Orcid_orcid = S2_MAG_Orciddf.filter($\"orcId\"=!=\"\"  && $\"orcId\".isNotNull)\n.cache()\n\n//S2_MAG_Orcid_S2doi.printSchema\nprintln(\"S2_MAG_Orcid_orcid cnt:\" + S2_MAG_Orcid_orcid.count())\n//S2_MAG_Orcid_S2doi.show(40)\n\nS2_MAG_Orcid_orcid.select(countDistinct(\"orcId\")).show(false)\nS2_MAG_Orcid_orcid.select(countDistinct(\"MAGpaperId\")).show(false)",
   "user": "anonymous",
   "dateUpdated": "2022-02-10T12:57:10+0200",
   "progress": 0.0,
   "config": {
    "editorHide": false,
    "tableHide": true,
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true,
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/media/ometaxas/nvme/datasets/MAG_S2/authorMagS2OrcidLarge\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:806)\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:803)\n  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644490630196_1013929625",
   "id": "paragraph_1644490630196_1013929625",
   "dateCreated": "2022-02-10T12:57:10+0200",
   "dateStarted": "2022-02-10T12:57:10+0200",
   "dateFinished": "2022-02-10T12:57:10+0200",
   "status": "ERROR"
  },
  {
   "text": "%spark\nval S2_MAG_Orciddf_old = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid.parquet\")\nval S2_MAG_Orcid_orcid_old = S2_MAG_Orciddf_old.filter($\"orcId\"=!=\"\"  && $\"orcId\".isNotNull)\n.cache()\n\n//S2_MAG_Orcid_S2doi.printSchema\nprintln(\"S2_MAG_Orcid_orcid cnt:\" + S2_MAG_Orcid_orcid_old.count())\n//S2_MAG_Orcid_S2doi.show(40)\n\nS2_MAG_Orcid_orcid_old.select(countDistinct(\"orcId\")).show(false)\nS2_MAG_Orcid_orcid_old.select(countDistinct(\"MAGpaperId\")).show(false)\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-10T12:57:16+0200",
   "progress": 0.0,
   "config": {
    "editorHide": false,
    "tableHide": true,
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true,
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid.parquet\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:806)\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:803)\n  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644490636662_397144084",
   "id": "paragraph_1644490636662_397144084",
   "dateCreated": "2022-02-10T12:57:16+0200",
   "dateStarted": "2022-02-10T12:57:16+0200",
   "dateFinished": "2022-02-10T12:57:16+0200",
   "status": "ERROR"
  },
  {
   "text": "%spark\nval HIND_HOME = \"/home/ometaxas/Datasets/Hindawi\"\nval hind_author_pub_df = spark.read\n           .options(Map(\"header\"-> \"true\")).\n             //.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n               // schema(authorSchema).\n                csv(s\"file://$HIND_HOME/author_pub.csv\")\n\nhind_author_pub_df.printSchema()\nhind_author_pub_df.show(5)\nprintln(hind_author_pub_df.count())\n\nhind_author_pub_df.\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-12T16:53:53+0200",
   "progress": 0.0,
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "<console>:162: \u001b[31merror: \u001b[0mvalue ccv is not a member of org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]\n       hind_author_pub_df.write.mode(\"overwrite\").ccv(\"/media/ometaxas/nvme/datasets/MAG_S2/hindLinkedAuthors.csv\")\n                                                  ^\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1636728833126_1926184669",
   "id": "paragraph_1636728833126_1926184669",
   "dateCreated": "2021-11-12T16:53:53+0200",
   "dateStarted": "2021-11-12T16:53:53+0200",
   "dateFinished": "2021-11-12T16:53:53+0200",
   "status": "ERROR"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\n",
   "config": {
    "editorHide": false
   }
  },
  {
   "title": "Test normalization functions",
   "text": "%spark\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport java.util.Locale;\nimport com.github.mrpowers.spark.stringmetric.SimilarityFunctions._\nimport com.github.mrpowers.spark.stringmetric.PhoneticAlgorithms._\n\n\nval a = 100\nprintln(a)\n\n\n\ndef normName1(e:String): String = {\n    \n return org.apache.commons.lang3.StringUtils.stripAccents(e.toLowerCase(Locale.ENGLISH))\n     .replaceAll(\"[ <>:´,’./\\\\'\\\\\\\";(){}!@#$%^&+‐–*\\\\\\\\-]+\", \"\")\n    .replaceAll(\"Æ\", \"AE\")\n    .replaceAll(\"Ð\", \"D\")\n    .replaceAll(\"Ø\", \"O\")\n    .replaceAll(\"Þ\", \"TH\")\n    .replaceAll(\"ß\", \"ss\")\n    .replaceAll(\"ð\", \"d\")\n    .replaceAll(\"æ\", \"ae\")\n    .replaceAll(\"ø\", \"o\")\n    .replaceAll(\"þ\", \"th\")\n    .replaceAll(\"Œ\", \"OE\")\n    .replaceAll(\"œ\", \"oe\")\n    .replaceAll(\"ƒ\", \"f\")\n\t.trim()\n    \n}\ndef shortNormName1(e: String): String = {\n  val normname = org.apache.commons.lang3.StringUtils.stripAccents(e.toLowerCase(Locale.ENGLISH))\n    //.replaceAll(\"[^\\\\p{ASCII}]\", \"\")\n     .replaceAll(\"[<>:´,’./\\\\'\\\\\\\";(){}!@#$%^&+‐–*\\\\\\\\-]+\", \"\")\n    .replaceAll(\"Æ\", \"AE\")\n    .replaceAll(\"Ð\", \"D\")\n    .replaceAll(\"Ø\", \"O\")\n    .replaceAll(\"Þ\", \"TH\")\n    .replaceAll(\"ß\", \"ss\")\n    .replaceAll(\"ð\", \"d\")\n    .replaceAll(\"æ\", \"ae\")\n    .replaceAll(\"ø\", \"o\")\n    .replaceAll(\"þ\", \"th\")\n    .replaceAll(\"Œ\", \"OE\")\n    .replaceAll(\"œ\", \"oe\")\n    .replaceAll(\"ƒ\", \"f\")\n\t.trim().split(\" \")\n    \n\n val shortName =  if (normname.length == 1) normname(0) else normname(0).take(1) + normname(normname.length-1)\n \n return  shortName\n  \n}\n\n//\"合阪  幸三\",\"平池  春子\",\"兵藤  博恵\",\"生月  弓子\",\"秋山  純子\",\"木村  好秀\",\"岡田  紀三男\",\"小畑  清一郎\",\"宮本  雄一郎\",\"平池  修\",\"兵藤  博信\",\"森  宏之\"],\"S2shortNormNames\":[\"合阪幸三\",\"平池春子\",\"兵藤博恵\",\"生月弓子\",\"秋山純子\",\"木村好秀\",\"岡田紀三男\",\"小畑清一郎\",\"宮本雄一郎\",\"平池修\",\"兵藤博信\",\"森宏之\"],\"S2normNames\":[\"合阪幸三\",\"平池春子\",\"兵藤博恵\",\"生月弓子\",\"秋山純子\",\"木村好秀\",\"岡田紀三男\",\"小畑清一郎\",\"宮本雄一郎\",\"平池修\",\"兵藤博信\",\"森宏之\"\n//println(\"chinese:\"+ unaccent2(\"福村 健\"))\n\nprintln(normName1(\"合阪  幸三\"))\nprintln(shortNormName1(\"合阪  幸三\"))\n\nprintln(normName1(\"兵藤  博恵\"))\nprintln(shortNormName1(\"兵藤  博恵\"))\n\nprintln(normName1(\"С.В.  Гувернюк\"))\nprintln(shortNormName1(\"С.В.  Гувернюк\"))\n\nprintln(normName1(\"C. E. Pankhurst\"))\nprintln(shortNormName1(\"C. E. Pankhurst\"))\n\n\nif (shortNormName1(\"合阪  幸三\")==shortNormName1(\"兵藤  博恵\")) println(\"short names match\")\n\nif (shortNormName1(\"合阪  幸三\")==shortNormName1(\"平池  春子\")) println(\"short names match\")\n\n//if (double_metaphone(normName(\"合阪  幸三\"))==double_metaphone(normName(\"兵藤  博恵\")) ) println(\"names match\")\n\nval df1 = spark.createDataFrame(Seq((1, \"andy\", 20, \"USA\"), (2, \"jeff\", 23, \"China\"), (3, \"james\", 18, \"USA\"))).toDF(\"id\", \"name\", \"age\", \"country\")\n\nval sourceDF = spark.createDataFrame(\n      Seq(\n          \n          (\"WeiQi Li\", \"Wei Li\"),\n          (\"D. Nenad\", \"T. Nenad\"),\n          (\"Van Vase\", \"Van Vaese\"),\n          (\"Van Vosse\", \"Van Voese\"),\n          (\"Van Vosse\", \"Ban Voshe\"),\n          (\"С.В.  Гувернюк\", \"Я.А.  Дынников\"),\n          (\"Marco  Apitz\",\"Marc  Opitz\"),\n          (\"Л.  Туйчиев\",\"Ш.  Туйчиев\"),\n          (\"mjillera\", \"millera\"),\n          (\"Ю.  Юров\",\"Ю. Б. Юров\"),\n          (\"В А Таряник\",\"В О Таряник\"),\n          (\"C. E. Pankhurst\",\"C.  Pankhurst\"),\n          (\"B  Geavlete\",\"P  Geavlete\"),\n          (\"Vladimiras  Bondarenko\",\"Vladimiras  Bondarenka\"),\n          (\"Quentin  Bonnardβ\",\"Quentin  Bonnard\"),\n          (\"А. С. Шуваев\",\"Н. С. Шуваев\"),\n          (\"Tong Bou Chang\",\"Tong-Bou  Chang\"),\n          (\"Zhong  Feng\",\"Zhong  Fei\"),\n          (\"Д. А. Минченко\",\"Т. А. Зинченко\"),\n          (\"合阪  幸三\", \"平池  春子\")\n        \n        )).toDF(\"word1\", \"word2\")\n        \n      /*\n        List(\n          (\"合阪  幸三\", \"兵藤  博恵\"),\n          (\"合阪  幸三\", \"平池  春子\")\n          \n        ), List(\n          (\"word1\", StringType, true),\n          (\"word2\", StringType, true)\n        )\n      )*/\n\nval levDF = sourceDF.withColumn(\n        \"w1_w2_lev\",\n        levenshtein(col(\"word1\"), col(\"word2\"))\n      ).show()\n      \n      val actualDF = sourceDF.withColumn(\n        \"w1_w2_jaro_winkler\",\n        jaro_winkler(col(\"word1\"), col(\"word2\"))\n      ).show()\n\n\n   val actualDF2 = sourceDF.withColumn(\n        \"w1_w2_jaccard\",\n        jaccard_similarity(col(\"word1\"), col(\"word2\"))\n      ).show()\n      \n        val actualDF3 = sourceDF.withColumn(\n        \"word1_double_metaphone\",\n        double_metaphone(col(\"word1\"))\n      )\n      .withColumn(\n        \"word2_double_metaphone\",\n        double_metaphone(col(\"word2\"))\n        )\n        .show()\n        \n        val actualDF4 = sourceDF.withColumn(\n        \"word1_soundex\",\n        soundex(col(\"word1\"))\n      )\n      .withColumn(\n        \"word2_soundex\",\n        soundex(col(\"word2\"))\n        )\n        .show()\n      \n      \n      val compdf = sourceDF.filter( shortNormName($\"word1\")===shortNormName($\"word2\")).show()\n      \n      val compdf2 = sourceDF.filter( levenshtein( normName($\"word1\"), normName($\"word2\"))>0.8   ).show() \n      \n      val compdf3 = sourceDF.filter(length(double_metaphone(normName($\"word1\")))>1 &&  double_metaphone(normName($\"word1\")) === double_metaphone(normName($\"word2\"))) .show() \n      \n      \n      val compdf4 = sourceDF.filter(shortNormName($\"word1\")===shortNormName($\"word2\") || ( jaro_winkler(normName($\"word1\"),normName($\"word2\"))>0.8  &&  levenshtein(normName($\"word1\"), normName($\"word2\"))<2  && double_metaphone(normName($\"word1\")) === double_metaphone(normName($\"word2\")) )).show() \n//( levenshtein($\"normName\", $\"S2normName\")>0.8  && double_metaphone($\"normName\") === double_metaphone($\"S2normName\") )\n\n\n\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2020-12-22 12:02:11.916",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "title": true,
    "editorHide": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1606472529243_799120251",
   "id": "paragraph_1606472529243_799120251",
   "dateCreated": "2020-11-27 12:22:09.243",
   "dateStarted": "2020-12-22 12:02:11.923",
   "dateFinished": "2020-12-22 12:02:13.600",
   "status": "FINISHED"
  },
  {
   "title": "Explode Affiliations & add seqNum ",
   "text": "%spark\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\n\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/20201109/mag\"\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\n\n\nval paperAuthorAffSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"authorId\", LongType, false).                \n                add(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n                \nval paperAuthorAffdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperAuthorAffSchema).\n                csv(s\"file://$MAG_HOME/$paperAuthorsAffTsvFilename\")\n                .persist(StorageLevel.DISK_ONLY)\n\n\nprintln(\"paperAuthorAffdf cnt:\" + paperAuthorAffdf.count())\nval paperIdsdf = paperAuthorAffdf.select(countDistinct(\"paperId\"))\npaperIdsdf.show(false)\nval authorIdsdf = paperAuthorAffdf.select(countDistinct(\"authorId\")) \nauthorIdsdf.show(false)\n\n\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid.parquet\")\n\nprintln(\"paperAuthorAffdf cnt:\" + S2_MAG_df.count())\nval paperIdsdf2 = S2_MAG_df.select(countDistinct(\"MAGpaperId\")) \npaperIdsdf2.show(false)\nval authorIdsdf2 = S2_MAG_df.select(countDistinct(\"MAGauthorId\")) \nauthorIdsdf2.show(false)\n\n/*\n        .select(S2_MAG_df(\"MAGpaperId\"), S2_MAG_df(\"MAGdoi\"),  S2_MAG_df(\"MAGdisplayName\"), S2_MAG_df(\"MAGname\"), S2_MAG_df(\"MAGnormName\"), \n                            S2_MAG_df(\"MAGshortNormName\"),  S2_MAG_df(\"MAGauthorId\"), S2_MAG_df(\"fosids\"), S2_MAG_df(\"fosids_lvl0\") ,S2_MAG_df(\"fosids_lvl1\"), \n                            S2_MAG_df(\"authorIds\"), S2_MAG_df(\"authorNormNames\"), S2_MAG_df(\"authorShortNormNames\"),S2_MAG_df(\"affiliationIds\"), \n                            S2_MAG_df(\"S2authorId\"),S2_MAG_df(\"S2paperId\"), S2_MAG_df(\"S2doi\"), S2_MAG_df(\"S2name\"), S2_MAG_df(\"S2shortNormName\"), S2_MAG_df(\"S2normName\"), S2_MAG_df(\"S2fos\") , orcid_df(\"orcId\"))\n                            \n*/\n\nval PKGinitdf = paperAuthorAffdf\n                   .join(S2_MAG_df, paperAuthorAffdf(\"paperId\") === S2_MAG_df(\"MAGpaperId\")  &&  paperAuthorAffdf(\"authorId\")===S2_MAG_df(\"MAGauthorId\") , \"inner\")                  \n                   .select(S2_MAG_df(\"MAGpaperId\"),   S2_MAG_df(\"MAGauthorId\"), paperAuthorAffdf(\"affiliationId\"), paperAuthorAffdf(\"authorSequenceNumber\"), \n                          S2_MAG_df(\"S2authorId\"),S2_MAG_df(\"S2paperId\"),S2_MAG_df(\"orcId\"))\n                          \nPKGinitdf.write.parquet(\"/media/datadisk/Datasets/MAG_S2/PKGinitdf.parquet\")                    ",
   "user": "anonymous",
   "dateUpdated": "2020-12-23 09:32:12.856",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "title": true,
    "editorHide": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=4"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=5"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=6"
      },
      {
       "jobUrl": "http://PC192.168.2.7.station:4040/jobs/job?id=7"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1606036250517_213251448",
   "id": "paragraph_1606036250517_213251448",
   "dateCreated": "2020-11-22 11:10:50.517",
   "dateStarted": "2020-12-23 09:32:12.860",
   "dateFinished": "2020-12-23 09:56:40.786",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\n\nval PKG_init_df = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/PKGinitdf.parquet\")\n\nPKG_init_df.limit(1000000).coalesce(1).write.parquet(\"/media/datadisk/Datasets/MAG_S2/PKGinitdf_1M.parquet\")                    ",
   "config": {}
  },
  {
   "text": "%spark\nimport org.apache.spark.storage.StorageLevel;\nimport org.apache.spark.sql.functions.countDistinct;\nimport  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils;\nimport java.util.Calendar;\n\n\n\nval S2_MAG_df = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/S2_MAG_df.parquet\")\nprintln(\"S2_MAG_df cnt:\"+S2_MAG_df.count())\n\nval S2_MAG_df_diffs = S2_MAG_df.filter($\"S2doi\"=!=\"\"  && $\"S2doi\".isNotNull && $\"S2doi\"=!=$\"MAGdoi\")\n    .cache()\n\nprintln(\"s2doi diffs cnt:\"+S2_MAG_df_diffs.count())\nprintln(Calendar.getInstance().getTime())\n\n/*\nprintln(Calendar.getInstance().getTime())\nS2_MAG_df.groupBy($\"MAGdoi\")\n.agg( collect_list($\"MAGshortNormName\") as \"magauthors\", collect_list($\"S2shortNormName\") as \"s2authors\").show(10)\n\nprintln(Calendar.getInstance().getTime())\n*/\n\n//val dfdoidiffs = S2_MAG_df.select(countDistinct(\"S2doi\"))\n//dfdoidiffs.show(false)\n\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-10T12:56:45+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/media/datadisk/Datasets/MAG_S2/S2_MAG_df.parquet\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:806)\n  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:803)\n  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n  at scala.util.Success.$anonfun$map$1(Try.scala:255)\n  at scala.util.Success.map(Try.scala:213)\n  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n  at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644490605130_1730246948",
   "id": "paragraph_1644490605130_1730246948",
   "dateCreated": "2022-02-10T12:56:45+0200",
   "dateStarted": "2022-02-10T12:56:45+0200",
   "dateFinished": "2022-02-10T12:56:45+0200",
   "status": "ERROR"
  },
  {
   "title": "Statistics ",
   "text": "%spark\n\nval orcid_df = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/orcid.parquet\")\n.cache()\n\nprintln(\"paperAuthorAffdf cnt:\" + orcid_df.count())\nval paperIdsdf4 = orcid_df.select(countDistinct(\"doi\")) \npaperIdsdf4.show(false)\nval authorIdsdf4 = orcid_df.select(countDistinct(\"orcId\")) \nauthorIdsdf4.show(false)\n",
   "user": "anonymous",
   "dateUpdated": "2020-11-24 11:13:27.069",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "title": true,
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=3"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1606138331418_98467935",
   "id": "paragraph_1606138331418_98467935",
   "dateCreated": "2020-11-23 15:32:11.418",
   "dateStarted": "2020-11-23 20:35:07.694",
   "dateFinished": "2020-11-23 20:35:45.582",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval PKGinitdf = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2/PKGinitdf.parquet\")\n.persist(StorageLevel.DISK_ONLY)\n\nprintln(\"paperAuthorAffdf cnt:\" + PKGinitdf.count())\nval paperIdsdf3 = PKGinitdf.select(countDistinct(\"MAGpaperId\")) \npaperIdsdf3.show(false)\nval authorIdsdf3 = PKGinitdf.select(countDistinct(\"MAGauthorId\")) \nauthorIdsdf3.show(false)\n\nPKGinitdf.show(20)\n",
   "user": "anonymous",
   "dateUpdated": "2020-11-22 18:04:36.521",
   "config": {
    "colWidth": 12.0,
    "fontSize": 9.0,
    "enabled": true,
    "results": {},
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "editorMode": "ace/mode/scala",
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=20"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=21"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=22"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=23"
      },
      {
       "jobUrl": "http://192.168.2.5:4040/jobs/job?id=24"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1606060994366_122591239",
   "id": "paragraph_1606060994366_122591239",
   "dateCreated": "2020-11-22 18:03:14.366",
   "dateStarted": "2020-11-22 18:04:36.524",
   "dateFinished": "2020-11-22 18:13:26.424",
   "status": "FINISHED"
  }
 ],
 "name": "Match_S2_MAG_ORCiD",
 "id": "2FQRWCV2A",
 "defaultInterpreterGroup": "spark",
 "version": "0.9.0-preview2",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false
 },
 "info": {}
}