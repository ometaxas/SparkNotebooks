{
 "paragraphs": [
  {
   "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explictly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\nSPARK_HOME /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2\n\n#spark.jars /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/rapids-4-spark_2.12-0.2.0.jar, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/cudf-0.15-cuda10-1.jar\n\n#spark.sql.warehouse.dir /home/ometaxas/Programs/zeppelin-0.9.0-preview2-bin-all/spark-warehouse\n\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryoserializer.buffer.max 1000M\nspark.driver.memory 100g\nspark.driver.maxResultSize 5g \n\n#spark.rapids.sql.concurrentGpuTasks=2\n#spark.rapids.sql.enabled true\n#spark.rapids.memory.pinnedPool.size 2G \n\n#spark.plugins com.nvidia.spark.SQLPlugin \n\n#spark.locality.wait 0s \n#spark.sql.files.maxPartitionBytes 512m \n#spark.sql.shuffle.partitions 100 \n#spark.executor.resource.gpu.amount=1\n\n\nSPARK_LOCAL_DIRS /media/ometaxas/nvme/spark, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp\n                                             \n# /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp,/media/datadisk/Datasets/Spark\n#,/media/datadisk/Datasets/Spark \n#/media/datadisk/Datasets/Spark\n\n# set executor memrory 110g\n# spark.executor.memory  60g\n\n\n# set executor number to be 6\n# spark.executor.instances  6\n\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode after Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use it after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-20T23:07:40+0200",
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": []
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611176860050_1101647111",
   "id": "paragraph_1611176860050_1101647111",
   "dateCreated": "2021-01-20T23:07:40+0200",
   "dateStarted": "2021-01-20T23:07:40+0200",
   "dateFinished": "2021-01-20T23:07:40+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\nimport java.util.Calendar;\n\n//org.apache.commons.lang3.StringUtils.stripAccents(input.toLowerCase(Locale.ENGLISH));\n\n\n\n//Get MAG AUthor - Pub pairs\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/20201109/mag\"\nval MAG_ADV =  \"/media/datadisk/Datasets/MAG/20201109/advanced\"\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\nval authorsAffTsvFilename = \"Authors.txt\"\nval papersTsvFilename = \"Papers.txt\"\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\n\n\nval paperSchema = new StructType().\n    add(\"paperId\", LongType, false).\n    add(\"magRank\", IntegerType, true).\n    add(\"doi\", StringType, true).\n    add(\"docTypetmp\", StringType, true).\n    add(\"normalizedTitle\", StringType, true).\n    add(\"title\", StringType, false).\n    add(\"bookTitle\", StringType, true).\n    add(\"pubYear\", IntegerType, true).\n    add(\"pubDate\", StringType, true).\n    add(\"onlineDate\", StringType, true).\n    add(\"publisherName\", StringType, true).\n    add(\"journalId\", StringType, true).\n    add(\"conferenceSeriesId\", LongType, true).\n    add(\"conferenceInstancesId\", LongType, true).\n    add(\"volume\", StringType, true).\n    add(\"issue\", StringType, true).\n    add(\"firstPage\", StringType, true).\n    add(\"lastPage\", StringType, true).\n    add(\"referenceCount\", LongType, true).\n    add(\"citationCount\", LongType, true).\n    add(\"estimatedCitation\", LongType, true).\n    add(\"originalVenue\", StringType, true).\n    add(\"familyId\", LongType, true).\n    add(\"familyRank\", IntegerType, true).\n    add(\"createdDate\", DateType, true)\n    \n//199944782 IEEE Transactions on Pattern Analysis and Machine Intelligence ~7K \n//199944782 Nature ~200K\n//3880285  Science ~200K\n// 202381698 PLOS ONE ~ 262K\n//2597175965 arXiv: Computer Vision and Pattern Recognition ~39K\n// 2597365278 arXiv: Machine Learning ~ 11K\n// 118988714 JMLR 3.7K\n// 1158167855 CVPR: Computer Vision and Pattern Recognition 11K\n// 103482838 Communications of The ACM 13K \n// 1127325140 NeurIPS: Neural Information Processing Systems 12.5K \n// 1180662882 ICML 9K\n// 1130985203 KDD 6.5K\n// 1203999783 IJCAI: International Joint Conference on Artificial Intelligence 11K\n\n/*\nSample dataset counts: \n\nAuthors: 1,357,143\nPaper_Author_Affiliations: 2,787,121\nPaper_FoS: 4,243,126\nPaper_references: 14,030,446\nPaper_Abstracts1: 198,285\nPaper_Abstracts2: 206,690\nPapers: 488,212\n\n */\n\n\nval papersdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\"))\n         .schema(paperSchema)\n         .csv(s\"file://$MAG_HOME/$papersTsvFilename\")\n        .filter(($\"conferenceSeriesId\" isin (1127325140, 1180662882, 1203999783, 1130985203)) ||  \n                ($\"journalId\" isin (\"199944782\",\"137773608\", \"3880285\", \"202381698\", \"2597175965\", \"2597365278\", \"118988714\", \"1158167855\")) \n                 && $\"pubYear\">2000)\n        .cache()\n\n\n//papersdf.printSchema\n//papersdf.show(5)\nprintln(\"papersdf: \"+papersdf.count())\n\npapersdf.coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$papersTsvFilename\")\n\nval paperIdsdf = papersdf.select($\"paperId\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-20T23:07:40+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "papersdf: 488212\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.concat_ws\nimport org.apache.spark.sql.functions.countDistinct\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\nimport java.util.Calendar\n\u001b[1m\u001b[34mMAG_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20201109/mag\n\u001b[1m\u001b[34mMAG_ADV\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20201109/advanced\n\u001b[1m\u001b[34mpaperAuthorsAffTsvFilename\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperAuthorAffiliations.txt\n\u001b[1m\u001b[34mauthorsAffTsvFilename\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = Authors.txt\n\u001b[1m\u001b[34mpapersTsvFilen...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611176860089_690260183",
   "id": "paragraph_1611176860089_690260183",
   "dateCreated": "2021-01-20T23:07:40+0200",
   "dateStarted": "2021-01-20T23:07:40+0200",
   "dateFinished": "2021-01-20T23:26:36+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval MAG_NLP =  \"/media/datadisk/Datasets/MAG/20201109/nlp\"\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/20201109/mag\"\nval MAG_ADV =  \"/media/datadisk/Datasets/MAG/20201109/advanced\"\n\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval abstr1df = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename1\")\n\nval abstr1_sampledf = abstr1df.join(broadcast(paperIdsdf),abstr1df(\"paperId\") === paperIdsdf(\"paperId\")).drop(paperIdsdf(\"paperId\"))\n                    .coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$abstractTsvFilename1\")\n                \n\nval abstr2df = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename2\")\n\nval abstr2_sampledf = abstr2df.join(broadcast(paperIdsdf),abstr2df(\"paperId\") === paperIdsdf(\"paperId\")).drop(paperIdsdf(\"paperId\"))\n                    .coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$abstractTsvFilename2\")\n\n//df3.printSchema\n//df3.show(5)\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-20T23:26:36+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mMAG_NLP\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20201109/nlp\n\u001b[1m\u001b[34mabstractTsvFilename1\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperAbstractsInvertedIndex.txt.1\n\u001b[1m\u001b[34mabstractTsvFilename2\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperAbstractsInvertedIndex.txt.2\n\u001b[1m\u001b[34mschema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m = StructType(StructField(paperId,StringType,false), StructField(abstractText,StringType,false))\n\u001b[1m\u001b[34mabstr1df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: string, abstractText: string]\n\u001b[1m\u001b[34mabstr1_sampledf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n\u001b[1m\u001b[34mabstr2df\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: string, abstractText: string]\n\u001b[1m\u001b[34mabstr2_sampledf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=5"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611177996882_2024358012",
   "id": "paragraph_1611177996882_2024358012",
   "dateCreated": "2021-01-20T23:26:36+0200",
   "dateStarted": "2021-01-20T23:26:36+0200",
   "dateFinished": "2021-01-21T00:04:16+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval paperReferencesTsvFilename = \"PaperReferences.txt\"\nval paperReferencesSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"paperReferenceId\", LongType, true)                \n\nval paperReferencesdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperReferencesSchema).\n                csv(s\"file://$MAG_HOME/$paperReferencesTsvFilename\")\n\nval paperReferences_sampledf = paperReferencesdf.join(broadcast(paperIdsdf),paperReferencesdf(\"paperId\") === paperIdsdf(\"paperId\")).drop(paperIdsdf(\"paperId\"))\n                    .coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$paperReferencesTsvFilename\")\n\n\n\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-21T00:04:16+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mpaperReferencesTsvFilename\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperReferences.txt\n\u001b[1m\u001b[34mpaperReferencesSchema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m = StructType(StructField(paperId,LongType,false), StructField(paperReferenceId,LongType,true))\n\u001b[1m\u001b[34mpaperReferencesdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, paperReferenceId: bigint]\n\u001b[1m\u001b[34mpaperReferences_sampledf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=7"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611180256521_1977389046",
   "id": "paragraph_1611180256521_1977389046",
   "dateCreated": "2021-01-21T00:04:16+0200",
   "dateStarted": "2021-01-21T00:04:16+0200",
   "dateFinished": "2021-01-21T00:29:49+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paperFieldsOfStudyTsvFilename = \"PaperFieldsOfStudy.txt\"\n\nval paperFieldsOfStudyschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"score\", DoubleType, true)\n                \nval paperFieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperFieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$paperFieldsOfStudyTsvFilename\")\n\nval paperFieldsOfStudy_sampledf = paperFieldsOfStudydf.join(broadcast(paperIdsdf),paperFieldsOfStudydf(\"paperId\") === paperIdsdf(\"paperId\")).drop(paperIdsdf(\"paperId\"))\n                    .coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$paperFieldsOfStudyTsvFilename\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-21T00:29:49+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mpaperFieldsOfStudyTsvFilename\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperFieldsOfStudy.txt\n\u001b[1m\u001b[34mpaperFieldsOfStudyschema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m = StructType(StructField(paperId,LongType,false), StructField(fieldsOfStudyId,LongType,false), StructField(score,DoubleType,true))\n\u001b[1m\u001b[34mpaperFieldsOfStudydf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, fieldsOfStudyId: bigint ... 1 more field]\n\u001b[1m\u001b[34mpaperFieldsOfStudy_sampledf\u001b[0m: \u001b[1m\u001b[32mUnit\u001b[0m = ()\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611181789526_398854583",
   "id": "paragraph_1611181789526_398854583",
   "dateCreated": "2021-01-21T00:29:49+0200",
   "dateStarted": "2021-01-21T00:29:49+0200",
   "dateFinished": "2021-01-21T00:55:00+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\n\nval paperAuthorAffSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"authorId\", LongType, false).                \n                add(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n                \nval paperAuthorAffdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperAuthorAffSchema).\n                csv(s\"file://$MAG_HOME/$paperAuthorsAffTsvFilename\")\n\nval paperAuthorAff_sampledf = paperAuthorAffdf.join(broadcast(paperIdsdf),paperAuthorAffdf(\"paperId\") === paperIdsdf(\"paperId\")).drop(paperIdsdf(\"paperId\"))\n        .cache()\n\npaperAuthorAff_sampledf.coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$paperAuthorsAffTsvFilename\")\n\nval sampleAuthorIdsdf = paperAuthorAff_sampledf.select($\"authorId\").dropDuplicates()\n\nval authorsTsvFilename = \"Authors.txt\" \n                              \nval authorSchema = new StructType().\n                add(\"authorId\", LongType, false).\n                add(\"rank\", LongType, true).                \n                add(\"normalizedName\", StringType, true).\n                add(\"displayName\",StringType, true).\n                add(\"lastKnownAffiliationId\", LongType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n\n                \nval authordf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(authorSchema).\n                csv(s\"file://$MAG_HOME/$authorsTsvFilename\")\n\nval authordf_sampledf = authordf.join(broadcast(sampleAuthorIdsdf),authordf(\"authorId\") === sampleAuthorIdsdf(\"authorId\")).drop(sampleAuthorIdsdf(\"authorId\"))        \n                .coalesce(1).write.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).mode(\"overwrite\").csv(s\"$MAG_HOME/sample/$authorsTsvFilename\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-21T00:55:00+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mpaperAuthorsAffTsvFilename\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = PaperAuthorAffiliations.txt\n\u001b[1m\u001b[34mpaperAuthorAffSchema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m = StructType(StructField(paperId,LongType,false), StructField(authorId,LongType,false), StructField(affiliationId,LongType,true), StructField(authorSequenceNumber,IntegerType,true), StructField(originalAuthor,StringType,true), StructField(originalAffiliation,StringType,true))\n\u001b[1m\u001b[34mpaperAuthorAffdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, authorId: bigint ... 4 more fields]\n\u001b[1m\u001b[34mpaperAuthorAff_sampledf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: bigint, authorId: bigint ... 4 more fields]\n\u001b[1m\u001b[34msampleAuthorIdsdf\u001b[0m: \u001b[1m\u001b[32morg.apach...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=11"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=13"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611183300705_1132049756",
   "id": "paragraph_1611183300705_1132049756",
   "dateCreated": "2021-01-21T00:55:00+0200",
   "dateStarted": "2021-01-21T00:55:00+0200",
   "dateFinished": "2021-01-21T01:21:02+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/20201109/subset\"\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\nval papersTsvFilename = \"Papers.txt\"\nval paperFieldsOfStudyTsvFilename = \"PaperFieldsOfStudy.txt\"\nval fieldsOfStudyTsvFilename = \"FieldsOfStudy.txt\"\nval paperReferencesTsvFilename = \"PaperReferences.txt\"\n\n\n// Specify schema for your csv file\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\nval paperSchema = new StructType().\n    add(\"paperId\", LongType, false).\n    add(\"magRank\", IntegerType, true).\n    add(\"doi\", StringType, true).\n    add(\"docTypetmp\", StringType, true).\n    add(\"normalizedTitle\", StringType, true).\n    add(\"title\", StringType, false).\n    add(\"bookTitle\", StringType, true).\n    add(\"pubYear\", IntegerType, true).\n    add(\"pubDate\", StringType, true).\n    add(\"onlineDate\", StringType, true).\n    add(\"publisherName\", StringType, true).\n    add(\"journalId\", StringType, true).\n    add(\"conferenceSeriesId\", LongType, true).\n    add(\"conferenceInstancesId\", LongType, true).\n    add(\"volume\", StringType, true).\n    add(\"issue\", StringType, true).\n    add(\"firstPage\", StringType, true).\n    add(\"lastPage\", StringType, true).\n    add(\"referenceCount\", LongType, true).\n    add(\"citationCount\", LongType, true).\n    add(\"estimatedCitation\", LongType, true).\n    add(\"originalVenue\", StringType, true).\n    add(\"familyId\", StringType, true).\n    add(\"createdDate\", DateType, true)\n  \n\nval papersdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                    schema(paperSchema).\n                    csv(s\"file://$MAG_HOME/$papersTsvFilename\")\n\n\n\n//val papersdfcnt = papersdf.count()\n//println(papersdfcnt)\n//papersdf.printSchema\npapersdf.show(5)\n\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval df3 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_HOME/$abstractTsvFilename1\")\n//df3.printSchema\n//df3.show(5)\n\nval df4 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_HOME/$abstractTsvFilename2\")\n//df4.printSchema\n//df4.show(5)\n\n\nval udf1 = udf[String, String]((index:String) => {\n    \n    val index2 = org.apache.commons.lang.StringUtils.replace(index, \"\\\\\\\"\", \"\\'\")\n    val indexNumber: String = org.apache.commons.lang.StringUtils.substringBetween(index2, \":\", \",\")\n    if (indexNumber == null) {\n      null\n    }\n    var outputText: StringBuilder = new StringBuilder()\n    try {\n      val textLength: Int = parseInt(indexNumber)\n      val text: Array[String] =  new Array[String](textLength)\n      val tokens: Array[String] =\n        org.apache.commons.lang.StringUtils.substringsBetween(index2, \"\\\"\", \"\\\"\")\n      for (i <- 0 until tokens.length - 2) {\n        if (tokens(i + 2).==(\"\\'\")) {\n          tokens(i + 2) = \"\\\"\"\n        }\n        val tokenPositions: Array[String] =\n          org.apache.commons.lang.StringUtils.substringsBetween(index2, \"[\", \"]\")\n        for (s <- tokenPositions(i).split(\",\")) {\n          if (s.matches(\"(.*)\\\\D(.*)\")) {\n          }\n          val wordPosition: Int = parseInt(s)\n          text(wordPosition) = tokens(i + 2)\n        }\n      }\n      for (j <- 0 until text.length) {\n        if (j < text.length - 1) {\n          outputText.append(text(j)).append(' ')\n        } else {\n          outputText.append(text(j))\n        }\n      }\n      outputText = new StringBuilder(\n        outputText.toString\n          .replaceAll(\"\\'\", \"\\\"\")\n          .replaceAll(\"\\\\w?\\\\\\\\\\\\w\\\\d+\", \" \"))\n    } catch {\n      case e: Exception => {\n        logger.error(\"Error while parsing \")\n        null\n      }\n\n    }\n    outputText.toString}\n    )\n\nval df5 = df3.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df5.show(5)\n\nval df6 = df4.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df6.show(5)\n\nval paperabstractsdf = df5.union(df6).dropDuplicates()\n//val abstractsdfcnt = abstractsdf.count()\n//println(s\"abstractsdfcnt\")\npaperabstractsdf.show(5)\n\n\n\nval paperIdFieldsOfStudyschema = new StructType().\n                add(\"paperIdFieldsOfStudy\", LongType, false).\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"score\", DoubleType, true)\n                \nval paperFieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperIdFieldsOfStudyschema).\n                csv(s\"file://$MAG_HOME/$paperFieldsOfStudyTsvFilename\")\n//paperFieldsOfStudydf.printSchema\npaperFieldsOfStudydf.show(5)\n\nval fieldsOfStudyschema = new StructType().\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"name\", StringType, true).\n                add(\"mainType\", StringType, true).\n                add(\"level\", IntegerType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval fieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldsOfStudyschema).\n                csv(s\"file://$MAG_HOME/$fieldsOfStudyTsvFilename\")\n//fieldsOfStudydf.printSchema\nfieldsOfStudydf.show(5)\n\n\n  \nval paperReferencesSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"paperReferenceId\", LongType, true)                \n                \nval paperReferencesdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperReferencesSchema).\n                csv(s\"file://$MAG_HOME/$paperReferencesTsvFilename\")\n  \n\npaperReferencesdf.show(5)\n\n//Test join \n\nval paper_fos = paperFieldsOfStudydf.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(paperFieldsOfStudydf(\"paperId\").as(\"paper_fos_paperId\"))\n                .agg( \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),\n                    //sum(when($\"date\" > \"2017-04\", $\"value\")).alias(\"value4\")\n                    collect_set(when($\"level\" > 1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids\"),\n                    collect_set(when($\"level\" ===  0, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl0\"),\n                    collect_set(when($\"level\" ===  1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl1\")\n                    )\n                    .persist(StorageLevel.DISK_ONLY)\n\npaper_fos.show(5)\n\n\nval paper_fos_abstractsdf = papersdf              \n              .join(paper_fos, paper_fos(\"paper_fos_paperId\")=== papersdf(\"paperId\"), \"outer\")\n              .join(paperabstractsdf, paperabstractsdf(\"paperId\")===papersdf(\"paperId\"), \"5\")\n              \npaper_fos_abstractsdf.show(5)\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-21T14:02:01+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "size": {
        "height": 274.0
       },
       "state": {
        "currentPage": "Table",
        "chart": {
         "series": [
          {
           "type": "Pie",
           "values": {
            "column": "paperId",
            "index": 0.0
           },
           "labels": {
            "column": "abstract",
            "index": 1.0
           },
           "showPercents": true
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----------+-------+--------------+----------+--------------------+--------------------+---------+-------+----------+----------+--------------------+----------+------------------+---------------------+------+-----+---------+--------+--------------+-------------+-----------------+--------------------+----------+-----------+\n|   paperId|magRank|           doi|docTypetmp|     normalizedTitle|               title|bookTitle|pubYear|   pubDate|onlineDate|       publisherName| journalId|conferenceSeriesId|conferenceInstancesId|volume|issue|firstPage|lastPage|referenceCount|citationCount|estimatedCitation|       originalVenue|  familyId|createdDate|\n+----------+-------+--------------+----------+--------------------+--------------------+---------+-------+----------+----------+--------------------+----------+------------------+---------------------+------+-----+---------+--------+--------------+-------------+-----------------+--------------------+----------+-----------+\n|2949314557|  18354|          null|Repository|parallel multi di...|Parallel Multi-Di...|     null|   2015|2015-06-24|2015-06-24|                null|2597175965|              null|                 null|  null| null|     null|    null|            20|           95|              135|arXiv: Computer V...| 855255571|       null|\n| 855255571|  19188|          null|Conference|parallel multi di...|Parallel multi-di...|     NIPS|   2015|2015-12-07|      null|           MIT Press|      null|        1127325140|            639028944|  null| null|     2998|    3006|            31|           62|               88|neural informatio...| 855255571|       null|\n|2950930467|  20947|          null|Repository|joint estimation ...|Joint Estimation ...|     null|   2013|2013-11-01|2014-10-08|                null|2597365278|              null|                 null|  null| null|     null|    null|            49|            2|                2|arXiv: Machine Le...|1754350508|       null|\n|2978740920|  21071|10.1038/NRM909|   Journal|stats transcripti...|STATs: transcript...|     null|   2002|2002-09-01|      null|Nature Publishing...| 137773608|              null|                 null|     3|    9|      651|    null|             0|            0|                0|              Nature|1821858011|       null|\n|2949109536|  22487|          null|Repository|probabilistic arc...|Probabilistic Arc...|     null|   2013|2013-12-29|2014-04-07|                null|2597365278|              null|                 null|  null| null|     null|    null|            16|            3|                3|arXiv: Machine Le...|1840600990|       null|\n+----------+-------+--------------+----------+--------------------+--------------------+---------+-------+----------+----------+--------------------+----------+------------------+---------------------+------+-----+---------+--------+--------------+-------------+-----------------+--------------------+----------+-----------+\nonly showing top 5 rows\n\n+----------+--------------------+\n|   paperId|            abstract|\n+----------+--------------------+\n|1561344631|This paper invest...|\n|2005380052|These days, galax...|\n|2005419662|Background\\r\\nThe...|\n|2005440757|Oil and natural g...|\n|2137786375|The similarity of...|\n+----------+--------------------+\nonly showing top 5 rows\n\n+--------------------+---------------+-----------+\n|paperIdFieldsOfStudy|fieldsOfStudyId|      score|\n+--------------------+---------------+-----------+\n|            54590658|     2777532764|  0.5028117|\n|            54590658|      161191863|0.391106516|\n|            54590658|       17744445|0.375210583|\n|            54590658|     2780623531|0.562717736|\n|           161568496|       11413529|0.454900056|\n+--------------------+---------------+-----------+\nonly showing top 5 rows\n\n+---------------+-------+--------------------+--------------------+--------+-----+----------+----------------+-------------+-----------+\n|fieldsOfStudyId|magRank|      normalizedName|                name|mainType|level|paperCount|paperFamilyCount|citationCount|createdDate|\n+---------------+-------+--------------------+--------------------+--------+-----+----------+----------------+-------------+-----------+\n|         417682|  16746|           night air|           Night air|    null|    2|        99|              99|          569| 2016-06-24|\n|        1443462|  15553|         immobiliser|         Immobiliser|    null|    2|       405|             405|         1096| 2016-06-24|\n|        1576492|  13015|       matrix pencil|       Matrix pencil|    null|    3|      1813|            1775|        26444| 2016-06-24|\n|        2657588|  13757|combinatorial top...|Combinatorial top...|    null|    3|       562|             536|        14714| 2016-06-24|\n|        3079626|   8587|quantum electrody...|Quantum electrody...|    null|    1|    226083|          223519|      2787811| 2016-06-24|\n+---------------+-------+--------------------+--------------------+--------+-----+----------+----------------+-------------+-----------+\nonly showing top 5 rows\n\n+-------+----------------+\n|paperId|paperReferenceId|\n+-------+----------------+\n|  58261|      1577938373|\n|  58261|      1997320166|\n|  58261|      2042008249|\n|  58261|      2043794661|\n|  58261|      2061141062|\n+-------+----------------+\nonly showing top 5 rows\n\norg.apache.spark.sql.AnalysisException: Cannot resolve column name \"paperId\" among (paperIdFieldsOfStudy, fieldsOfStudyId, score);\n  at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:270)\n  at scala.Option.getOrElse(Option.scala:189)\n  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:263)\n  at org.apache.spark.sql.Dataset.col(Dataset.scala:1353)\n  at org.apache.spark.sql.Dataset.apply(Dataset.scala:1320)\n  ... 50 elided\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=19"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=20"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=21"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=22"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=23"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611230521468_1111364908",
   "id": "paragraph_1611230521468_1111364908",
   "dateCreated": "2021-01-21T14:02:01+0200",
   "dateStarted": "2021-01-21T14:02:01+0200",
   "dateFinished": "2021-01-21T14:02:58+0200",
   "status": "ERROR"
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}