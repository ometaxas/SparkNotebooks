{
 "paragraphs": [
  {
   "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explictly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\nSPARK_HOME /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2\n\n#spark.jars /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/rapids-4-spark_2.12-0.2.0.jar, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/cudf-0.15-cuda10-1.jar\n\n#spark.sql.warehouse.dir /home/ometaxas/Programs/zeppelin-0.9.0-preview2-bin-all/spark-warehouse\n\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryoserializer.buffer.max 1000M\nspark.driver.memory 95g\nspark.driver.maxResultSize 5g \n\n#spark.rapids.sql.concurrentGpuTasks=2\n#spark.rapids.sql.enabled true\n#spark.rapids.memory.pinnedPool.size 2G \n\n#spark.plugins com.nvidia.spark.SQLPlugin \n\n#spark.locality.wait 0s \n#spark.sql.files.maxPartitionBytes 512m \n#spark.sql.shuffle.partitions 100 \n#spark.executor.resource.gpu.amount=1\n\n\nSPARK_LOCAL_DIRS /media/ometaxas/nvme/spark\n#, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp\n                                             \n# /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp,/media/datadisk/Datasets/Spark\n#,/media/datadisk/Datasets/Spark \n#/media/datadisk/Datasets/Spark\n\n# set executor memrory 110g\n# spark.executor.memory  60g\n\n\n# set executor number to be 6\n# spark.executor.instances  6\n\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode after Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use it after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:02:54+0300",
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": []
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274573976_1820666662",
   "id": "paragraph_1621274573976_1820666662",
   "dateCreated": "2021-05-17T21:02:53+0300",
   "dateStarted": "2021-05-17T21:02:54+0300",
   "dateFinished": "2021-05-17T21:02:54+0300",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.spark.sql.functions.concat_ws;\nimport org.apache.spark.sql.functions.countDistinct;\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\nimport java.util.Calendar;\n\n//he features that we need are: pkgID or magID, publicationDate, abstract, normalizedTitle, authors(theirs pkgID and displayNames), publisher, publicationYear, publishedIn(displayName, normalizedName, pkgId) and tags(normalizedName, pkgId)\n/*\npkgId\npublicationDate\nabstract\n            normalizedTitle\n            authors {\n              pkgId\n              displayNames\n            }\n            publisher\n            publicationYear\n            publishedIn {\n              displayName\n              normalizedName\n              pkgId\n            }\n            tags {\n                normalizedName\n                pkgId\n            }\n */\nval MAG_NLP =  \"/media/datadisk/Datasets/MAG/20210201/nlp\"\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/20210201/mag\"\nval MAG_ADV =  \"/media/datadisk/Datasets/MAG/20210201/advanced\"\nval outPath = \"/media/ometaxas/nvme/datasets/scitrus/FOS\";\n\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\nval authorsTsvFilename = \"Authors.txt\" \n                              \nval authorSchema = new StructType().\n                add(\"authorId\", LongType, false).\n                add(\"rank\", LongType, true).                \n                add(\"normalizedName\", StringType, true).\n                add(\"displayName\",StringType, true).\n                add(\"lastKnownAffiliationId\", LongType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n\n                \nval authordf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(authorSchema).\n                csv(s\"file://$MAG_HOME/$authorsTsvFilename\").select($\"authorId\",$\"displayName\", $\"paperCount\")\n\n\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\n\nval paperAuthorAffSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"authorId\", LongType, false).                \n                add(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n                \nval paperAuthorAffdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperAuthorAffSchema).\n                csv(s\"file://$MAG_HOME/$paperAuthorsAffTsvFilename\")\n               //.select($\"paperId\", $\"authorId\")\n\nval papersTsvFilename = \"Papers.txt\"\nval paperSchema = new StructType().\n    add(\"paperId\", LongType, false).\n    add(\"magRank\", IntegerType, true).\n    add(\"doi\", StringType, true).\n    add(\"docTypetmp\", StringType, true).\n    add(\"normalizedTitle\", StringType, true).\n    add(\"title\", StringType, false).\n    add(\"bookTitle\", StringType, true).\n    add(\"pubYear\", IntegerType, true).\n    add(\"pubDate\", StringType, true).\n    add(\"onlineDate\", StringType, true).\n    add(\"publisherName\", StringType, true).\n    add(\"journalId\", StringType, true).\n    add(\"conferenceSeriesId\", LongType, true).\n    add(\"conferenceInstancesId\", LongType, true).\n    add(\"volume\", StringType, true).\n    add(\"issue\", StringType, true).\n    add(\"firstPage\", StringType, true).\n    add(\"lastPage\", StringType, true).\n    add(\"referenceCount\", LongType, true).\n    add(\"citationCount\", LongType, true).\n    add(\"estimatedCitation\", LongType, true).\n    add(\"originalVenue\", StringType, true).\n    add(\"familyId\", StringType, true).\n    add(\"createdDate\", DateType, true)\n  \n\nval papersdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                    schema(paperSchema).\n                    csv(s\"file://$MAG_HOME/$papersTsvFilename\")\n        .select($\"paperId\", $\"normalizedTitle\", $\"publisherName\", $\"journalId\", $\"conferenceSeriesId\", $\"pubYear\")    \n\n\n\n\n//val papersdfcnt = papersdf.count()\n//println(papersdfcnt)\n//papersdf.printSchema\n//papersdf.show(5)\n\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval df3 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename1\")\n//df3.printSchema\n//df3.show(5)\n\nval df4 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename2\")\n//df4.printSchema\n//df4.show(5)\n\n\nval udf1 = udf[String, String]((index:String) => {\n    \n    val index2 = org.apache.commons.lang.StringUtils.replace(index, \"\\\\\\\"\", \"\\'\")\n    val indexNumber: String = org.apache.commons.lang.StringUtils.substringBetween(index2, \":\", \",\")\n    if (indexNumber == null) {\n      null\n    }\n    var outputText: StringBuilder = new StringBuilder()\n    try {\n      val textLength: Int = parseInt(indexNumber)\n      val text: Array[String] =  new Array[String](textLength)\n      val tokens: Array[String] =\n        org.apache.commons.lang.StringUtils.substringsBetween(index2, \"\\\"\", \"\\\"\")\n      for (i <- 0 until tokens.length - 2) {\n        if (tokens(i + 2).==(\"\\'\")) {\n          tokens(i + 2) = \"\\\"\"\n        }\n        val tokenPositions: Array[String] =\n          org.apache.commons.lang.StringUtils.substringsBetween(index2, \"[\", \"]\")\n        for (s <- tokenPositions(i).split(\",\")) {\n          if (s.matches(\"(.*)\\\\D(.*)\")) {\n          }\n          val wordPosition: Int = parseInt(s)\n          text(wordPosition) = tokens(i + 2)\n        }\n      }\n      for (j <- 0 until text.length) {\n        if (j < text.length - 1) {\n          outputText.append(text(j)).append(' ')\n        } else {\n          outputText.append(text(j))\n        }\n      }\n      outputText = new StringBuilder(\n        outputText.toString\n          .replaceAll(\"\\'\", \"\\\"\")\n          .replaceAll(\"\\\\w?\\\\\\\\\\\\w\\\\d+\", \" \"))\n    } catch {\n      case e: Exception => {\n        logger.error(\"Error while parsing \")\n        null\n      }\n\n    }\n    outputText.toString}\n    )\n\nval paperabstractsdf1 = df3.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df5.show(5)\n\nval paperabstractsdf2 = df4.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df6.show(5)\n\n//val paperabstractsdf = paperabstractsdf1.union(paperabstractsdf2).dropDuplicates()\n//val abstractsdfcnt = abstractsdf.count()\n//println(s\"abstractsdfcnt\")\n//paperabstractsdf.show(5)\n\nval paperFieldsOfStudyTsvFilename = \"PaperFieldsOfStudy.txt\"\n\nval paperIdFieldsOfStudyschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"score\", DoubleType, true)\n                \nval paperFieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperIdFieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$paperFieldsOfStudyTsvFilename\")\n//paperFieldsOfStudydf.printSchema\n//paperFieldsOfStudydf.show(5)\n\nval fieldsOfStudyTsvFilename = \"FieldsOfStudy.txt\"\n\nval fieldsOfStudyschema = new StructType().\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"name\", StringType, true).\n                add(\"mainType\", StringType, true).\n                add(\"level\", IntegerType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval fieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$fieldsOfStudyTsvFilename\")\n                .select($\"fieldsOfStudyId\", $\"normalizedName\", $\"level\", $\"paperCount\")\n//fieldsOfStudydf.printSchema\n//fieldsOfStudydf.show(5)\n/*\nval paper_fos = paperFieldsOfStudydf.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(paperFieldsOfStudydf(\"paperId\").as(\"paper_fos_paperId\"))\n                .agg( \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),\n                    //sum(when($\"date\" > \"2017-04\", $\"value\")).alias(\"value4\")\n                    collect_set(when($\"level\" > 1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids\"),\n                    collect_set(when($\"level\" ===  0, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl0\"),\n                    collect_set(when($\"level\" ===  1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl1\")\n                    )\n                    .persist(StorageLevel.DISK_ONLY)\n\n//paper_fos.show(5)\n*/\nval confSeriesTsvFilename = \"ConferenceSeries.txt\"\n\nval confSeriesschema = new StructType().\n                add(\"conferenceSeriesId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"conferenceName\", StringType, true).                \n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval confSeriesdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(confSeriesschema).\n                csv(s\"file://$MAG_HOME/$confSeriesTsvFilename\")\n                .select($\"conferenceSeriesId\", $\"conferenceName\")\n\nval journalsTsvFilename =  \"Journals.txt\"\n\nval journalschema = new StructType().\n                add(\"journalId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"journalName\", StringType, true).                \n                add(\"issn\", StringType, true).\n                add(\"publisher\", StringType, true).\n                add(\"webpage\", StringType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval journaldf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(journalschema).\n                csv(s\"file://$MAG_HOME/$journalsTsvFilename\")\n                .select($\"journalId\", $\"journalName\")\n\n\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:03:01+0300",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.concat_ws\nimport org.apache.spark.sql.functions.countDistinct\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\nimport java.util.Calendar\n\u001b[1m\u001b[34mMAG_NLP\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20210201/nlp\n\u001b[1m\u001b[34mMAG_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20210201/mag\n\u001b[1m\u001b[34mMAG_ADV\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/20210201/advanced\n\u001b[1m\u001b[34moutPath\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/ometaxas/nvme/datasets/scitrus/FOS\n\u001b[1m\u001b[34mlog...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274581342_433904073",
   "id": "paragraph_1621274581342_433904073",
   "dateCreated": "2021-05-17T21:03:01+0300",
   "dateStarted": "2021-05-17T21:03:01+0300",
   "dateFinished": "2021-05-17T21:03:12+0300",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n/*\nadd(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n */\nval noResolvedInstitutions = paperAuthorAffdf\n        .filter(($\"affiliationId\" === \"\" || $\"affiliationId\".isNull) && ($\"originalAffiliation\"=!=\"\" && $\"originalAffiliation\".isNotNull))\n        .cache()\n\n\nnoResolvedInstitutions.show(10)\nprintln(noResolvedInstitutions.count())\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:03:17+0300",
   "config": {
    "results": [
     {}
    ],
    "editorHide": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "size": {
        "height": 777.0
       },
       "state": {}
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+----------+-------------+--------------------+-----------------+----------------------------+\n|paperId|  authorId|affiliationId|authorSequenceNumber|   originalAuthor|         originalAffiliation|\n+-------+----------+-------------+--------------------+-----------------+----------------------------+\n|     15| 199142497|         null|                   1|  Robert Münscher|        Heidelberg, Deuts...|\n|     15| 680395887|         null|                   2|    Julia Hormuth|        Reutlingen, Deuts...|\n|    138|2825090221|         null|                   1|   Blackwell John|        CROMPTON & KNOWLE...|\n|    407|2570002773|         null|                   1|       Miller Ian|        School of Arts & ...|\n|    548|2700246010|         null|                   1|        P. Andrik|        Hygieneinstitut M...|\n|    782|2170536514|         null|                   1|Robert E. Hoffman|                     HOFFMAN|\n|    889|2636264142|         null|                   2|     M. Olivereau|        Laboratoire de Ph...|\n|    889|2690928654|         null|                   1|    A. M. Lemoine|        Laboratoire de Ph...|\n|   1276| 360007973|         null|                   1|          達 鶴石|岐阜大学大学院連合農学研究科|\n|   1276|2700330818|         null|                   2|        利男 吉田|              信州大学農学部|\n+-------+----------+-------------+--------------------+-----------------+----------------------------+\nonly showing top 10 rows\n\norg.apache.spark.SparkException: Job 1 cancelled part of cancelled job group zeppelin|anonymous|2FNHZZ9Z6|paragraph_1621274597739_670254703\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1955)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:954)\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:953)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2208)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n  at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2981)\n  at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2980)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2980)\n  ... 47 elided\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274597739_670254703",
   "id": "paragraph_1621274597739_670254703",
   "dateCreated": "2021-05-17T21:03:17+0300",
   "dateStarted": "2021-05-17T21:03:17+0300",
   "dateFinished": "2021-05-17T21:03:53+0300",
   "status": "ABORT"
  },
  {
   "text": "%spark\nnoResolvedInstitutions.show(50, false)",
   "user": "anonymous",
   "dateUpdated": "2021-01-29T19:43:28+0200",
   "config": {
    "editorHide": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-------+----------+-------------+--------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|paperId|authorId  |affiliationId|authorSequenceNumber|originalAuthor        |originalAffiliation                                                                                                                                                                   |\n+-------+----------+-------------+--------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|15     |199142497 |null         |1                   |Robert Münscher       |Heidelberg, Deutschland                                                                                                                                                               |\n|15     |680395887 |null         |2                   |Julia Hormuth         |Reutlingen, Deutschland                                                                                                                                                               |\n|138    |2825090221|null         |1                   |Blackwell John        |CROMPTON & KNOWLES CORPORATION A MA CORP                                                                                                                                              |\n|407    |2570002773|null         |1                   |Miller Ian            |School of Arts & Humanities                                                                                                                                                           |\n|548    |2700246010|null         |1                   |Andrik P              |Hygieneinstitut Miskolc, Ungarn.                                                                                                                                                      |\n|782    |2170536514|null         |1                   |Robert E. Hoffman     |HOFFMAN                                                                                                                                                                               |\n|889    |2690928654|null         |1                   |A. M. Lemoine         |Laboratoire de Physiologie de l'Institut Océanographique, Paris                                                                                                                       |\n|889    |2699070023|null         |2                   |M. Olivereau          |Laboratoire de Physiologie de l'Institut Océanographique, Paris                                                                                                                       |\n|1276   |360007973 |null         |1                   |達 鶴石               |岐阜大学大学院連合農学研究科                                                                                                                                                          |\n|1276   |2700330818|null         |2                   |利男 吉田             |信州大学農学部                                                                                                                                                                        |\n|1314   |1970640371|null         |4                   |S. Ljunghall          |University Hospital                                                                                                                                                                   |\n|1314   |2251021420|null         |2                   |B. Fellström          |University Hospital                                                                                                                                                                   |\n|1314   |2627395902|null         |5                   |B. Vessby             |University Hospital                                                                                                                                                                   |\n|1314   |3059624959|null         |3                   |H. Lithell            |University Hospital                                                                                                                                                                   |\n|1314   |3074609438|null         |1                   |B. G. Danielson       |University Hospital                                                                                                                                                                   |\n|1358   |2301517854|null         |1                   |Hiroyuki Ueda         |Canon Business Machines, Inc. (Costa Mesa, CA)                                                                                                                                        |\n|1358   |2618570631|null         |2                   |Naoki Shimada         |Canon Business Machines, Inc. (Costa Mesa, CA)                                                                                                                                        |\n|1382   |568333046 |null         |2                   |Norbert Koubek        |lehrt Betriebwirtschaftlehre, insbesondere Produktion und Arbeitswirtschaft, an der Bergischen Universität/GH                                                                         |\n|1382   |1180379018|null         |3                   |Gerd R. Wiedemeyer    |Deutschland                                                                                                                                                                           |\n|1382   |2169372911|null         |1                   |Heinz Gester          |Justitiar des Deutschen Gewerkschaftsbundes                                                                                                                                           |\n|1554   |2885971703|null         |1                   |Edward A. Hein        |Harrison, Sydney (all of, CA)                #N#                            Rothstein, Reuben (all of, CA)                #N#                            Kalynchuk, Dinah (all of, CA)|\n|1727   |2023770299|null         |2                   |Herbert D. Thier      |AMERICAN SCIENCE ENG INC,US                                                                                                                                                           |\n|1727   |2165316602|null         |1                   |Marshall A. Montgomery|AMERICAN SCIENCE ENG INC,US                                                                                                                                                           |\n|1727   |2950628121|null         |3                   |John B. Orfei         |AMERICAN SCIENCE ENG INC,US                                                                                                                                                           |\n|1921   |2671105436|null         |1                   |Malloy Dc             |Immunology Department, Maryland Medical laboratory, Inc.                                                                                                                              |\n|2012   |2125377539|null         |1                   |John McGinley         |MCGINLEY                                                                                                                                                                              |\n|2208   |2096138569|null         |7                   |Kimiyasu Shiraki      |富山大学医学部ウイルス学講座                                                                                                                                                          |\n|2208   |2099528057|null         |6                   |Takao Ozaki           |江南厚生病院こども医療センター                                                                                                                                                        |\n|2208   |2099564275|null         |5                   |Naoko Nishimura       |江南厚生病院こども医療センター                                                                                                                                                        |\n|2208   |2112486750|null         |4                   |Suguru Takeuchi       |江南厚生病院こども医療センター                                                                                                                                                        |\n|2208   |2165306425|null         |2                   |Kensei Gotoh          |江南厚生病院こども医療センター                                                                                                                                                        |\n|2208   |2528547620|null         |3                   |Fumihiko Hattori      |江南厚生病院こども医療センター                                                                                                                                                        |\n|2208   |2938529011|null         |1                   |Kazuhiro Horiba       |江南厚生病院こども医療センター                                                                                                                                                        |\n|3008   |2536252418|null         |1                   |Strand T              |Hagavik Orthopaedic Hospital, Bergen, Norway.                                                                                                                                         |\n|3027   |2568320690|null         |1                   |Stephen M. Dominick   |DOMINICK STEPHEN M.                #N#                            CAMPISI ROBERT M.                                                                                                   |\n|3027   |2569293101|null         |2                   |Robert M. Campisi     |DOMINICK STEPHEN M.                #N#                            CAMPISI ROBERT M.                                                                                                   |\n|3073   |2172110720|null         |1                   |H. A. Bock            |Division of Nephrology, Department of Internal MedicineKantonsspital, Basel, Switzerland                                                                                              |\n|3179   |2303844987|null         |2                   |Lothar Rogge          |Universität-GH-Duisburg Fachbereich Mathematik                                                                                                                                        |\n|3179   |2524948875|null         |1                   |Dieter Landers        |Universität Köln                                                                                                                                                                      |\n|3245   |2147454174|null         |1                   |John D. Branch        |BRANCH                                                                                                                                                                                |\n|3699   |2236731196|null         |2                   |Nancy Breen           |Applied Research Program, Division of Cancer Control and Population Sciences, National Cancer Institute, Bethesda, Maryland                                                           |\n|4008   |2660230209|null         |1                   |Suleman Mm            |Directorate of Health Services, Islamabad Capital Territory.                                                                                                                          |\n|4147   |2119876328|null         |1                   |N. Murai              |Kokura Steel Works Sumitomo Metal Ind., Ltd., Konomicho 1, Kokurakitaku, Kitakyushushi, Japan                                                                                         |\n|4937   |1980287250|null         |1                   |Georg Glaeser         |Universität für Angewandte Kunst Wien                                                                                                                                                 |\n|4960   |2024973056|null         |1                   |Magda E. Tawfik       |Polymers & Pigments Department, National Research Centre, Cairo, Egypt                                                                                                                |\n|5249   |2144510979|null         |1                   |Don N. Hanley         |HANLEY                                                                                                                                                                                |\n|5388   |2659745463|null         |1                   |H. Kranz              |Friesenrath b. Aachen,                                                                                                                                                                |\n|5424   |2973651746|null         |1                   |S. M. Nikol’skii      |Institute Academy of Sciences                                                                                                                                                         |\n|5604   |3073798330|null         |1                   |Wood Cliff            |TRIMENSIONS INC                                                                                                                                                                       |\n|5690   |349351047 |null         |1                   |Magat                 |Gastabteilung für Biologie des Kaiser Wilhelm-Instituts, Berlin-Dahlem,                                                                                                               |\n+-------+----------+-------------+--------------------+----------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 50 rows\n\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=88"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611942207993_171306237",
   "id": "paragraph_1611942207993_171306237",
   "dateCreated": "2021-01-29T19:43:27+0200",
   "dateStarted": "2021-01-29T19:43:28+0200",
   "dateFinished": "2021-01-29T19:43:28+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval topauthordf = authordf.filter($\"paperCount\">100 && $\"paperCount\"<500).orderBy($\"paperCount\" desc).limit(5000).cache()\n\ntopauthordf.show(5)\n\nval topPaperidsdf = paperAuthorAffdf.join(broadcast(topauthordf), paperAuthorAffdf(\"authorId\")===topauthordf(\"authorId\"), \"inner\")\n        .select(paperAuthorAffdf(\"paperId\")).dropDuplicates().cache()\n\ntopPaperidsdf.show(5)\nprintln(topPaperidsdf.count())\ntopPaperidsdf.write.parquet(s\"$outPath/topPaperidsdf.parquet\")\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:04:07+0300",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "paperCount",
            "index": 2.0
           },
           "y": {
            "column": "authorId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[33mwarning: \u001b[0mthere was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\norg.apache.spark.SparkException: Job 2 cancelled part of cancelled job group zeppelin|anonymous|2FNHZZ9Z6|paragraph_1621274647111_483412980\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1955)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:954)\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:953)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2208)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:824)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:783)\n  ... 47 elided\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274647111_483412980",
   "id": "paragraph_1621274647111_483412980",
   "dateCreated": "2021-05-17T21:04:07+0300",
   "dateStarted": "2021-05-17T21:04:07+0300",
   "dateFinished": "2021-05-17T21:05:25+0300",
   "status": "ABORT"
  },
  {
   "text": "%spark\nval topFoS = fieldsOfStudydf.filter($\"paperCount\">1000 && $\"paperCount\"<4000 && $\"level\">1).orderBy($\"paperCount\" desc)\n            .select($\"fieldsOfStudyId\").limit(100).cache()\n\ntopFoS.show(5)\n\nval topPaperidsdf = paperFieldsOfStudydf.join(broadcast(topFoS), paperFieldsOfStudydf(\"fieldsOfStudyId\")===topFoS(\"fieldsOfStudyId\"), \"inner\")\n        .select(paperFieldsOfStudydf(\"paperId\")).dropDuplicates().cache()\n\ntopPaperidsdf.show(5)\nprintln(topPaperidsdf.count())\ntopPaperidsdf.write.parquet(s\"$outPath/topPaperidsdf.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T19:32:17+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Pie",
           "values": {
            "column": "fieldsOfStudyId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[33mwarning: \u001b[0mthere was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n+---------------+\n|fieldsOfStudyId|\n+---------------+\n|     2779614053|\n|     2778111679|\n|     2780762811|\n|       21021354|\n|     2779613291|\n+---------------+\nonly showing top 5 rows\n\n+---------+\n|  paperId|\n+---------+\n|223564317|\n|180846668|\n| 55329057|\n|162352970|\n|162356519|\n+---------+\nonly showing top 5 rows\n\n398626\n\u001b[1m\u001b[34mtopFoS\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [fieldsOfStudyId: bigint]\n\u001b[1m\u001b[34mtopPaperidsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: bigint]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=5"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=7"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611768737539_891266080",
   "id": "paragraph_1611768737539_891266080",
   "dateCreated": "2021-01-27T19:32:17+0200",
   "dateStarted": "2021-01-27T19:32:17+0200",
   "dateFinished": "2021-01-27T19:35:02+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaper_fos = paperFieldsOfStudydf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===paperFieldsOfStudydf(\"paperId\"), \"inner\")    \n                    .drop(topPaperidsdf(\"paperId\"))        \n                    .join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                    .withColumn(\"FoS\", struct(fieldsOfStudydf(\"fieldsOfStudyId\"), fieldsOfStudydf(\"normalizedName\"), fieldsOfStudydf(\"level\"), fieldsOfStudydf(\"paperCount\")))                    \n                   // .persist(StorageLevel.DISK_ONLY)\n\n//toppaper_fos.show()\n\nval toppaper_fosgrp = toppaper_fos.groupBy(\"paperId\").agg(collect_list(\"FoS\").alias(\"FoSs\"))\n//.persist(StorageLevel.DISK_ONLY)\n\n//toppaper_fosgrp.show()\ntoppaper_fosgrp.write.parquet(s\"$outPath/toppaper_fosgrp.parquet\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:06:38+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "table": {
         "visibleRow": 5.0
        },
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "normalizedName",
            "index": 5.0
           },
           "y": {
            "column": "paperId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mtoppaper_fos\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [fieldsOfStudyId: bigint, score: double ... 6 more fields]\n\u001b[1m\u001b[34mtoppaper_fosgrp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, FoSs: array<struct<fieldsOfStudyId:bigint,normalizedName:string,level:int,paperCount:bigint>>]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=12"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611770798272_710855049",
   "id": "paragraph_1611770798272_710855049",
   "dateCreated": "2021-01-27T20:06:38+0200",
   "dateStarted": "2021-01-27T20:06:38+0200",
   "dateFinished": "2021-01-27T20:09:32+0200",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\n",
   "id": "",
   "config": {}
  },
  {
   "text": "%spark\nval topauthors = paperAuthorAffdf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===paperAuthorAffdf(\"paperId\"), \"inner\")    \n                    .drop(topPaperidsdf(\"paperId\"))        \n                    .join(authordf, paperAuthorAffdf(\"authorId\")===authordf(\"authorId\"), \"inner\")\n                    .withColumn(\"author\", struct(authordf(\"authorId\"), authordf(\"displayName\"), authordf(\"paperCount\")))                    \n                   // .persist(StorageLevel.DISK_ONLY)\n\n//topauthors.show()\n\nval topauthorsgrp = topauthors.groupBy(\"paperId\").agg(collect_list(\"author\").alias(\"authors\"))\n//.persist(StorageLevel.DISK_ONLY)\n\n//topauthorsgrp.show(5)\ntopauthorsgrp.write.parquet(s\"$outPath/topauthorsgrp.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:06:42+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "table": {
         "columnWidths": {
          "authors": 643.0
         }
        },
        "chart": {
         "series": [
          {
           "type": "Pie",
           "values": {
            "column": "paperId",
            "index": 0.0
           },
           "labels": {
            "column": "authors",
            "index": 1.0
           },
           "showPercents": true
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mtopauthors\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, authorId: bigint ... 4 more fields]\n\u001b[1m\u001b[34mtopauthorsgrp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, authors: array<struct<authorId:bigint,displayName:string,paperCount:bigint>>]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=14"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611770802812_1916288571",
   "id": "paragraph_1611770802812_1916288571",
   "dateCreated": "2021-01-27T20:06:42+0200",
   "dateStarted": "2021-01-27T20:06:42+0200",
   "dateFinished": "2021-01-27T20:31:35+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaperdf = papersdf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===papersdf(\"paperId\"), \"inner\")\n                  .drop(topPaperidsdf(\"paperId\"))\n                 // .persist(StorageLevel.DISK_ONLY)\n\n//toppaperdf.show(5)\ntoppaperdf.write.parquet(s\"$outPath/toppaperdf.parquet\")\n\n\nval toppaper_journalsdf = toppaperdf\n               .join(journaldf, journaldf(\"journalId\")=== toppaperdf(\"journalId\"), \"outer\")\n              .join(confSeriesdf, confSeriesdf(\"conferenceSeriesId\")=== toppaperdf(\"conferenceSeriesId\"), \"outer\")\n                .drop(journaldf(\"journalId\"))\n                .drop(confSeriesdf(\"conferenceSeriesId\"))\n                //.persist(StorageLevel.DISK_ONLY)\n\ntoppaper_journalsdf.write.parquet(s\"$outPath/toppaper_journalsdf.parquet\")\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:16+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table",
        "table": {
         "columnWidths": {
          "normalizedTitle": 225.0
         }
        },
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "normalizedTitle",
            "index": 1.0
           },
           "y": {
            "column": "paperId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mtoppaperdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, normalizedTitle: string ... 4 more fields]\n\u001b[1m\u001b[34mtoppaper_journalsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, normalizedTitle: string ... 6 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=16"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=18"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772876096_1290067306",
   "id": "paragraph_1611772876096_1290067306",
   "dateCreated": "2021-01-27T20:41:16+0200",
   "dateStarted": "2021-01-27T20:41:16+0200",
   "dateFinished": "2021-01-27T21:17:12+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaper_abstractsdf1 = paperabstractsdf1\n        .join(broadcast(topPaperidsdf), paperabstractsdf1(\"paperId\")===topPaperidsdf(\"paperId\"), \"inner\").drop(topPaperidsdf(\"paperId\"))\n        \nval toppaper_abstractsdf2 = paperabstractsdf2\n        .join(broadcast(topPaperidsdf), paperabstractsdf2(\"paperId\")===topPaperidsdf(\"paperId\"), \"inner\").drop(topPaperidsdf(\"paperId\"))\n         \nval toppaper_abstractsdf = toppaper_abstractsdf1.union(toppaper_abstractsdf2).dropDuplicates()\n        //.persist(StorageLevel.DISK_ONLY)\n//toppaper_abstractsdf.show(5)\ntoppaper_abstractsdf.write.parquet(s\"$outPath/toppaper_abstractsdf.parquet\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:27+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mtoppaper_abstractsdf1\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: string, abstract: string]\n\u001b[1m\u001b[34mtoppaper_abstractsdf2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: string, abstract: string]\n\u001b[1m\u001b[34mtoppaper_abstractsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: string, abstract: string]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=20"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772887129_1473427034",
   "id": "paragraph_1611772887129_1473427034",
   "dateCreated": "2021-01-27T20:41:27+0200",
   "dateStarted": "2021-01-27T20:41:27+0200",
   "dateFinished": "2021-01-27T22:33:44+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\n//val outPath = \"$MAG_HOME\";\n//val outPath = \"/media/ometaxas/nvme/datasets\";\n\nval toppaper_abstractsdf = spark.read.parquet(s\"$outPath/toppaper_abstractsdf.parquet\")\n//val toppaperdf= spark.read.parquet(s\"$MAG_HOME/scitrus/toppaperdf.parquet\").dropDuplicates()\nval topauthorsgrp= spark.read.parquet(s\"$outPath/topauthorsgrp.parquet\")\nval toppaper_fosgrp= spark.read.parquet(s\"$outPath/toppaper_fosgrp.parquet\")\nval toppaper_journalsdf = spark.read.parquet(s\"$outPath/toppaper_journalsdf.parquet\")\n\n\nval paper_author = toppaper_journalsdf\n              .join(topauthorsgrp, topauthorsgrp(\"paperId\")===toppaper_journalsdf(\"paperId\"), \"inner\")\n        .drop(topauthorsgrp(\"paperId\"))\n        .persist(StorageLevel.DISK_ONLY)\n        \npaper_author.write.parquet(s\"$outPath/paper_author.parquet\")\n\nval paper_author_fos = paper_author.join(toppaper_fosgrp, toppaper_fosgrp(\"paperId\")=== paper_author(\"paperId\"), \"inner\")\n.drop(toppaper_fosgrp(\"paperId\"))\n        .persist(StorageLevel.DISK_ONLY)\n        \npaper_author_fos.write.parquet(s\"$outPath/paper_author_fos.parquet\")\n    \nval paper_author_fos_abstractsdf = paper_author_fos              \n              .join(toppaper_abstractsdf, toppaper_abstractsdf(\"paperId\")===toppaper_journalsdf(\"paperId\"), \"inner\")            \n                   .drop(toppaper_abstractsdf(\"paperId\"))\n.write.parquet(s\"$outPath/paper_author_fos_abstractsdf.parquet\")\n  //          .persist(StorageLevel.DISK_ONLY)\n\n//paper_author_fos_abstractsdf2.printSchema()    \n//paper_author_fos_abstractsdf2.show(5)\n//println(paper_author_fos_abstractsdf2.count())\n///media/ometaxas/nvme/datasets\n//paper_author_fos_abstractsdf2.coalesce(1).write.json(s\"$MAG_HOME/scitrus/authorPapers.json\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:44+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mtoppaper_abstractsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: string, abstract: string]\n\u001b[1m\u001b[34mtopauthorsgrp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, authors: array<struct<authorId:bigint,displayName:string,paperCount:bigint>>]\n\u001b[1m\u001b[34mtoppaper_fosgrp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, FoSs: array<struct<fieldsOfStudyId:bigint,normalizedName:string,level:int,paperCount:bigint>>]\n\u001b[1m\u001b[34mtoppaper_journalsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, normalizedTitle: string ... 6 more fields]\n\u001b[1m\u001b[34mpaper_author\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: bigint, normalizedTitle: string ... 7 more fields]\n\u001b[1m\u001b[34mpaper_author_...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=21"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=22"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=23"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=24"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=25"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=26"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=27"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772904307_794577835",
   "id": "paragraph_1611772904307_794577835",
   "dateCreated": "2021-01-27T20:41:44+0200",
   "dateStarted": "2021-01-27T21:17:12+0200",
   "dateFinished": "2021-01-27T22:33:55+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paper_author_fos_abstractsdf = spark.read.parquet(s\"$outPath/paper_author_fos_abstractsdf.parquet\")\n\nprintln(paper_author_fos_abstractsdf.count())\npaper_author_fos_abstractsdf.coalesce(1).write.json(s\"$outPath/FoSPapers.json\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T22:37:23+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "330945\n\u001b[1m\u001b[34mpaper_author_fos_abstractsdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [paperId: bigint, normalizedTitle: string ... 9 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=28"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=29"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=30"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611779843816_1004342527",
   "id": "paragraph_1611779843816_1004342527",
   "dateCreated": "2021-01-27T22:37:23+0200",
   "dateStarted": "2021-01-27T22:37:23+0200",
   "dateFinished": "2021-01-27T22:37:31+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paper_author_fos_abstractsdfjson = spark.read.json(s\"$outPath/scitrus/authorPapers.json\")\npaper_author_fos_abstractsdfjson.printSchema()\nprintln(paper_author_fos_abstractsdf.count())\npaper_author_fos_abstractsdfjson.show(5)\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T18:41:29+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "paperId",
            "index": 8.0
           },
           "y": {
            "column": "pubYear",
            "index": 9.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=35"
      },
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=36"
      },
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=37"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611765689362_1962694430",
   "id": "paragraph_1611765689362_1962694430",
   "dateCreated": "2021-01-27T18:41:29+0200",
   "dateStarted": "2021-01-27T18:41:29+0200",
   "dateFinished": "2021-01-27T18:42:17+0200",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nval MAG_NLP =  \"/media/datadisk/Datasets/MAG/2021-08-30/nlp\"\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\nval abstractTsvFilename3 = \"PaperAbstractsInvertedIndex.txt.3\"\nval abstractTsvFilename4 = \"PaperAbstractsInvertedIndex.txt.4\"\nval abstractTsvFilename5 = \"PaperAbstractsInvertedIndex.txt.5\"\n//val abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval df1 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename1\")\n        .select($\"paperId\")\n//df3.printSchema\n//df3.show(5)\n\nval df2 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename2\")\n.select($\"paperId\")\n\nval df3 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename3\")\n.select($\"paperId\")\n\nval df4 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename4\")\n.select($\"paperId\")\n\nval df5 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename5\")\n.select($\"paperId\")\n\n\n\nprintln(df2.count())\nprintln(df3.count())\nprintln(df4.count())\nprintln(df5.count())\n\n",
   "config": {}
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}