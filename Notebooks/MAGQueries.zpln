{
 "paragraphs": [
  {
   "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explictly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\nSPARK_HOME /home/ometaxas/Programs/spark-3.1.2-bin-hadoop3.2\n\n#spark.jars /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/rapids-4-spark_2.12-0.2.0.jar, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/plugins/cudf-0.15-cuda10-1.jar\n\n#spark.sql.warehouse.dir /home/ometaxas/Programs/zeppelin-0.9.0-preview2-bin-all/spark-warehouse\n\n \nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryoserializer.buffer.max 1000M\nspark.driver.memory 60g\nspark.driver.maxResultSize 5g \n\n#spark.kryo.registrator com.nvidia.spark.rapids.GpuKryoRegistrator\n#spark.rapids.sql.concurrentGpuTasks=3\n#spark.rapids.sql.enabled true\n#spark.rapids.memory.pinnedPool.size 2G \n#spark.plugins com.nvidia.spark.SQLPlugin \n\n#spark.locality.wait 0s \n#spark.sql.files.maxPartitionBytes 512m \n#spark.sql.shuffle.partitions 100 \nspark.executor.resource.gpu.amount=1\n#spark.task.resource.gpu.amount=0.25\n\nSPARK_LOCAL_DIRS /media/ometaxas/nvme/spark                                         \n# /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp, /media/datadisk/Datasets/Spark\n#,/media/datadisk/Datasets/Spark \n#/media/datadisk/Datasets/Spark\n\n# set executor memrory 110g\n#spark.executor.memory  30g\n\n\n# set executor number to be 6\n# spark.executor.instances  6\n#spark.executor.cores=4\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode after Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use it after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true\n\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T10:16:21+0200",
   "progress": 0.0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12.0,
    "editorMode": "ace/mode/scala",
    "fontSize": 9.0,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": []
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643962580964_1866583197",
   "id": "paragraph_1643962580964_1866583197",
   "dateCreated": "2022-02-04T10:16:20+0200",
   "dateStarted": "2022-02-04T10:16:21+0200",
   "dateFinished": "2022-02-04T10:16:21+0200",
   "status": "FINISHED"
  },
  {
   "title": "MAG DFs",
   "text": "%spark\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport org.apache.spark.sql.{DataFrame, Dataset}\n\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.{broadcast, concat_ws, countDistinct};\n//import  org.apache.spark.sql.functions._\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer;\nimport java.util.Locale;\nimport org.apache.spark.storage.StorageLevel;\nimport java.util.Calendar;\n\n\nval MAG_NLP =  \"/media/datadisk/Datasets/MAG/2021-11-10/nlp\"\nval MAG_HOME = \"/media/datadisk/Datasets/MAG/2021-11-10/mag\"\nval MAG_ADV =  \"/media/datadisk/Datasets/MAG/2021-11-10/advanced\"\nval outPath =  \"/media/datadisk/Datasets/MAG/2021-11-10/out\"\n\n\n/*\n\nval MAG_NLP =  \"/media/ometaxas/nvme/datasets/MAGsample/nlp\"\nval MAG_HOME = \"/media/ometaxas/nvme/datasets/MAGsample/mag\"\nval MAG_ADV =  \"/media/ometaxas/nvme/datasets/MAGsample/advanced\"\nval outPath =  \"/media/ometaxas/nvme/datasets/MAGsample/out\"\n\n*/\n\nval logger: Logger = LoggerFactory.getLogger(\"MyZeppelinLogger\");\nlogger.info(\"Test my logger\");\n\nval authorsTsvFilename = \"Authors.txt\" \n                              \nval authorSchema = new StructType().\n                add(\"authorId\", LongType, false).\n                add(\"rank\", LongType, true).                \n                add(\"normalizedName\", StringType, true).\n                add(\"displayName\",StringType, true).\n                add(\"lastKnownAffiliationId\", LongType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n\n                \nval authordf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(authorSchema).\n                csv(s\"file://$MAG_HOME/$authorsTsvFilename\").select($\"authorId\",$\"displayName\", $\"paperCount\")\n\n\nval paperAuthorsAffTsvFilename = \"PaperAuthorAffiliations.txt\"\n\nval paperAuthorAffSchema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"authorId\", LongType, false).                \n                add(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n                \nval paperAuthorAffdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperAuthorAffSchema).\n                csv(s\"file://$MAG_HOME/$paperAuthorsAffTsvFilename\").select($\"paperId\", $\"authorId\")\n\nval papersTsvFilename = \"Papers.txt\"\nval paperSchema = new StructType().\n    add(\"paperId\", LongType, false).\n    add(\"magRank\", IntegerType, true).\n    add(\"doi\", StringType, true).\n    add(\"docTypetmp\", StringType, true).\n    add(\"normalizedTitle\", StringType, true).\n    add(\"title\", StringType, false).\n    add(\"bookTitle\", StringType, true).\n    add(\"pubYear\", IntegerType, true).\n    add(\"pubDate\", StringType, true).\n    add(\"onlineDate\", StringType, true).\n    add(\"publisherName\", StringType, true).\n    add(\"journalId\", StringType, true).\n    add(\"conferenceSeriesId\", LongType, true).\n    add(\"conferenceInstancesId\", LongType, true).\n    add(\"volume\", StringType, true).\n    add(\"issue\", StringType, true).\n    add(\"firstPage\", StringType, true).\n    add(\"lastPage\", StringType, true).\n    add(\"referenceCount\", LongType, true).\n    add(\"citationCount\", LongType, true).\n    add(\"estimatedCitation\", LongType, true).\n    add(\"originalVenue\", StringType, true).\n    add(\"familyId\", StringType, true).\n    add(\"createdDate\", DateType, true)\n  \n\nval papersdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                    schema(paperSchema).\n                    csv(s\"file://$MAG_HOME/$papersTsvFilename\")\n        .select($\"paperId\", $\"normalizedTitle\", $\"publisherName\", $\"journalId\", $\"conferenceSeriesId\", $\"pubYear\", $\"pubDate\", $\"magRank\", $\"docTypetmp\", $\"doi\")    \n\n//val papersdfcnt = papersdf.count()\n//println(papersdfcnt)\n//papersdf.printSchema\n//papersdf.show(5)\n\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval df3 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename1\")\n//df3.printSchema\n//df3.show(5)\n\nval df4 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename2\")\n//df4.printSchema\n//df4.show(5)\n\n\nval udf1 = udf[String, String]((index:String) => {\n    \n    val index2 = org.apache.commons.lang.StringUtils.replace(index, \"\\\\\\\"\", \"\\'\")\n    val indexNumber: String = org.apache.commons.lang.StringUtils.substringBetween(index2, \":\", \",\")\n    if (indexNumber == null) {\n      null\n    }\n    var outputText: StringBuilder = new StringBuilder()\n    try {\n      val textLength: Int = parseInt(indexNumber)\n      val text: Array[String] =  new Array[String](textLength)\n      val tokens: Array[String] =\n        org.apache.commons.lang.StringUtils.substringsBetween(index2, \"\\\"\", \"\\\"\")\n      for (i <- 0 until tokens.length - 2) {\n        if (tokens(i + 2).==(\"\\'\")) {\n          tokens(i + 2) = \"\\\"\"\n        }\n        val tokenPositions: Array[String] =\n          org.apache.commons.lang.StringUtils.substringsBetween(index2, \"[\", \"]\")\n        for (s <- tokenPositions(i).split(\",\")) {\n          if (s.matches(\"(.*)\\\\D(.*)\")) {\n          }\n          val wordPosition: Int = parseInt(s)\n          text(wordPosition) = tokens(i + 2)\n        }\n      }\n      for (j <- 0 until text.length) {\n        if (j < text.length - 1) {\n          outputText.append(text(j)).append(' ')\n        } else {\n          outputText.append(text(j))\n        }\n      }\n      outputText = new StringBuilder(\n        outputText.toString\n          .replaceAll(\"\\'\", \"\\\"\")\n          .replaceAll(\"\\\\w?\\\\\\\\\\\\w\\\\d+\", \" \"))\n    } catch {\n      case e: Exception => {\n        logger.error(\"Error while parsing \")\n        null\n      }\n\n    }\n    outputText.toString}\n    )\n\nval paperabstractsdf1 = df3.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df5.show(5)\n\nval paperabstractsdf2 = df4.select($\"paperId\", udf1($\"abstractText\").as(\"abstract\"))\n//df6.show(5)\n\n//val paperabstractsdf = paperabstractsdf1.union(paperabstractsdf2).dropDuplicates()\n//val abstractsdfcnt = abstractsdf.count()\n//println(s\"abstractsdfcnt\")\n//paperabstractsdf.show(5)\n\nval paperFieldsOfStudyTsvFilename = \"PaperFieldsOfStudy.txt\"\n\nval paperIdFieldsOfStudyschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"score\", DoubleType, true)\n                \nval paperFieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperIdFieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$paperFieldsOfStudyTsvFilename\")\n//paperFieldsOfStudydf.printSchema\n//paperFieldsOfStudydf.show(5)\n\nval fieldsOfStudyTsvFilename = \"FieldsOfStudy.txt\"\n\nval fieldsOfStudyschema = new StructType().\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"name\", StringType, true).\n                add(\"mainType\", StringType, true).\n                add(\"level\", IntegerType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval fieldsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldsOfStudyschema).\n                csv(s\"file://$MAG_ADV/$fieldsOfStudyTsvFilename\")\n                .select($\"fieldsOfStudyId\", $\"normalizedName\", $\"level\", $\"paperCount\")\n//fieldsOfStudydf.printSchema\n//fieldsOfStudydf.show(5)\n\nval fieldsOfStudyExtAttrTsvFilename = \"FieldOfStudyExtendedAttributes.txt\"\n\nval fieldsOfStudyExtAttrschema = new StructType().\n                add(\"fieldsOfStudyId\", LongType, false).\n                add(\"AttributeType\", IntegerType, true).\n                add(\"AttributeValue\", StringType, true)\n                \n                \nval fieldsOfStudyExtAttrdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(fieldsOfStudyExtAttrschema).\n                csv(s\"file://$MAG_ADV/$fieldsOfStudyExtAttrTsvFilename\")\n                \n\n/*\n\n\nval paperMeSHTsvFilename = \"PaperMeSH.txt\"\n\nval paperMeSHschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"DescriptorUI\", StringType, false).\n                add(\"DescriptorName\", StringType, false).                \n                add(\"QualifierUI\", StringType, true).\n                add(\"QualifierName\", StringType, true).\n                add(\"IsMajorTopic\", BooleanType, true)\n\n                \nval paperMeSHsOfStudydf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperMeSHschema).\n                csv(s\"file://$MAG_ADV/$paperMeSHTsvFilename\")\n\n*/\n\nval paperReferencesTsvFilename = \"PaperReferences.txt\"\n\nval paperRefsschema = new StructType().\n                add(\"paperId\", LongType, false).\n                add(\"paperReferenceId\", LongType, false)\n                \n                \nval paperReferencesdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(paperRefsschema).\n                csv(s\"file://$MAG_HOME/$paperReferencesTsvFilename\")\n\n\n/*\nval paper_fos = paperFieldsOfStudydf.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(paperFieldsOfStudydf(\"paperId\").as(\"paper_fos_paperId\"))\n                .agg( \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),\n                    //sum(when($\"date\" > \"2017-04\", $\"value\")).alias(\"value4\")\n                    collect_set(when($\"level\" > 1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids\"),\n                    collect_set(when($\"level\" ===  0, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl0\"),\n                    collect_set(when($\"level\" ===  1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl1\")\n                    )\n                    .persist(StorageLevel.DISK_ONLY)\n\n//paper_fos.show(5)\n*/\nval confSeriesTsvFilename = \"ConferenceSeries.txt\"\n\nval confSeriesschema = new StructType().\n                add(\"conferenceSeriesId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"conferenceName\", StringType, true).                \n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval confSeriesdf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(confSeriesschema).\n                csv(s\"file://$MAG_HOME/$confSeriesTsvFilename\")\n                //.select($\"conferenceSeriesId\", $\"conferenceName\")\n\nval journalsTsvFilename =  \"Journals.txt\"\n\nval journalschema = new StructType().\n                add(\"journalId\", LongType, false).\n                add(\"magRank\", IntegerType, true).\n                add(\"normalizedName\", StringType, true).\n                add(\"journalName\", StringType, true).                \n                add(\"issn\", StringType, true).\n                add(\"publisher\", StringType, true).\n                add(\"webpage\", StringType, true).\n                add(\"paperCount\", LongType, true).\n                add(\"paperFamilyCount\", LongType, true).\n                add(\"citationCount\", LongType, true).\n                add(\"createdDate\", DateType, true)\n                \nval journaldf = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(journalschema).\n                csv(s\"file://$MAG_HOME/$journalsTsvFilename\")\n               // .select($\"journalId\", $\"journalName\")\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T10:21:36+0200",
   "progress": 0.0,
   "config": {
    "title": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.apache.spark.sql.functions.{broadcast, concat_ws, countDistinct}\nimport org.apache.commons.lang3.StringUtils\nimport java.text.Normalizer\nimport java.util.Locale\nimport org.apache.spark.storage.StorageLevel\nimport java.util.Calendar\n\u001b[1m\u001b[34mMAG_NLP\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/2021-11-10/nlp\n\u001b[1m\u001b[34mMAG_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/2021-11-10/mag\n\u001b[1m\u001b[34mMAG_ADV\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Datasets/MAG/2021-11-10/advanced\n\u001b[1m\u001b[34moutPath\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/datadisk/Dataset...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643962896520_322177728",
   "id": "paragraph_1643962896520_322177728",
   "dateCreated": "2022-02-04T10:21:36+0200",
   "dateStarted": "2022-02-04T10:21:36+0200",
   "dateFinished": "2022-02-04T10:21:40+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval paperFieldsOfStudydf_zero = paperFieldsOfStudydf.filter($\"score\"===0).cache()\nprintln(paperFieldsOfStudydf_zero.count())\nprintln(paperFieldsOfStudydf_zero.filter($\"fieldsOfStudyId\"===\"133488467\").count())\n/*\n\nval paper_fos = paperFieldsOfStudydf_zero.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf_zero(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(paperFieldsOfStudydf(\"paperId\").as(\"paper_fos_paperId\"))\n                .agg( \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),\n                    //sum(when($\"date\" > \"2017-04\", $\"value\")).alias(\"value4\")\n                    collect_set(when($\"level\" > 1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids\"),\n                    collect_set(when($\"level\" ===  0, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl0\"),\n                    collect_set(when($\"level\" ===  1, paperFieldsOfStudydf(\"fieldsOfStudyId\"))).as(\"fosids_lvl1\")\n                    )\n                    .persist(StorageLevel.DISK_ONLY)\n\n//paper_fos.show(5)\n*/\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T10:43:02+0200",
   "progress": 64.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "138218829\n2572\n\u001b[1m\u001b[34mpaperFieldsOfStudydf_zero\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: bigint, fieldsOfStudyId: bigint ... 1 more field]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643964182130_522234310",
   "id": "paragraph_1643964182130_522234310",
   "dateCreated": "2022-02-04T10:43:02+0200",
   "dateStarted": "2022-02-04T10:43:02+0200",
   "dateFinished": "2022-02-04T10:59:02+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\npaperFieldsOfStudydf_zero.join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf_zero(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                .groupBy(fieldsOfStudydf(\"level\"))\n        .count().show()\n                \n                    //    sum(when($\"date\" > \"2017-03\", $\"value\")).alias(\"value3\"),",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T11:10:25+0200",
   "progress": 91.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-----+--------+\n|level|   count|\n+-----+--------+\n|    1|11273580|\n|    3|44722684|\n|    5| 3120728|\n|    4|11550803|\n|    2|67546789|\n|    0|    4245|\n+-----+--------+\n\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=6"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=7"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=9"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=10"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643965825269_2073117260",
   "id": "paragraph_1643965825269_2073117260",
   "dateCreated": "2022-02-04T11:10:25+0200",
   "dateStarted": "2022-02-04T11:10:25+0200",
   "dateFinished": "2022-02-04T11:10:29+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval lstmpapers = paperFieldsOfStudydf_zero.filter($\"fieldsOfStudyId\"===\"133488467\")\nlstmpapers.join(papersdf, papersdf(\"paperId\") === lstmpapers(\"paperId\"), \"inner\" ).show(20, false)",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T11:56:44+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----------+---------------+-----+----------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+----------+------------------+-------+----------+-------+----------+--------------------------------+\n|paperId   |fieldsOfStudyId|score|paperId   |normalizedTitle                                                                                                                |publisherName                                    |journalId |conferenceSeriesId|pubYear|pubDate   |magRank|docTypetmp|doi                             |\n+----------+---------------+-----+----------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+----------+------------------+-------+----------+-------+----------+--------------------------------+\n|2875325023|133488467      |0.0  |2875325023|predictive analysis of rfid supply chain path using long short term memory lstm recurrent neural networks                      |MECS Publisher                                   |2506422076|null              |2018   |2018-07-08|21205  |Journal   |10.5815/IJWMT.2018.04.05        |\n|2921857201|133488467      |0.0  |2921857201|singing voice conversion with non parallel data                                                                                |IEEE                                             |null      |null              |2019   |2019-03-28|19177  |null      |10.1109/MIPR.2019.00059         |\n|2969365230|133488467      |0.0  |2969365230|application of long short term memory algorithm for prediction of shale gas production in alberta                              |European Association of Geoscientists & Engineers|null      |null              |2019   |2019-09-02|24182  |null      |10.3997/2214-4609.201902192     |\n|3006019540|133488467      |0.0  |3006019540|extraction of relationship between japanese and us interest rates using machine learning methods                               |IEEE                                             |null      |2754506284        |2019   |2019-07-01|20457  |Conference|10.1109/IIAI-AAI.2019.00135     |\n|3112884630|133488467      |0.0  |3112884630|handwritten bangla character recognition using convolutional neural network and bidirectional long short term memory           |Springer, Singapore                              |null      |null              |2021   |2021-01-01|22036  |null      |10.1007/978-981-33-4673-4_8     |\n|3136547436|133488467      |0.0  |3136547436|solar power forecasting with lstm network ensemble                                                                             |WIP                                              |null      |null              |2019   |2019-10-22|22866  |null      |10.4229/EUPVSEC20192019-5CV.4.28|\n|3152189433|133488467      |0.0  |3152189433|deep learning based fault diagnosis in transmission lines via long short term memory networks                                  |null                                             |null      |null              |2021   |2021-03-01|23100  |Thesis    |null                            |\n|3174031655|133488467      |0.0  |3174031655|recurrent long short term memory deep learning based drug consumption analysis and forecasting                                 |null                                             |null      |null              |2019   |2019-01-01|23307  |null      |10.5373/JARDCS/V11SP10/20192865 |\n|3181044968|133488467      |0.0  |3181044968|real time pm10 concentration prediction lstm model based on iot streaming sensor data                                          |The Korea Academia-Industrial cooperation Society|2946866068|null              |2018   |2018-01-01|20728  |Journal   |10.5762/KAIS.2018.19.11.310     |\n|2794343888|133488467      |0.0  |2794343888|a recurrent neural network approach in predicting daily stock prices an application to the sri lankan stock market             |IEEE                                             |null      |2622396909        |2017   |2017-12-01|19606  |Conference|10.1109/ICIINFS.2017.8300345    |\n|2915018205|133488467      |0.0  |2915018205|customer prediction using parking logs with recurrent neural networks                                                          |Atlantis Press                                   |2764679164|null              |2018   |2018-07-31|21218  |Journal   |10.2991/IJNDC.2018.6.3.2        |\n|3025233697|133488467      |0.0  |3025233697|nonlinear system modeling using the takagi sugeno fuzzy model and long short term memory cells                                 |IOS Press                                        |179157397 |null              |2020   |2020-01-01|21789  |Journal   |10.3233/JIFS-200491             |\n|3037644265|133488467      |0.0  |3037644265|on generalization bounds of a family of recurrent neural networks                                                              |null                                             |null      |2622962978        |2018   |2018-09-27|20534  |Conference|null                            |\n|3121661722|133488467      |0.0  |3121661722|wind power interval prediction based on long short term memory network and error prediction                                    |null                                             |null      |null              |2019   |2019-12-01|23307  |null      |null                            |\n|3156193420|133488467      |0.0  |3156193420|challenges and solutions in developing convolutional neural networks and long short term memory networks for industry problems |Springer, Singapore                              |null      |null              |2021   |2021-01-01|23100  |null      |10.1007/978-981-33-6518-6_2     |\n|3158464467|133488467      |0.0  |3158464467|fusing stacked autoencoder and long short term memory for regional multistep ahead flood inundation forecasts                  |Elsevier                                         |55737203  |null              |2021   |2021-07-01|21119  |Journal   |10.1016/J.JHYDROL.2021.126371   |\n|3193502410|133488467      |0.0  |3193502410|an optimized indirect method to estimate groundwater table depth anomalies over europe based on long short term memory networks|null                                             |null      |null              |2020   |2020-12-01|24507  |null      |null                            |\n|3200191661|133488467      |0.0  |3200191661|a performance based study on deep learning algorithms in the effective prediction of breast cancer                             |IEEE                                             |null      |1140449422        |2021   |2021-07-18|22147  |Conference|10.1109/IJCNN52387.2021.9534293 |\n|2574070559|133488467      |0.0  |2574070559|incremental construction of lstm recurrent neural network                                                                      |null                                             |2764799076|null              |2002   |2002-12-01|23531  |Journal   |null                            |\n|2902455138|133488467      |0.0  |2902455138|long short term memory networks for anomaly detection in time series                                                           |null                                             |null      |1126616278        |2015   |2015-01-01|16639  |Conference|null                            |\n+----------+---------------+-----+----------+-------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------+----------+------------------+-------+----------+-------+----------+--------------------------------+\nonly showing top 20 rows\n\n\u001b[1m\u001b[34mlstmpapers\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [paperId: bigint, fieldsOfStudyId: bigint ... 1 more field]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=13"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=14"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643968604734_1159463782",
   "id": "paragraph_1643968604734_1159463782",
   "dateCreated": "2022-02-04T11:56:44+0200",
   "dateStarted": "2022-02-04T11:56:44+0200",
   "dateFinished": "2022-02-04T12:17:21+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nfieldsOfStudydf.filter($\"fieldsOfStudyId\"===\"133488467\").show()",
   "user": "anonymous",
   "dateUpdated": "2022-02-04T11:08:03+0200",
   "progress": 30.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+---------------+--------------------+-----+----------+\n|fieldsOfStudyId|      normalizedName|level|paperCount|\n+---------------+--------------------+-----+----------+\n|      133488467|long short term m...|    4|      2572|\n+---------------+--------------------+-----+----------+\n\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=4"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643965683977_1070106629",
   "id": "paragraph_1643965683977_1070106629",
   "dateCreated": "2022-02-04T11:08:03+0200",
   "dateStarted": "2022-02-04T11:08:03+0200",
   "dateFinished": "2022-02-04T11:08:06+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval fieldsOfStudyExtAttrdfgrp = fieldsOfStudyExtAttrdf\n        .filter( $\"AttributeType\"===3)\n        .groupBy($\"AttributeValue\")          \n        .count()\n        \nprintln(fieldsOfStudyExtAttrdfgrp.filter($\"count\">1).count())\n",
   "user": "anonymous",
   "dateUpdated": "2021-12-17T15:22:37+0200",
   "progress": 86.0,
   "config": {
    "tableHide": false
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {}
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+--------------+-----+\n|AttributeValue|count|\n+--------------+-----+\n|      C1293132|    2|\n|      C0017715|    3|\n|      C0772313|    4|\n|      C0327888|    3|\n|      C0534283|    1|\n|      C0034083|    1|\n|      C0600604|    1|\n|      C4505443|    1|\n|      C1563144|    2|\n|      C4505375|    1|\n|      C0683603|    1|\n|      C0141599|    2|\n|      C0949879|    1|\n|      C0019853|    1|\n|      C0023962|    1|\n|      C0019932|    4|\n|      C0597066|    1|\n|      C1520118|    1|\n|      C3658315|    1|\n|      C0681025|    2|\n+--------------+-----+\nonly showing top 20 rows\n\n62818\n\u001b[1m\u001b[34mfieldsOfStudyExtAttrdfgrp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [AttributeValue: string, count: bigint]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1639747357787_1521068236",
   "id": "paragraph_1639747357787_1521068236",
   "dateCreated": "2021-12-17T15:22:37+0200",
   "dateStarted": "2021-12-17T15:22:37+0200",
   "dateFinished": "2021-12-17T15:22:42+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval paper_types_all = papersdf        \n                .groupBy( $\"docTypetmp\")      \n                .count()\n        \npaper_types_all.show(10)\n        \n",
   "user": "anonymous",
   "dateUpdated": "2022-01-27T16:45:32+0200",
   "progress": 26.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.SparkException: Job 239 cancelled part of cancelled job group zeppelin|anonymous|2GM4YJ5VQ|paragraph_1643294732904_348267752\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2154)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1048)\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1047)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2407)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)\n  ... 81 elided\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=239"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1643294732904_348267752",
   "id": "paragraph_1643294732904_348267752",
   "dateCreated": "2022-01-27T16:45:32+0200",
   "dateStarted": "2022-01-27T16:45:32+0200",
   "dateFinished": "2022-01-27T16:51:48+0200",
   "status": "ABORT"
  },
  {
   "text": "%spark\nval datasets = papersdf\n        .filter( $\"docTypetmp\"===\"Dataset\")        \n                \n        \n\nval paperReferencesdf_onDatasets = paperReferencesdf.join(broadcast(datasets), paperReferencesdf(\"paperId\")===datasets(\"paperId\"), \"inner\")\n        .select(paperReferencesdf(\"paperReferenceId\"))\n\nprintln(paperReferencesdf_onDatasets.count())\n\n        ",
   "user": "anonymous",
   "dateUpdated": "2021-12-10T17:08:51+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1639148931510_1665884934",
   "id": "paragraph_1639148931510_1665884934",
   "dateCreated": "2021-12-10T17:08:51+0200",
   "dateStarted": "2021-12-10T17:08:51+0200",
   "status": "ABORT"
  },
  {
   "text": "%spark\n\n\nval paper_types_all = papersdf\n        .filter($\"pubYear\">\"2011\" && $\"docTypetmp\"=!=\"Patent\")\n                .groupBy( $\"pubYear\")      \n                .count()\n        .orderBy($\"pubYear\")\n\npaper_types_all.coalesce(1).write.mode(\"overwrite\").csv(s\"$outPath/outputPerYear.csv\")\n\nval paper_types = papersdf\n        .filter($\"pubYear\">\"2011\" && $\"docTypetmp\"=!=\"Patent\")\n                .groupBy($\"docTypetmp\", $\"pubYear\")      \n                .count()\n        .orderBy($\"pubYear\", $\"docTypetmp\")\n         //.agg( \n//                   count(\"paperReferenceId\").alias(\"refsCount\")\n  //      )\n        //.cache()\n    \n//paper_types.show()\n\npaper_types.coalesce(1).write.mode(\"overwrite\").csv(s\"$outPath/outputPerTypePerYear.csv\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-24T15:09:01+0200",
   "progress": 0.0,
   "config": {
    "results": [
     {
      "keys": [
       {
        "name": "pubYear",
        "index": 1.0,
        "aggr": "sum"
       }
      ],
      "groups": [],
      "values": [
       {
        "name": "count",
        "index": 2.0,
        "aggr": "avg"
       }
      ],
      "setting": {
       "lineChart": {}
      },
      "mode": "lineChart"
     }
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console"
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=6"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=7"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637759341859_836588513",
   "id": "paragraph_1637759341859_836588513",
   "dateCreated": "2021-11-24T15:09:01+0200",
   "dateStarted": "2021-11-24T15:09:01+0200",
   "dateFinished": "2021-11-24T15:10:48+0200",
   "status": "FINISHED"
  },
  {
   "title": "Test Reading speed",
   "text": "%spark\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\n\nprintln(LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYYMMdd_HHmmss\")))\n\nval MAG_pub_authorsdf = spark.read.parquet(\"/media/datadisk/Datasets/MAG_S2_ORCID/S2_MAG_Orcid.parquet\")\nprintln(\"MAG_pub_authorsdf  cnt:\"+MAG_pub_authorsdf.count())\nprintln(LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYYMMdd_HHmmss\")))\n\nval S2_MAG_df = spark.read.parquet(\"/media/ometaxas/nvme/datasets/MAG_S2/S2_MAG_Orcid_all_2021.parquet\")\nprintln(\"S2_MAG_df  cnt:\"+S2_MAG_df.count())\nprintln(LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYYMMdd_HHmmss\")))\n\nprintln(\"papersdf  cnt:\"+papersdf.count())\nprintln(LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYYMMdd_HHmmss\")))\n/*\nCPU\n20211116_181543\nMAG_pub_authorsdf  cnt:661651048\n20211116_181546 -->\nS2_MAG_df  cnt:721177821\n20211116_181547\npapersdf  cnt:268253235\n20211116_183157 (15 min)\n */\n\n/*\nMAG_pub_authorsdf.show(20)\nMAG_pub_authorsdf.printSchema()\nprintln(\"MAG_pub_authorsdf  cnt:\"+MAG_pub_authorsdf.count())\n*/\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-16T18:43:34+0200",
   "progress": 88.0,
   "config": {
    "title": true,
    "editorHide": false
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=10"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=11"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=12"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=13"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=14"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637081014014_1150823523",
   "id": "paragraph_1637081014014_1150823523",
   "dateCreated": "2021-11-16T18:43:34+0200",
   "dateStarted": "2021-11-16T18:43:34+0200",
   "dateFinished": "2021-11-16T18:43:57+0200",
   "status": "FINISHED"
  },
  {
   "title": "Test Window based aggregation",
   "text": "%spark\nimport org.apache.spark.sql.expressions.Window\n\nval journal_partition_sorted_by_rank = Window.partitionBy(\"journalId\").orderBy($\"magRank\")\nval journal_partition_sorted_by_date = Window.partitionBy(\"journalId\").orderBy($\"pubYear\".desc)\nval journal_id_partition = Window.partitionBy(\"journalId\")\n\nval top_k_papers = papersdf \n            .filter(papersdf(\"journalId\").isNotNull)                   \n            .withColumn(\"rank_within_journal\", row_number().over(journal_partition_sorted_by_rank))\n            .withColumn(\"recency_within_journal\", row_number().over(journal_partition_sorted_by_date))\n            .withColumn(\"paperCnt_in_journal\", count(col(\"paperId\")).over(journal_id_partition))\n            .withColumn(\"threshold_in_journal\", lit(0.2) * count(col(\"paperId\")).over(journal_id_partition))\n            .filter($\"rank_within_journal\"  <= $\"threshold_in_journal\" || $\"recency_within_journal\"  <= $\"threshold_in_journal\")\n\nprintln(papersdf.count())\nprintln(top_k_papers.count())\n//papersdf.groupBy(\"journalId\").count().show(50)\n//top_k_papers.groupBy(\"journalId\").count().show(50)\n\n\ntop_k_papers.show(20)       \n/*\nval journal_paper_fos_df = top_k_papers\n        .join(paperFieldsOfStudydf, top_k_papers(\"paperId\") === paperFieldsOfStudydf(\"paperId\"),  \"inner\")\n        .select(top_k_papers(\"journalId\"), top_k_papers(\"paperId\"), paperFieldsOfStudydf(\"fieldsOfStudyId\"))\n        //.filter(papersdf(\"journalId\").isNotNull)\n*/\nval journal_paper_fos_df = papersdf\n        .join(paperFieldsOfStudydf, papersdf(\"paperId\") === paperFieldsOfStudydf(\"paperId\"),  \"inner\")\n        .select(papersdf(\"journalId\"), papersdf(\"paperId\"), paperFieldsOfStudydf(\"fieldsOfStudyId\"))\n\n\nprintln(journal_paper_fos_df.count())\n\nval   journal_field_of_study_partition = Window.partitionBy(\"journalId\", \"fieldsOfStudyId\")\n\n\nval     journal_fos_weight_df = journal_paper_fos_df\n        .withColumn(\"total_fields_of_study_alias\", count(col(\"fieldsOfStudyId\")).over(journal_id_partition)). \n        withColumn(\"fields_of_study_per_journal_alias\", count(col(\"paperId\")).over(journal_field_of_study_partition)). \n        withColumn(\"field_of_study_normalized_weight\", (col(\"fields_of_study_per_journal_alias\") / col(\"total_fields_of_study_alias\")))\n\njournal_fos_weight_df.show(10)\n\n\n\nval journal_agg_df = journal_fos_weight_df. \n        groupBy(\"journalId\", \"fieldsOfStudyId\").\n        agg(\n            first(col(\"field_of_study_normalized_weight\")).alias(\"field_of_study_normalized_weight\")\n        )\n\njournal_agg_df.show(20)\n \n//val papersds =papersdf.as(\"Paper\")\n///papersds.show(5)\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T12:51:52+0200",
   "progress": 99.0,
   "config": {
    "editorHide": false,
    "title": true,
    "results": [
     {
      "mode": "table"
     }
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table"
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=4"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=5"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=6"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637232712061_2055452840",
   "id": "paragraph_1637232712061_2055452840",
   "dateCreated": "2021-11-18T12:51:52+0200",
   "dateStarted": "2021-11-18T12:51:52+0200",
   "dateFinished": "2021-11-18T12:52:07+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nimport org.apache.spark.sql.{Encoder, Row}\nimport spark.implicits._\n\nval ds = Seq((\"99227100\", \"42222245\", \"2018-04-26\"),\n  (\"99227100\", \"42222245\", \"2018-05-01\"),\n  (\"34011381\", \"42830849\", \"2015-12-20\"),\n  (\"34011381\", \"42830849\", \"2016-11-27\"),\n  (\"34011381\", \"42830849\", \"2016-12-19\"),\n  (\"34011381\", \"42830849\", \"2017-08-05\")).toDS()\n\n//.toDF(\"ckey\", \"twkey\", \"s_date\")\n\nds.show()\n\ndef encoder(columns: Seq[String]): Encoder[Row] = RowEncoder(StructType(columns.map(StructField(_, StringType, nullable = false))))\n\nval outputCols = Seq(\"ckey\", \"twkey\", \"s_date\")\n\nval result = ds.groupByKey(_._1)\n  .flatMapGroups((_, rowsForEach) => {\n    val list1 = scala.collection.mutable.ListBuffer[Row]()\n    for (elem <- rowsForEach) {\n      list1.append(Row(elem._1, elem._2, elem._3))\n    }\n    list1\n  })(encoder(outputCols)).toDF\n\nresult.show()",
   "user": "anonymous",
   "dateUpdated": "2021-11-17T20:00:11+0200",
   "progress": 92.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=146"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=147"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=148"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=149"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=150"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637172011685_1557517488",
   "id": "paragraph_1637172011685_1557517488",
   "dateCreated": "2021-11-17T20:00:11+0200",
   "dateStarted": "2021-11-17T20:00:11+0200",
   "dateFinished": "2021-11-17T20:00:12+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\ncase class Sales(region: String, country: String, itemType: String, salesChannel: String, orderPriority: String, orderDate: java.sql.Date, unitsSold: Integer)\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T17:18:33+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637248713882_818135739",
   "id": "paragraph_1637248713882_818135739",
   "dateCreated": "2021-11-18T17:18:33+0200",
   "dateStarted": "2021-11-18T17:18:33+0200",
   "dateFinished": "2021-11-18T17:18:47+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nimport scala.annotation.tailrec\n  \ncase class RolledUpSales(region: String, orderPriority: String, unitsSold: Integer)\n\n  def rollUpSales(region: String, sales: Iterator[Sales]): Seq[RolledUpSales] = {\n    \n    // 4\n    val sortedDataset = sales.toSeq.sortWith((a, b) => a.orderDate.before(b.orderDate))\n\n    // 5\n    @tailrec\n    def rollUp(items: List[Sales], accumulator: Seq[RolledUpSales]): Seq[RolledUpSales] = {\n      items match {\n        case x::xs =>\n          val matchingPriority = xs.takeWhile(p => p.orderPriority.equalsIgnoreCase(x.orderPriority))\n          val nonMatchingPriority = xs.dropWhile(p => p.orderPriority.equalsIgnoreCase(x.orderPriority))\n          val record = RolledUpSales(region, x.orderPriority, matchingPriority.map(_.unitsSold).foldLeft(x.unitsSold)(_ + _))\n          val rolledUpRecord = record +: accumulator\n          rollUp(nonMatchingPriority, rolledUpRecord)\n        case Nil => accumulator\n      }\n    }\n\n    rollUp(sortedDataset.toList, Seq.empty).reverse\n  }\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T17:18:56+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637248736609_805566614",
   "id": "paragraph_1637248736609_805566614",
   "dateCreated": "2021-11-18T17:18:56+0200",
   "dateStarted": "2021-11-18T17:18:56+0200",
   "dateFinished": "2021-11-18T17:18:57+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n    import spark.implicits._\n    import org.apache.spark.sql.functions._\n\n    \n\n\n    // 2 6/27/2010\n    val sales = spark\n      .read.option(\"delimiter\", \",\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\n      .csv(s\"/media/ometaxas/nvme/datasets/MAGsample/Sales_Records.csv\")\n        .select(\n          $\"Region\".as(\"region\"),\n          $\"Country\".as(\"country\"),\n          $\"Item Type\".as(\"itemType\"),\n          $\"Sales Channel\".as(\"salesChannel\"),\n          $\"Order Priority\".as(\"orderPriority\"),\n          to_date($\"Order Date\", \"M/d/yyyy\").as(\"orderDate\"),\n          $\"Units Sold\".as(\"unitsSold\")).as[Sales]\n\n    \n    // 3\n    //sales.show(10)\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T17:19:11+0200",
   "progress": 100.0,
   "config": {
    "results": [
     {
      "mode": "table"
     }
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table"
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637248751666_83159081",
   "id": "paragraph_1637248751666_83159081",
   "dateCreated": "2021-11-18T17:19:11+0200",
   "dateStarted": "2021-11-18T17:19:11+0200",
   "dateFinished": "2021-11-18T17:19:16+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n    import spark.implicits._\n    \n    \n    val output = sales.groupByKey(item => item.region).flatMapGroups(rollUpSales)\n    output.show(false)",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T17:19:19+0200",
   "progress": 80.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=3"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=4"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637248759522_2094780102",
   "id": "paragraph_1637248759522_2094780102",
   "dateCreated": "2021-11-18T17:19:19+0200",
   "dateStarted": "2021-11-18T17:19:19+0200",
   "dateFinished": "2021-11-18T17:19:22+0200",
   "status": "ERROR"
  },
  {
   "text": "%spark\n\n\ntrait HasFosScore {\n  def fosId: Long\n  def score: Double\n}\n\n\ncase class FosMagPaperDS (\n  paperId:               Long, \n  fosId:                 Long, \n  score:                 Double,\n  magRank:               Int,  \n  pubYear:               Option[Int],  \n  journalId:             String  \n)\nextends HasFosScore\n\ncase class AnnotationStatisticStep0 (\n  journalId:       String,\n  fosId:          Long,\n  paperCount:     Long,\n  scoreSum:       Double,\n  normalizedScore: Double\n)\n\n//def encoder(columns: Seq[String]): Encoder[Row] = RowEncoder(StructType(columns.map(StructField(_, StringType, nullable = false))))\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T14:43:29+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637239408988_23685626",
   "id": "paragraph_1637239408988_23685626",
   "dateCreated": "2021-11-18T14:43:28+0200",
   "dateStarted": "2021-11-18T14:43:29+0200",
   "dateFinished": "2021-11-18T14:43:29+0200",
   "status": "FINISHED"
  },
  {
   "title": "GroupByKey & FlatMapGroups",
   "text": "%spark\n\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nimport org.apache.spark.sql.{Encoder, Row}\n\nval journal_paper_fos_df = papersdf\n        .join(paperFieldsOfStudydf, papersdf(\"paperId\") === paperFieldsOfStudydf(\"paperId\"),  \"inner\")\n        .select(papersdf(\"journalId\"), papersdf(\"paperId\"), paperFieldsOfStudydf(\"fieldsOfStudyId\").as(\"fosId\"), lit(1).as(\"score\"),    papersdf(\"magRank\"), papersdf(\"pubYear\"))\n        .filter(papersdf(\"journalId\").isNotNull)\n              \n\nval journal_paper_fos_ds =  journal_paper_fos_df.as[FosMagPaperDS]\n\njournal_paper_fos_ds.show(20)\n  \n\ndef encoder(columns: Seq[String]): Encoder[Row] = RowEncoder(StructType(columns.map(StructField(_, StringType, nullable = false))))\nval outputCols = Seq(\"ckey\")\n\nval journalFosStepTest:Dataset[FosMagPaperDS] =\n    journal_paper_fos_ds\n      .where(col(\"journalId\").isNotNull)\n      .groupByKey(_.journalId)\n      .flatMapGroups((journalIdOpt: String, it: Iterator[FosMagPaperDS]) => {          \n        /*val list1 = scala.collection.mutable.ListBuffer[Row]()         \n        for (elem <- it) {\n            list1.append(Row(elem.journalId))\n        }        \n        list1*/\n          val list1 = it.toList\n          list1\n    })\n//(encoder(outputCols)).toDF\n  \n\njournalFosStepTest.show(10)\n\n/*\n\n def fosStatistics(entityId: String, allFosList: List[HasFosScore]): List[AnnotationStatisticStep0] = {\n    val allFosSumScore                         = allFosList.foldLeft(0d)(_ + _.score)\n    val fosIdMap: Map[Long, List[HasFosScore]] = allFosList.groupBy(_.fosId)\n    val statisticList: List[AnnotationStatisticStep0] = fosIdMap.map { case (fosId: Long, fosList: List[HasFosScore]) =>\n      val oneFosCount     = fosList.size\n      val oneFosSumScore  = fosList.foldLeft(0d)(_ + _.score)\n      val normalizedScore = oneFosSumScore / allFosSumScore\n      AnnotationStatisticStep0(entityId, fosId, oneFosCount, oneFosSumScore, normalizedScore)\n    }.toList\n    statisticList\n  }\n\n\nval journalFosStep0DS: Dataset[AnnotationStatisticStep0] =\n    journal_paper_fos_ds\n      .where(col(\"journalId\").isNotNull)\n      .groupByKey(_.journalId)\n      .flatMapGroups { (journalIdOpt: String, it: Iterator[FosMagPaper]) =>\n        val journalId  = journalIdOpt\n        val allFosList = it.toList\n        fosStatistics(journalId, allFosList)\n      }\n\njournalFosStep0DS.show(10)\n*/",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T16:15:55+0200",
   "progress": 60.0,
   "config": {
    "tableHide": true,
    "title": true,
    "results": [
     {
      "mode": "table"
     }
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table"
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=10"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637244955307_1866770190",
   "id": "paragraph_1637244955307_1866770190",
   "dateCreated": "2021-11-18T16:15:55+0200",
   "dateStarted": "2021-11-18T16:15:55+0200",
   "dateFinished": "2021-11-18T16:16:00+0200",
   "status": "ERROR"
  },
  {
   "text": "%spark\n\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport spark.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.catalyst.encoders.RowEncoder\nimport org.apache.spark.sql.{Encoder, Row}\n\nval journal_paper_fos_df = papersdf\n        .join(paperFieldsOfStudydf, papersdf(\"paperId\") === paperFieldsOfStudydf(\"paperId\"),  \"inner\")\n        .select(papersdf(\"journalId\"), papersdf(\"paperId\"), paperFieldsOfStudydf(\"fieldsOfStudyId\").as(\"fosId\"), lit(1).as(\"score\"),    papersdf(\"magRank\"), papersdf(\"pubYear\"))\n        .filter(papersdf(\"journalId\").isNotNull)\n              \n\nval journal_paper_fos_ds =  journal_paper_fos_df.as[FosMagPaperDS]\n\n//journal_paper_fos_ds.show(20)\n  \n\n/*\ndef encoder(columns: Seq[String]): Encoder[Row] = RowEncoder(StructType(columns.map(StructField(_, StringType, nullable = false))))\nval outputCols = Seq(\"ckey\")\nval journalFosStepTest =\n    journal_paper_fos_ds\n      .where(col(\"journalId\").isNotNull)\n      .groupByKey(_.journalId)\n      .flatMapGroups((_, rowsForEach) => {          \n        val list1 = scala.collection.mutable.ListBuffer[Row]()         \n        for (elem <- rowsForEach) {\n            list1.append(Row(elem.journalId))\n        }        \n        list1\n    })(encoder(outputCols)).toDF\n  \n\njournalFosStepTest.show(10)\n*/\n/*\n\n def fosStatistics(entityId: String, allFosList: List[HasFosScore])\n //: List[AnnotationStatisticStep0] = \n {\n    val allFosSumScore                         = allFosList.foldLeft(0d)(_ + _.score)\n    val fosIdMap: Map[Long, List[HasFosScore]] = allFosList.groupBy(_.fosId)\n    allFosList \n    /*val statisticList: List[AnnotationStatisticStep0] = fosIdMap.map { case (fosId: Long, fosList: List[HasFosScore]) =>\n      val oneFosCount     = fosList.size\n      val oneFosSumScore  = fosList.foldLeft(0d)(_ + _.score)\n      val normalizedScore = oneFosSumScore / allFosSumScore\n      AnnotationStatisticStep0(entityId, fosId, oneFosCount, oneFosSumScore, normalizedScore)\n    }.toList\n    statisticList\n    */\n     \n    \n  }\n*/\n\n\nval journalFosStep0DS:Dataset[FosMagPaperDS]\n//: Dataset[AnnotationStatisticStep0] \n=\n    journal_paper_fos_ds\n      .where(col(\"journalId\").isNotNull)\n      .groupByKey(_.journalId)\n      .flatMapGroups ((journalIdOpt: String, it:Iterator[FosMagPaperDS]) =>\n            {\n              //val journalId  = journalIdOpt\n              //   val allFosList = it.toList\n               // allFosList\n                it\n        //fosStatistics(journalId, allFosList)\n      })\n\njournalFosStep0DS.count()\n",
   "user": "anonymous",
   "dateUpdated": "2021-11-18T14:33:55+0200",
   "progress": 53.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=0"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1637238835682_151826577",
   "id": "paragraph_1637238835682_151826577",
   "dateCreated": "2021-11-18T14:33:55+0200",
   "dateStarted": "2021-11-18T14:33:55+0200",
   "dateFinished": "2021-11-18T14:34:04+0200",
   "status": "ERROR"
  },
  {
   "text": "%spark\n/*\nadd(\"affiliationId\", LongType, true).\n                add(\"authorSequenceNumber\",IntegerType, true).\n                add(\"originalAuthor\", StringType, true).\n                add(\"originalAffiliation\", StringType, true)\n\n */\nval noResolvedInstitutions = paperAuthorAffdf\n        .filter(($\"affiliationId\" === \"\" || $\"affiliationId\".isNull) && ($\"originalAffiliation\"=!=\"\" && $\"originalAffiliation\".isNotNull))\n        .cache()\n\n\nnoResolvedInstitutions.show(10)\nprintln(noResolvedInstitutions.count())\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:03:17+0300",
   "config": {
    "results": [
     {}
    ],
    "editorHide": false,
    "tableHide": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274597739_670254703",
   "id": "paragraph_1621274597739_670254703",
   "dateCreated": "2021-05-17T21:03:17+0300",
   "dateStarted": "2021-05-17T21:03:17+0300",
   "dateFinished": "2021-05-17T21:03:53+0300",
   "status": "ABORT"
  },
  {
   "text": "%spark\nnoResolvedInstitutions.show(50, false)",
   "user": "anonymous",
   "dateUpdated": "2021-01-29T19:43:28+0200",
   "config": {
    "editorHide": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=88"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611942207993_171306237",
   "id": "paragraph_1611942207993_171306237",
   "dateCreated": "2021-01-29T19:43:27+0200",
   "dateStarted": "2021-01-29T19:43:28+0200",
   "dateFinished": "2021-01-29T19:43:28+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\nval topauthordf = authordf.filter($\"paperCount\">100 && $\"paperCount\"<500).orderBy($\"paperCount\" desc).limit(5000).cache()\n\ntopauthordf.show(5)\n\nval topPaperidsdf = paperAuthorAffdf.join(broadcast(topauthordf), paperAuthorAffdf(\"authorId\")===topauthordf(\"authorId\"), \"inner\")\n        .select(paperAuthorAffdf(\"paperId\")).dropDuplicates().cache()\n\ntopPaperidsdf.show(5)\nprintln(topPaperidsdf.count())\ntopPaperidsdf.write.parquet(s\"$outPath/topPaperidsdf.parquet\")\n\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-05-17T21:04:07+0300",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "paperCount",
            "index": 2.0
           },
           "y": {
            "column": "authorId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://LAPTOP-N7E9V9OJ.station:4040/jobs/job?id=2"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1621274647111_483412980",
   "id": "paragraph_1621274647111_483412980",
   "dateCreated": "2021-05-17T21:04:07+0300",
   "dateStarted": "2021-05-17T21:04:07+0300",
   "dateFinished": "2021-05-17T21:05:25+0300",
   "status": "ABORT"
  },
  {
   "text": "%spark\nval topFoS = fieldsOfStudydf.filter($\"paperCount\">1000 && $\"paperCount\"<4000 && $\"level\">1).orderBy($\"paperCount\" desc)\n            .select($\"fieldsOfStudyId\").limit(100).cache()\n\ntopFoS.show(5)\n\nval topPaperidsdf = paperFieldsOfStudydf.join(broadcast(topFoS), paperFieldsOfStudydf(\"fieldsOfStudyId\")===topFoS(\"fieldsOfStudyId\"), \"inner\")\n        .select(paperFieldsOfStudydf(\"paperId\")).dropDuplicates().cache()\n\ntopPaperidsdf.show(5)\nprintln(topPaperidsdf.count())\ntopPaperidsdf.write.parquet(s\"$outPath/topPaperidsdf.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T19:32:17+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Pie",
           "values": {
            "column": "fieldsOfStudyId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=5"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=7"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=8"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=9"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611768737539_891266080",
   "id": "paragraph_1611768737539_891266080",
   "dateCreated": "2021-01-27T19:32:17+0200",
   "dateStarted": "2021-01-27T19:32:17+0200",
   "dateFinished": "2021-01-27T19:35:02+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaper_fos = paperFieldsOfStudydf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===paperFieldsOfStudydf(\"paperId\"), \"inner\")    \n                    .drop(topPaperidsdf(\"paperId\"))        \n                    .join(broadcast(fieldsOfStudydf), paperFieldsOfStudydf(\"fieldsOfStudyId\")=== fieldsOfStudydf(\"fieldsOfStudyId\"), \"inner\")\n                    .withColumn(\"FoS\", struct(fieldsOfStudydf(\"fieldsOfStudyId\"), fieldsOfStudydf(\"normalizedName\"), fieldsOfStudydf(\"level\"), fieldsOfStudydf(\"paperCount\")))                    \n                   // .persist(StorageLevel.DISK_ONLY)\n\n//toppaper_fos.show()\n\nval toppaper_fosgrp = toppaper_fos.groupBy(\"paperId\").agg(collect_list(\"FoS\").alias(\"FoSs\"))\n//.persist(StorageLevel.DISK_ONLY)\n\n//toppaper_fosgrp.show()\ntoppaper_fosgrp.write.parquet(s\"$outPath/toppaper_fosgrp.parquet\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:06:38+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "table": {
         "visibleRow": 5.0
        },
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "normalizedName",
            "index": 5.0
           },
           "y": {
            "column": "paperId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=12"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611770798272_710855049",
   "id": "paragraph_1611770798272_710855049",
   "dateCreated": "2021-01-27T20:06:38+0200",
   "dateStarted": "2021-01-27T20:06:38+0200",
   "dateFinished": "2021-01-27T20:09:32+0200",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\n",
   "id": "",
   "config": {}
  },
  {
   "text": "%spark\nval topauthors = paperAuthorAffdf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===paperAuthorAffdf(\"paperId\"), \"inner\")    \n                    .drop(topPaperidsdf(\"paperId\"))        \n                    .join(authordf, paperAuthorAffdf(\"authorId\")===authordf(\"authorId\"), \"inner\")\n                    .withColumn(\"author\", struct(authordf(\"authorId\"), authordf(\"displayName\"), authordf(\"paperCount\")))                    \n                   // .persist(StorageLevel.DISK_ONLY)\n\n//topauthors.show()\n\nval topauthorsgrp = topauthors.groupBy(\"paperId\").agg(collect_list(\"author\").alias(\"authors\"))\n//.persist(StorageLevel.DISK_ONLY)\n\n//topauthorsgrp.show(5)\ntopauthorsgrp.write.parquet(s\"$outPath/topauthorsgrp.parquet\")\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:06:42+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "table": {
         "columnWidths": {
          "authors": 643.0
         }
        },
        "chart": {
         "series": [
          {
           "type": "Pie",
           "values": {
            "column": "paperId",
            "index": 0.0
           },
           "labels": {
            "column": "authors",
            "index": 1.0
           },
           "showPercents": true
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=14"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611770802812_1916288571",
   "id": "paragraph_1611770802812_1916288571",
   "dateCreated": "2021-01-27T20:06:42+0200",
   "dateStarted": "2021-01-27T20:06:42+0200",
   "dateFinished": "2021-01-27T20:31:35+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaperdf = papersdf.join(broadcast(topPaperidsdf), topPaperidsdf(\"paperId\")===papersdf(\"paperId\"), \"inner\")\n                  .drop(topPaperidsdf(\"paperId\"))\n                 // .persist(StorageLevel.DISK_ONLY)\n\n//toppaperdf.show(5)\ntoppaperdf.write.parquet(s\"$outPath/toppaperdf.parquet\")\n\n\nval toppaper_journalsdf = toppaperdf\n               .join(journaldf, journaldf(\"journalId\")=== toppaperdf(\"journalId\"), \"outer\")\n              .join(confSeriesdf, confSeriesdf(\"conferenceSeriesId\")=== toppaperdf(\"conferenceSeriesId\"), \"outer\")\n                .drop(journaldf(\"journalId\"))\n                .drop(confSeriesdf(\"conferenceSeriesId\"))\n                //.persist(StorageLevel.DISK_ONLY)\n\ntoppaper_journalsdf.write.parquet(s\"$outPath/toppaper_journalsdf.parquet\")\n\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:16+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Table",
        "table": {
         "columnWidths": {
          "normalizedTitle": 225.0
         }
        },
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "normalizedTitle",
            "index": 1.0
           },
           "y": {
            "column": "paperId",
            "index": 0.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=16"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=18"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772876096_1290067306",
   "id": "paragraph_1611772876096_1290067306",
   "dateCreated": "2021-01-27T20:41:16+0200",
   "dateStarted": "2021-01-27T20:41:16+0200",
   "dateFinished": "2021-01-27T21:17:12+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval toppaper_abstractsdf1 = paperabstractsdf1\n        .join(broadcast(topPaperidsdf), paperabstractsdf1(\"paperId\")===topPaperidsdf(\"paperId\"), \"inner\").drop(topPaperidsdf(\"paperId\"))\n        \nval toppaper_abstractsdf2 = paperabstractsdf2\n        .join(broadcast(topPaperidsdf), paperabstractsdf2(\"paperId\")===topPaperidsdf(\"paperId\"), \"inner\").drop(topPaperidsdf(\"paperId\"))\n         \nval toppaper_abstractsdf = toppaper_abstractsdf1.union(toppaper_abstractsdf2).dropDuplicates()\n        //.persist(StorageLevel.DISK_ONLY)\n//toppaper_abstractsdf.show(5)\ntoppaper_abstractsdf.write.parquet(s\"$outPath/toppaper_abstractsdf.parquet\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:27+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=20"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772887129_1473427034",
   "id": "paragraph_1611772887129_1473427034",
   "dateCreated": "2021-01-27T20:41:27+0200",
   "dateStarted": "2021-01-27T20:41:27+0200",
   "dateFinished": "2021-01-27T22:33:44+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\n\n//val outPath = \"$MAG_HOME\";\n//val outPath = \"/media/ometaxas/nvme/datasets\";\n\nval toppaper_abstractsdf = spark.read.parquet(s\"$outPath/toppaper_abstractsdf.parquet\")\n//val toppaperdf= spark.read.parquet(s\"$MAG_HOME/scitrus/toppaperdf.parquet\").dropDuplicates()\nval topauthorsgrp= spark.read.parquet(s\"$outPath/topauthorsgrp.parquet\")\nval toppaper_fosgrp= spark.read.parquet(s\"$outPath/toppaper_fosgrp.parquet\")\nval toppaper_journalsdf = spark.read.parquet(s\"$outPath/toppaper_journalsdf.parquet\")\n\n\nval paper_author = toppaper_journalsdf\n              .join(topauthorsgrp, topauthorsgrp(\"paperId\")===toppaper_journalsdf(\"paperId\"), \"inner\")\n        .drop(topauthorsgrp(\"paperId\"))\n        .persist(StorageLevel.DISK_ONLY)\n        \npaper_author.write.parquet(s\"$outPath/paper_author.parquet\")\n\nval paper_author_fos = paper_author.join(toppaper_fosgrp, toppaper_fosgrp(\"paperId\")=== paper_author(\"paperId\"), \"inner\")\n.drop(toppaper_fosgrp(\"paperId\"))\n        .persist(StorageLevel.DISK_ONLY)\n        \npaper_author_fos.write.parquet(s\"$outPath/paper_author_fos.parquet\")\n    \nval paper_author_fos_abstractsdf = paper_author_fos              \n              .join(toppaper_abstractsdf, toppaper_abstractsdf(\"paperId\")===toppaper_journalsdf(\"paperId\"), \"inner\")            \n                   .drop(toppaper_abstractsdf(\"paperId\"))\n.write.parquet(s\"$outPath/paper_author_fos_abstractsdf.parquet\")\n  //          .persist(StorageLevel.DISK_ONLY)\n\n//paper_author_fos_abstractsdf2.printSchema()    \n//paper_author_fos_abstractsdf2.show(5)\n//println(paper_author_fos_abstractsdf2.count())\n///media/ometaxas/nvme/datasets\n//paper_author_fos_abstractsdf2.coalesce(1).write.json(s\"$MAG_HOME/scitrus/authorPapers.json\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T20:41:44+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=21"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=22"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=23"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=24"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=25"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=26"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=27"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611772904307_794577835",
   "id": "paragraph_1611772904307_794577835",
   "dateCreated": "2021-01-27T20:41:44+0200",
   "dateStarted": "2021-01-27T21:17:12+0200",
   "dateFinished": "2021-01-27T22:33:55+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paper_author_fos_abstractsdf = spark.read.parquet(s\"$outPath/paper_author_fos_abstractsdf.parquet\")\n\nprintln(paper_author_fos_abstractsdf.count())\npaper_author_fos_abstractsdf.coalesce(1).write.json(s\"$outPath/FoSPapers.json\")",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T22:37:23+0200",
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=28"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=29"
      },
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=30"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611779843816_1004342527",
   "id": "paragraph_1611779843816_1004342527",
   "dateCreated": "2021-01-27T22:37:23+0200",
   "dateStarted": "2021-01-27T22:37:23+0200",
   "dateFinished": "2021-01-27T22:37:31+0200",
   "status": "FINISHED"
  },
  {
   "text": "%spark\nval paper_author_fos_abstractsdfjson = spark.read.json(s\"$outPath/scitrus/authorPapers.json\")\npaper_author_fos_abstractsdfjson.printSchema()\nprintln(paper_author_fos_abstractsdf.count())\npaper_author_fos_abstractsdfjson.show(5)\n",
   "user": "anonymous",
   "dateUpdated": "2021-01-27T18:41:29+0200",
   "config": {
    "results": [
     {}
    ]
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "state": {
        "currentPage": "Console",
        "chart": {
         "series": [
          {
           "type": "Line",
           "x": {
            "column": "paperId",
            "index": 8.0
           },
           "y": {
            "column": "pubYear",
            "index": 9.0
           }
          }
         ]
        }
       }
      }
     }
    },
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=35"
      },
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=36"
      },
      {
       "jobUrl": "http://192.168.2.7:4040/jobs/job?id=37"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1611765689362_1962694430",
   "id": "paragraph_1611765689362_1962694430",
   "dateCreated": "2021-01-27T18:41:29+0200",
   "dateStarted": "2021-01-27T18:41:29+0200",
   "dateFinished": "2021-01-27T18:42:17+0200",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "%spark\nimport org.apache.spark.sql.types._\nimport org.apache.commons.lang.StringUtils\nimport java.lang.Integer.parseInt\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nval MAG_NLP =  \"/media/datadisk/Datasets/MAG/2021-08-30/nlp\"\nval abstractTsvFilename1 = \"PaperAbstractsInvertedIndex.txt.1\"\nval abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\nval abstractTsvFilename3 = \"PaperAbstractsInvertedIndex.txt.3\"\nval abstractTsvFilename4 = \"PaperAbstractsInvertedIndex.txt.4\"\nval abstractTsvFilename5 = \"PaperAbstractsInvertedIndex.txt.5\"\n//val abstractTsvFilename2 = \"PaperAbstractsInvertedIndex.txt.2\"\n\nval schema = new StructType().\n                add(\"paperId\", StringType, false).\n                add(\"abstractText\", StringType, false)\n                \nval df1 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename1\")\n        .select($\"paperId\")\n//df3.printSchema\n//df3.show(5)\n\nval df2 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename2\")\n.select($\"paperId\")\n\nval df3 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename3\")\n.select($\"paperId\")\n\nval df4 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename4\")\n.select($\"paperId\")\n\nval df5 = spark.read.options(Map(\"sep\"->\"\\t\", \"header\"-> \"false\")).\n                schema(schema).\n                csv(s\"file://$MAG_NLP/$abstractTsvFilename5\")\n.select($\"paperId\")\n\n\n\nprintln(df2.count())\nprintln(df3.count())\nprintln(df4.count())\nprintln(df5.count())\n\n",
   "config": {}
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}