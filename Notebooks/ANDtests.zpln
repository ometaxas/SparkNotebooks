{
 "paragraphs": [
  {
   "text": "%spark.conf\n\n# It is strongly recommended to set SPARK_HOME explictly instead of using the embedded spark of Zeppelin. As the function of embedded spark of Zeppelin is limited and can only run in local mode.\nSPARK_HOME /home/ometaxas/Programs/spark-3.1.2-bin-hadoop3.2\n#SPARK_HOME /home/ometaxas/Programs/spark-3.2.0-bin-hadoop3.2\n#com.github.haifengl:smile-scala_2.12:2.5.3,com.databricks:spark-xml_2.12:0.10.0,com.github.mrpowers:spark-stringmetric_2.12:0.3.0\n#spark.jars /home/ometaxas/Programs/spark-3.1.2-bin-hadoop3.2/plugins/cudf-21.10.0-cuda11.jar, /home/ometaxas/Programs/spark-3.0\n#.1-bin-hadoop3.2/plugins/rapids-4-spark_2.12-21.10.0.jar\n\n#spark.sql.warehouse.dir /home/ometaxas/Programs/zeppelin-0.9.0-preview2-bin-all/spark-warehouse\n\n\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.kryoserializer.buffer.max 1000M\nspark.driver.memory 100g\nspark.driver.maxResultSize 5g \n\n#spark.kryo.registrator com.nvidia.spark.rapids.GpuKryoRegistrator\n#spark.rapids.sql.concurrentGpuTasks=2\n#spark.rapids.sql.enabled true\n#spark.plugins com.nvidia.spark.SQLPlugin\n#spark.rapids.memory.pinnedPool.size 2G \n#spark.locality.wait 0s \n#spark.sql.files.maxPartitionBytes 512m \n#spark.sql.shuffle.partitions 100 \n#spark.executor.resource.gpu.amount=1\n\nspark.executor.defaultJavaOptions -XX:+UseG1GC \n#-XX:G1HeapRegionSize=32 -XX:ConcGCThreads=16 -XX:InitiatingHeapOccupancyPercent=70\n#spark.executor.defaultJavaOptions -XX:+UseParallelGC \n\n#spark.jars.packages com.microsoft.onnxruntime:onnxruntime:1.10.0 \n#com.microsoft.onnxruntime com.microsoft.azure:synapseml_2.12:0.9.4\n#spark.jars.repositories https://mmlspark.azureedge.net/maven\n\nSPARK_LOCAL_DIRS /media/ometaxas/nvme/spark\n#, /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp\n                                             \n# /home/ometaxas/Programs/spark-3.0.1-bin-hadoop3.2/tmp,/media/datadisk/Datasets/Spark\n#,/media/datadisk/Datasets/Spark \n#/media/datadisk/Datasets/Spark\n\n# set executor memrory 110g\n# spark.executor.memory  60g\n\n\n# set executor number to be 6\n# spark.executor.instances  6\n\n\n# Uncomment the following line if you want to use yarn-cluster mode (It is recommended to use yarn-cluster mode after Zeppelin 0.8, as the driver will run on the remote host of yarn cluster which can mitigate memory pressure of zeppelin server)\n# master yarn-cluster\n\n# Uncomment the following line if you want to use yarn-client mode (It is not recommended to use it after 0.8. Because it would launch the driver in the same host of zeppelin server which will increase memory pressure of zeppelin server)\n# master yarn-client\n\n# Uncomment the following line to enable HiveContext, and also put hive-site.xml under SPARK_CONF_DIR\n# zeppelin.spark.useHiveContext true\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-14T11:06:18+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": []
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644829578714_536183695",
   "id": "paragraph_1644829578714_536183695",
   "dateCreated": "2022-02-14T11:06:18+0200",
   "dateStarted": "2022-02-14T11:06:18+0200",
   "dateFinished": "2022-02-14T11:06:18+0200",
   "status": "FINISHED"
  },
  {
   "text": "\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.storage.StorageLevel\n// cartesian 5m.44\n// broadcastJoin / array 1.04\n// sortMerge / array 1.14\n// 48sec using cartesian on DF i,j indices  63.5GB\n// 38sec broadcast cartesian on DF i,j indices 49GB\n\n//33sec with broadcast / renamed datasets / selecting only features 26.8GB !!\n\nval OA_HOME = \"/media/ometaxas/nvme/datasets/AND_sample_ds/baseSignature1\"\nval signdf = spark.read\n        .json(s\"file://$OA_HOME\")\n        .select($\"affiliationIds\", $\"fosIdsLevel0\",$\"fosIdsLevel1\", $\"authorNormNames\", $\"magAuthorId\", $\"orcId\", $\"s2AuthorId\",$\"paperVector\")\n        .withColumn(\"id\",monotonically_increasing_id())\n        //.cache()\n        //.withColumn(\"vector\", convertArrayToVector($\"paperVector\"))\n        //.drop(\"paperVector\")\n        //.cache() \n\n//println(\"signdf  cnt:\"+signdf.count())\n//signdf.printSchema()\n//signdf.show(10)\n\n//Create pairwise signature records (diagonal matrix)\n\n// dummy approach --> cartesian and then filtering upper diagonal matrix --> ~ 6min for 10K signatures\n//val signdistdf = signdf.crossJoin(signdf2)\n//              .filter(signdf(\"id\") > signdf2(\"id_\"))\n\n\n// more efficient approach --> create matrix coordinate i,j of the diagonal matrix and then two left outer joins to populate signature records  \n\n// prepare indices\nval count = signdf.count().toInt\n\nval dfi = (0 until count).toDF(\"id\")\nval dfj = (0 until count).toDF(\"id_\")\n\nval indices = dfi.crossJoin(dfj)\n                .filter(dfi(\"id\") > dfj(\"id_\"))\n       // .cache()\n\n// the following approach is significantly slower and needs more memory  \n//val indices = sc.parallelize(for(i <- 0L until count; j <- 0L until count; if i > j) yield (i, j)).toDF()\n//        .select($\"_1\".as(\"id\"),$\"_2\".as(\"id_\"))\n     \n//println(\"indices  cnt:\"+indices.count())\n//indices.printSchema()\n//indices.show(5)\n\nvar signdf2 = signdf\n  for(col <- signdf.columns){\n    signdf2 = signdf2.withColumnRenamed(col,col.concat(\"_\"))\n  }\n\nval signdistdf2 = indices.join(broadcast(signdf),signdf(\"id\")===indices(\"id\"),\"left_outer\")\n        .drop(signdf(\"id\"))\n    \nval signdistdf = signdistdf2.join(broadcast(signdf2),signdf2(\"id_\")===signdistdf2(\"id_\"),\"left_outer\")\n    .drop(signdf2(\"id_\"))\n\n/*  // this leads to broadcastnestedloop join in the second join --> 11min for 10K \n        .select(signdistdf2(\"affiliationIds\"), signdistdf2(\"fosIdsLevel0\"),signdistdf2(\"fosIdsLevel1\"), signdistdf2(\"authorNormNames\"), signdistdf(\"magAuthorId\"), signdistdf2(\"orcId\"), signdistdf2(\"s2AuthorId\"),signdistdf2(\"paperVector\"),\n        signdf(\"affiliationIds\").as(\"affiliationIds_\"), signdf(\"fosIdsLevel0\").as(\"fosIdsLevel0_\"),signdf(\"fosIdsLevel1\").as(\"fosIdsLevel1_\"), signdf(\"authorNormNames\").as(\"authorNormNames_\"), signdf(\"magAuthorId\").as(\"magAuthorId_\"), signdf(\"orcId\").as(\"orcId_\"), signdf(\"s2AuthorId\").as(\"s2AuthorId_\"),signdf(\"paperVector\").as(\"paperVector_\"))\n//val signdistdf2 = indices.join(broadcast(signdf),signdf(\"id\")===indices(\"id\"),\"left_outer\")\n\n */\n//val signdistdf = signdistdf2.join(broadcast(signdf2),signdf2(\"id_\")===signdistdf2(\"id_\"),\"left_outer\")\n\n//signdistdf.cache()\n\n//println(\"signdistdf  cnt:\"+signdistdf.count())\n//signdistdf.printSchema()\n\n\n//Compute features Jaccard, cosine etc\nval distdf= signdistdf\n        \n         .withColumn(\"jaccard_aff\", size(array_intersect($\"affiliationIds\", $\"affiliationIds_\")) / size(array_union($\"affiliationIds\", $\"affiliationIds_\")) )\n        .withColumn(\"jaccard_fos_0\", size(array_intersect($\"fosIdsLevel0\", $\"fosIdsLevel0_\")) / size(array_union($\"fosIdsLevel0\", $\"fosIdsLevel0_\")) )\n        .withColumn(\"jaccard_fos_1\", size(array_intersect($\"fosIdsLevel1\", $\"fosIdsLevel1_\")) / size(array_union($\"fosIdsLevel1\", $\"fosIdsLevel1_\")) )\n        .withColumn(\"author_names\", size(array_intersect($\"authorNormNames\", $\"authorNormNames_\")) / size(array_union($\"authorNormNames\", $\"authorNormNames_\")) )\n        .withColumn(\"dot\",  expr(\"aggregate(arrays_zip(paperVector, paperVector_), 0D, (acc, x) -> acc + (x.paperVector * x.paperVector_))\")) \n        .withColumn(\"norm1\", expr(\"sqrt(aggregate(paperVector, 0D, (acc, x) -> acc + (x * x)))\")) \n        .withColumn(\"norm2\", expr(\"sqrt(aggregate(paperVector_, 0D, (acc, x) -> acc + (x * x)))\"))\n        .withColumn(\"emb_sim\", expr(\"dot / (norm1 * norm2)\"))\n        .select($\"id\",$\"id_\",$\"jaccard_aff\",$\"jaccard_fos_0\",$\"jaccard_fos_1\",$\"author_names\",$\"emb_sim\")\n        //.cache()\n        //.withColumn(\"emb_sim2\", cosineSimilarityUdf($\"paperVector\",$\"paperVector_\"))\n        //.persist(StorageLevel.DISK_ONLY)        \n        .write.mode(\"overwrite\").parquet(s\"$OA_HOME/distdf.parquet\")\n      \n  //distdf.printSchema()\n//distdf.show(10)\n//println(\"distdf  cnt:\"+distdf.count())\n//distdf.select($\"jaccard_aff\", $\"emb_sim\", $\"jaccard_fos_0\", $\"jaccard_fos_1\", $\"author_names\").show(40)\n\n/*\ndistdf.filter(size($\"paperVector_\")>0 && size($\"paperVector\")>0).select($\"affiliationIds\", $\"fosIdsLevel0\",$\"fosIdsLevel1\", $\"authorNormNames\", $\"magAuthorId\", $\"orcId\", $\"s2AuthorId\",$\"affiliationIds_\", $\"fosIdsLevel0_\",$\"fosIdsLevel1_\", $\"authorNormNames_\", $\"magAuthorId_\", $\"orcId_\", $\"s2AuthorId_\",\n    //$\"paperVector_\", $\"paperVector\",\n$\"jaccard_aff\", $\"emb_sim\", $\"jaccard_fos_0\", $\"jaccard_fos_1\", $\"author_names\")\n        //.show(100)\n        .limit(1000).coalesce(1).write.mode(\"overwrite\").json(s\"$OA_HOME/features.json\")\n */\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-14T11:06:45+0200",
   "progress": 93.0,
   "config": {},
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {}
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.sql.functions._\nimport org.apache.spark.storage.StorageLevel\n\u001b[1m\u001b[34mOA_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/ometaxas/nvme/datasets/AND_sample_ds/baseSignature1\n\u001b[1m\u001b[34msigndf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [affiliationIds: array<bigint>, fosIdsLevel0: array<bigint> ... 7 more fields]\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 116910\n\u001b[1m\u001b[34mdfi\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id: int]\n\u001b[1m\u001b[34mdfj\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [id_: int]\n\u001b[1m\u001b[34mindices\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [id: int, id_: int]\n\u001b[1m\u001b[34msigndf2\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [affiliationIds_: array<bigint>, fosIdsLevel0_: array<bigint> ... 7 more fields]\n\u001b[1m\u001b[3...\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=0"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=1"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=2"
      },
      {
       "jobUrl": "http://192.168.2.10:4040/jobs/job?id=6"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644829605980_857425638",
   "id": "paragraph_1644829605980_857425638",
   "dateCreated": "2022-02-14T11:06:45+0200",
   "dateStarted": "2022-02-14T11:06:45+0200",
   "dateFinished": "2022-02-14T11:22:12+0200",
   "status": "FINISHED"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "import ai.onnxruntime._\nimport collection.JavaConverters._\n\n\n  val pathToOnnxFile: String = \"data/onnx/ensemble.onnx\"\n  val env = OrtEnvironment.getEnvironment()\n  val options = new OrtSession.SessionOptions()  //runtime options => PkgConfig\n  val session = env.createSession(pathToOnnxFile, options)\n\n  def forwardDistance(features: List[Double]): Double =\n    forwardForList(List(features)).head\n\n  def close(): Unit = env.close()\n\n  def forwardForList(featuresList: List[List[Double]]): List[Double]  = {\n    try {\n      val featuresArray: Array[Array[Float]] = featuresList.map(_.map(_.toFloat).toArray).toArray\n      val inputsTensor: OnnxTensor = OnnxTensor.createTensor(env, featuresArray)\n      val inputMap: Map[String, OnnxTensor] = Map(\"input\" -> inputsTensor)\n      val inputs: java.util.Map[String, OnnxTensor] = inputMap.asJava\n      val onnxResult:OrtSession.Result = session.run(inputs)\n      val tensorOutput: OnnxTensor = onnxResult.get(0).asInstanceOf[OnnxTensor]\n      val array = tensorOutput.getValue.asInstanceOf[Array[Array[Float]]]\n      val result = array map (_.head)\n      result.toList.map(_.toDouble)\n    }\n    catch {\n      case e:Exception => throw new RuntimeException(e)\n    }\n \n ",
   "id": "",
   "config": {}
  },
  {
   "text": "%spark\n\nval OA_HOME = \"/media/ometaxas/nvme/datasets/AND_sample_ds\"\n\nval count = 40000\nval ind = for(i <- 0 until count; j <- 0 until count; if i > j) yield (i, j)\n\nprintln(\"indices  cnt:\"+ ind.size)\n\n\n        \nval indices =\n    //spark.createDataFrame(ind)    \n    //ind.toDF()\n      sc.parallelize(ind,10000)\n              .toDF()\n        .cache()\n        //.select($\"_1\".as(\"id\"),$\"_2\".as(\"id_\")).cache()\n       \n//indices.write.mode(\"overwrite\").parquet(s\"$OA_HOME/indices.parquet\")\nprintln(\"indices  cnt:\"+indices.count())\n//indices.printSchema()\n//indices.show(5)\n//indices.write.mode(\"overwrite\").parquet(s\"$OA_HOME/indices40K.parquet\")\n\n//GPU GC_1 5m6s 90.7GB \n//CPU GC_1  5m 8s 91GB\n//CPU GC_1 10000 3m 19s 98GB\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-13T17:28:24+0200",
   "progress": 0.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644766104044_1025498527",
   "id": "paragraph_1644766104044_1025498527",
   "dateCreated": "2022-02-13T17:28:24+0200",
   "dateStarted": "2022-02-13T17:28:24+0200",
   "status": "ABORT"
  },
  {
   "text": "%spark\nval OA_HOME = \"/media/ometaxas/nvme/datasets/AND_sample_ds\"\n\nval count = 40000\n\nval dfi = (0 until count).toDF(\"i\")\nval dfj = (0 until count).toDF(\"j\")\n\nval indices = dfi.crossJoin(dfj)\n                .filter(dfi(\"i\") > dfj(\"j\"))\n        .cache()\n\n//indices.write.mode(\"overwrite\").parquet(s\"$OA_HOME/indices.parquet\")\nprintln(\"indices  cnt:\"+indices.count())\n//indices.show(5)\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-12T18:05:48+0200",
   "progress": 88.0,
   "config": {},
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "indices  cnt:799980000\n\u001b[1m\u001b[34mOA_HOME\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = /media/ometaxas/nvme/datasets/AND_sample_ds\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 40000\n\u001b[1m\u001b[34mdfi\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [i: int]\n\u001b[1m\u001b[34mdfj\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [j: int]\n\u001b[1m\u001b[34mindices\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [i: int, j: int]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://omiros.station:4040/jobs/job?id=1"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644681948310_1034080251",
   "id": "paragraph_1644681948310_1034080251",
   "dateCreated": "2022-02-12T18:05:48+0200",
   "dateStarted": "2022-02-12T18:05:48+0200",
   "dateFinished": "2022-02-12T18:06:47+0200",
   "status": "FINISHED"
  },
  {
   "text": "import org.apache.spark.ml.linalg.Vectors._\nimport org.apache.spark.ml.linalg.Vectors\n\n//def convertArrayToVector = udf((features:Array[Double]) => Vectors.dense(features))\n//spark.udf.register(\"convertArrayToVector\", convertArrayToVector)\n\n// Cosine similarity between Vectors.\n  def cosine_similarity(arrayX:Array[Double], arrayY:Array[Double]):Double =\n  {\n   if (arrayX == null || arrayY == null)\n       { 0 }\n   else {\n\n       val X = Vectors.dense(arrayX)\n       val Y = Vectors.dense(arrayY)\n\n       val denom = norm(X, 2) * norm(Y, 2)\n\n       //Computing the dot product between\n\n       val x = X.toSparse // spark ml.linalg.Vector is an instance of SparseVector, hence converting to Sparse representation\n       val y = Y.toSparse\n\n       val xValues = x.values\n       val xIndices = x.indices\n       val yValues = y.values\n       val yIndices = y.indices\n       val nnzx = xIndices.length\n       val nnzy = yIndices.length\n\n       var kx = 0\n       var ky = 0\n       var sum = 0.0\n       // y catching x\n       while (kx < nnzx && ky < nnzy) {\n           val ix = xIndices(kx)\n           while (ky < nnzy && yIndices(ky) < ix) {\n               ky += 1\n           }\n           if (ky < nnzy && yIndices(ky) == ix) {\n               sum += xValues(kx) * yValues(ky)\n               ky += 1\n           }\n           kx += 1\n       }\n\n       if (denom == 0.0) -1.0 else sum / denom.toDouble\n   }\n  }\n\nval cosineSimilarityUdf = udf(cosine_similarity _)\n\nspark.udf.register(\"cosineSimilarityUdf\", cosineSimilarityUdf)\n\n",
   "user": "anonymous",
   "dateUpdated": "2022-02-12T22:58:06+0200",
   "progress": 0.0,
   "config": {
    "tableHide": true
   },
   "settings": {
    "params": {
     "bdtMeta": {
      "inlay": {
       "collapsed": true
      }
     }
    },
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.ml.linalg.Vectors._\nimport org.apache.spark.ml.linalg.Vectors\n\u001b[1m\u001b[34mcosine_similarity\u001b[0m: \u001b[1m\u001b[32m(arrayX: Array[Double], arrayY: Array[Double])Double\u001b[0m\n\u001b[1m\u001b[34mcosineSimilarityUdf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = SparkUserDefinedFunction($Lambda$2197/0x00007f4d7ddff040@3d7f4a58,DoubleType,List(Some(class[value[0]: array<double>]), Some(class[value[0]: array<double>])),Some(class[value[0]: double]),None,false,true)\n\u001b[1m\u001b[34mres1\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.UserDefinedFunction\u001b[0m = SparkUserDefinedFunction($Lambda$2197/0x00007f4d7ddff040@3d7f4a58,DoubleType,List(Some(class[value[0]: array<double>]), Some(class[value[0]: array<double>])),Some(class[value[0]: double]),None,false,true)\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500.0,
   "jobName": "paragraph_1644699485991_928402506",
   "id": "paragraph_1644699485991_928402506",
   "dateCreated": "2022-02-12T22:58:05+0200",
   "dateStarted": "2022-02-12T22:58:06+0200",
   "dateFinished": "2022-02-12T22:58:16+0200",
   "status": "FINISHED"
  }
 ],
 "name": "Zeppelin Notebook",
 "id": "",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {}
}